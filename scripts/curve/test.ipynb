{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timeit import default_timer\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../../\")\n",
    "from utility.adam import Adam\n",
    "from utility.losses import LpLoss\n",
    "from utility.normalizer import UnitGaussianNormalizer\n",
    "from pcno.geo_utility import compute_edge_gradient_weights\n",
    "from generate_curves_data import compute_unit_normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use normalized raw measures\n",
      "Computing normal vector\n",
      "use normalized raw measures\n",
      "Computing normal vector\n"
     ]
    }
   ],
   "source": [
    "def get_data(data_path, range_index, add_normal_vector = False, add_normal_prod = False, weight_max = None, normal_vector_in_feature = False):\n",
    "    data = np.load(data_path)\n",
    "\n",
    "    nnodes, node_mask, nodes = data[\"nnodes\"], data[\"node_mask\"], data[\"nodes\"]\n",
    "    node_weights = data[\"node_measures_raw\"]\n",
    "    # print('use node_weight')\n",
    "    if not weight_max:\n",
    "        weight_max = np.amax(np.sum(node_weights, axis = 1))\n",
    "    node_weights = node_weights/weight_max\n",
    "    print('use normalized raw measures')\n",
    "    directed_edges, edge_gradient_weights = data[\"directed_edges\"], data[\"edge_gradient_weights\"]\n",
    "    features = data[\"features\"]\n",
    "    node_measures = data[\"node_measures\"]\n",
    "    node_measures_raw = data[\"node_measures_raw\"]\n",
    "    indices = np.isfinite(node_measures_raw)\n",
    "    node_rhos = np.copy(node_weights)\n",
    "    node_rhos[indices] = node_rhos[indices]/node_measures[indices]\n",
    "\n",
    "    if add_normal_vector:\n",
    "        print('Computing normal vector')\n",
    "        normal_vector = np.zeros_like(nodes)\n",
    "        for i in range(nodes.shape[0]):\n",
    "            normal_vector[i] = compute_unit_normals(nodes[i])\n",
    "        normal_vector = torch.from_numpy(normal_vector.astype(np.float32))[range_index]\n",
    "\n",
    "    nnodes = torch.from_numpy(nnodes)\n",
    "    node_mask = torch.from_numpy(node_mask)[range_index]\n",
    "    nodes = torch.from_numpy(nodes.astype(np.float32))[range_index]\n",
    "    node_weights = torch.from_numpy(node_weights.astype(np.float32))[range_index]\n",
    "    node_rhos = torch.from_numpy(node_rhos.astype(np.float32))[range_index]\n",
    "    features = torch.from_numpy(features.astype(np.float32))[range_index]\n",
    "    directed_edges = torch.from_numpy(directed_edges.astype(np.int64))[range_index]\n",
    "    edge_gradient_weights = torch.from_numpy(edge_gradient_weights.astype(np.float32))[range_index]\n",
    "\n",
    "    nodes_input = nodes.clone()\n",
    "\n",
    "    if not normal_vector_in_feature:\n",
    "        if add_normal_vector:\n",
    "            x_all = torch.cat((features[:, :, :1], normal_vector, nodes_input, node_rhos), -1)\n",
    "            if add_normal_prod:\n",
    "                x_all = torch.cat((x_all, x_all[..., 0:1] * x_all[..., 1:3]), dim = -1)\n",
    "    else:\n",
    "        if add_normal_prod:\n",
    "            x_all = torch.cat((features[:, :, :3], nodes_input, node_rhos, features[:, :, :1] * features[..., 1:3]), -1)\n",
    "        else:\n",
    "            x_all = torch.cat((features[:, :, :3], nodes_input, node_rhos), -1)\n",
    "\n",
    "    y_all = features[:, :, -1:]\n",
    "    aux_all = (node_mask, nodes, node_weights, directed_edges, edge_gradient_weights)\n",
    "    return x_all, y_all, aux_all, weight_max\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# pcno_curve_data_1_0.5_5_5_grad_deformed\n",
    "data_path_train = \"../../data/curve/pcno_curve_data_1_0.5_5_5_grad_deformed.npz\"\n",
    "data_path_test = \"../../data/curve/pcno_curve_data_1_1_5_5_grad_deformed_combined.npz\"\n",
    "add_normal_vector = True\n",
    "add_normal_prod = True\n",
    "normal_vector_in_feature = True\n",
    "range_index_train = list(range(0, 9000))\n",
    "range_index_test = list(range(0, 1000))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "N = len(range_index_train) + len(range_index_test)\n",
    "x_train, y_train, aux_train, weight_max = get_data(data_path_train, range_index = range_index_train, add_normal_vector = add_normal_vector, add_normal_prod = add_normal_prod)\n",
    "x_test, y_test, aux_test, weight_max = get_data(data_path_test, range_index = range_index_test, add_normal_vector = add_normal_vector, add_normal_prod = add_normal_prod,\n",
    "                                                 weight_max=weight_max, normal_vector_in_feature = normal_vector_in_feature)\n",
    "\n",
    "# x_all = torch.cat((x_train, x_test), dim = 0)\n",
    "# y_all = torch.cat((y_train, y_test), dim = 0)\n",
    "# aux_all = tuple(torch.cat((a, b), dim=0) for a, b in zip(aux_train, aux_test))\n",
    "normalization_x = False\n",
    "normalization_y = True\n",
    "normalization_dim_x = []\n",
    "normalization_dim_y = []\n",
    "non_normalized_dim_x = 4\n",
    "non_normalized_dim_y = 0\n",
    "\n",
    "config = {\"train\" : {\"normalization_x\": normalization_x,\"normalization_y\": normalization_y, \n",
    "                     \"normalization_dim_x\": normalization_dim_x, \"normalization_dim_y\": normalization_dim_y, \n",
    "                     \"non_normalized_dim_x\": non_normalized_dim_x, \"non_normalized_dim_y\": non_normalized_dim_y}\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from contextlib import redirect_stdout\n",
    "from pcno.pcno import compute_Fourier_modes, PCNO, PCNO_train\n",
    "from generate_curves_data import compute_unit_normals\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 17\n",
    "def test_normalizer(all_tuple, config):\n",
    "    x_train,y_train,x_test,y_test,node_mask_test,nodes_test,node_weights_test,directed_edges_test,edge_gradient_weights_test = all_tuple\n",
    " \n",
    "\n",
    "    normalization_x, normalization_y = config[\"train\"][\"normalization_x\"], config[\"train\"][\"normalization_y\"]\n",
    "    normalization_dim_x, normalization_dim_y = config[\"train\"][\"normalization_dim_x\"], config[\"train\"][\"normalization_dim_y\"]\n",
    "    non_normalized_dim_x, non_normalized_dim_y = config[\"train\"][\"non_normalized_dim_x\"], config[\"train\"][\"non_normalized_dim_y\"]\n",
    "\n",
    "    if normalization_x:\n",
    "        x_normalizer = UnitGaussianNormalizer(x_train, non_normalized_dim = non_normalized_dim_x, normalization_dim=normalization_dim_x)\n",
    "        x_test = x_normalizer.encode(x_test)\n",
    "    else:\n",
    "        x_normalizer = None\n",
    "    if normalization_y:\n",
    "        y_normalizer = UnitGaussianNormalizer(y_train, non_normalized_dim = non_normalized_dim_y, normalization_dim=normalization_dim_y)\n",
    "        y_test = y_normalizer.encode(y_test)\n",
    "\n",
    "        y_normalizer.to(device)\n",
    "    else:\n",
    "        y_normalizer = None\n",
    "\n",
    "    test_tuple = (x_test, y_test, node_mask_test,nodes_test,node_weights_test,directed_edges_test,edge_gradient_weights_test)\n",
    "\n",
    "    return test_tuple, y_normalizer\n",
    "\n",
    "def test_model(model,test_tuple, y_normalizer):\n",
    "\n",
    "    x_test,y_test,node_mask,nodes,node_weights,directed_edges,edge_gradient_weights = test_tuple\n",
    "\n",
    "    rel_l2 = []\n",
    "    index = []\n",
    "    myloss = LpLoss(d=1, p=2, size_average=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(x_test.shape[0]):\n",
    "            \n",
    "            x, y = x_test[i:i+1].to(device), y_test[i:i+1].to(device)\n",
    "            aux_batch = (\n",
    "            node_mask[i:i+1].to(device), nodes[i:i+1].to(device),\n",
    "            node_weights[i:i+1].to(device), directed_edges[i:i+1].to(device),\n",
    "            edge_gradient_weights[i:i+1].to(device)\n",
    "            )\n",
    "\n",
    "            out = model(x, aux_batch) #.reshape(batch_size_,  -1)\n",
    "            if y_normalizer:\n",
    "                out = y_normalizer.decode(out)\n",
    "                y = y_normalizer.decode(y)\n",
    "            batch_size_ = x.shape[0]\n",
    "            out = out * node_mask[i:i+1].to(device) #mask the padded value with 0,(1 for node, 0 for padding)\n",
    "            test_rel_l2 = myloss(out.view(batch_size_,-1), y.view(batch_size_,-1)).item()\n",
    "            # test_l2 = myloss.abs(out.view(batch_size_,-1), y.view(batch_size_,-1)).item()\n",
    "\n",
    "            rel_l2.append(test_rel_l2)\n",
    "            index.append(i)            \n",
    "            print(f'test index: {range_index_test[i]}, test_rel_l2: {test_rel_l2}')\n",
    "    return  rel_l2, index\n",
    "\n",
    "def sorted_result( rel_l2, index):\n",
    "\n",
    "\n",
    "    sorted_l2 = sorted(enumerate(rel_l2), key=lambda x: x[1], reverse=True)\n",
    "    average_loss = sum(rel_l2)/len(rel_l2)\n",
    "    print('average_rel_l2_of all :  ',round(average_loss,5), flush = True)\n",
    "    print()\n",
    "    n = 3\n",
    "    index_3 = [index[sorted_l2[0][0]],index[sorted_l2[len(sorted_l2)//2][0]],index[sorted_l2[-1][0]]]\n",
    "    for j in range(n):\n",
    "        print(f'{j+1}th_worst_rel_l2_of all :  ',round(sorted_l2[j][1],5), ' index : ',index[sorted_l2[j][0]])\n",
    "        print('medium_rel_l2_of all : ',round(sorted_l2[len(sorted_l2)//2][1],5), ' index : ',index_3[1])\n",
    "    for j in range(n):\n",
    "        print(f'{j+1}th_best_rel_l2_of all :  ',round(sorted_l2[-j-1][1],5), ' index : ',index[sorted_l2[-j-1][0]],flush = True)\n",
    "    print()\n",
    "    return average_loss,index_3\n",
    "\n",
    "\n",
    "\n",
    "def myplot(index_plot,save_figure_path,\n",
    "           model,test_tuple,y_normalizer):\n",
    "\n",
    "    x_test,y_test,node_mask,nodes,node_weights,directed_edges,edge_gradient_weights = test_tuple\n",
    "    myloss = LpLoss(d=1, p=2, size_average=False)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        fig, axs = plt.subplots(len(index_plot), 4, figsize=(20, 4*len(index_plot)))\n",
    "\n",
    "        for j in range(len(index_plot)):\n",
    "            i = index_plot[j]\n",
    "            x, y = x_test[i:i+1].to(device), y_test[i:i+1].to(device)\n",
    "            aux_batch = (\n",
    "            node_mask[i:i+1].to(device), nodes[i:i+1].to(device),\n",
    "            node_weights[i:i+1].to(device), directed_edges[i:i+1].to(device),\n",
    "            edge_gradient_weights[i:i+1].to(device)\n",
    "            )\n",
    "            out = model(x, aux_batch)\n",
    "            if y_normalizer:\n",
    "                out = y_normalizer.decode(out)\n",
    "                y = y_normalizer.decode(y)\n",
    "            batch_size_ = x.shape[0]\n",
    "            out = out * node_mask[i:i+1].to(device)\n",
    "            test_rel_l2 = myloss(out.view(batch_size_,-1), y.view(batch_size_,-1)).item()\n",
    "\n",
    "            nodes_plot = nodes[i].detach().cpu()\n",
    "            normal_plot = compute_unit_normals(nodes_plot)\n",
    "            f_plot = x_test[i:i+1][:,:,0].reshape(-1).detach().cpu()\n",
    "            g_plot = y.reshape(-1).detach().cpu()\n",
    "            out_plot = out.reshape(-1).detach().cpu()\n",
    "            error_plot = out_plot - g_plot\n",
    "\n",
    "            # Consistent color scale for g and prediction\n",
    "            vmin_go = min(g_plot.min().item(), out_plot.min().item())\n",
    "            vmax_go = max(g_plot.max().item(), out_plot.max().item())\n",
    "            norm_go = mpl.colors.Normalize(vmin=vmin_go, vmax=vmax_go)\n",
    "\n",
    "            # Symmetric color scale around 0 for error\n",
    "            vmax_err = torch.max(torch.abs(error_plot)).item()\n",
    "            norm_err = mpl.colors.TwoSlopeNorm(vmin=-vmax_err, vcenter=0.0, vmax=vmax_err)\n",
    "\n",
    "            axs[j,0].plot(nodes_plot[:, 0], nodes_plot[:, 1], color='blue', alpha=0.5)\n",
    "            scatter_f = axs[j,0].scatter(nodes_plot[:, 0], nodes_plot[:, 1], c=f_plot, cmap='viridis', s=40)\n",
    "            axs[j,0].quiver(nodes_plot[:, 0], nodes_plot[:, 1], normal_plot[:, 0], normal_plot[:, 1], color='red', scale=10, width=0.001, alpha=0.7)\n",
    "            axs[j,0].set_title('Input f(x)')\n",
    "            axs[j,0].axis('equal')\n",
    "            fig.colorbar(scatter_f, ax=axs[j,0])\n",
    "\n",
    "            axs[j,1].plot(nodes_plot[:, 0], nodes_plot[:, 1], color='blue', alpha=0.5)\n",
    "            scatter_g = axs[j,1].scatter(nodes_plot[:, 0], nodes_plot[:, 1], c=g_plot, cmap='viridis', s=40, norm=norm_go)\n",
    "            axs[j,1].set_title('Ground Truth g(x)')\n",
    "            axs[j,1].axis('equal')\n",
    "            fig.colorbar(scatter_g, ax=axs[j,1])\n",
    "\n",
    "            axs[j,2].plot(nodes_plot[:, 0], nodes_plot[:, 1], color='blue', alpha=0.5)\n",
    "            scatter_out = axs[j,2].scatter(nodes_plot[:, 0], nodes_plot[:, 1], c=out_plot, cmap='viridis', s=40, norm=norm_go)\n",
    "            axs[j,2].set_title('Prediction g_pred(x)')\n",
    "            axs[j,2].axis('equal')\n",
    "            fig.colorbar(scatter_out, ax=axs[j,2])\n",
    "\n",
    "            axs[j,3].plot(nodes_plot[:, 0], nodes_plot[:, 1], color='blue', alpha=0.5)\n",
    "            scatter_error = axs[j,3].scatter(nodes_plot[:, 0], nodes_plot[:, 1], c=error_plot, cmap='coolwarm', s=40, norm=norm_err)\n",
    "            axs[j,3].set_title(f'Error, loss = {round(test_rel_l2,5)}')\n",
    "            axs[j,3].axis('equal')\n",
    "            fig.colorbar(scatter_error, ax=axs[j,3])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if not os.path.exists(save_figure_path):\n",
    "            os.makedirs(save_figure_path)\n",
    "        fig.savefig(save_figure_path + f'test_{[range_index_test[i] for i in index_plot]}.png', format='png', bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "with open('output.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "# pcno_curve_data_1_0.5_5_5_grad_deformed\n",
    "        model_path = 'E:/codes/mygithub/scripts/curve/' + f'model/1_0.5_5_5_grad_deformed/k8_L10_normal_prod.pth'\n",
    "        save_figure_path = 'E:/codes/mygithub/scripts/curve/' + f'figures/'\n",
    "\n",
    "\n",
    "        k_max = 8\n",
    "        ndim = 2\n",
    "        L = 10\n",
    "\n",
    "        model_train_inv_L_scale = False\n",
    "        modes = compute_Fourier_modes(ndim, [k_max,k_max], [L,L])\n",
    "        modes = torch.tensor(modes, dtype=torch.float).to(device)\n",
    "        model = PCNO(ndim, modes, nmeasures=1,\n",
    "                    layers=[128,128,128,128,128],\n",
    "                    fc_dim=128,\n",
    "                    in_dim=x_test.shape[-1], out_dim=y_test.shape[-1],\n",
    "                    inv_L_scale_hyper = [model_train_inv_L_scale, 0.5, 2.0],\n",
    "                    act='gelu').to(device)\n",
    "\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print('Train Data : ',data_path_train,'\\n')\n",
    "        print('Test Data : ',data_path_test,'\\n')\n",
    "        print(f'Model : {model_path}', flush = True)\n",
    "        tuple_all = (x_train,y_train,x_test,y_test) + aux_test\n",
    "        test_tuple, y_normalizer = test_normalizer(tuple_all, config)\n",
    "        rel_l2, index = test_model(model, test_tuple, y_normalizer)\n",
    "        average_loss_list,index_3 = sorted_result(rel_l2, index)\n",
    "\n",
    "        # test_index = [925,952,999]\n",
    "        myplot(index_3,save_figure_path,\n",
    "                model,test_tuple, y_normalizer)\n",
    "        print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_exponential = np.array(rel_l2_exponential)\n",
    "# loss_linear = np.array(rel_l2_linear)\n",
    "# loss_uniform = np.array(rel_l2_uniform)\n",
    "# np.savez('test_result/test_losses.npz', loss_exponential=loss_exponential, loss_linear=loss_linear, loss_uniform = loss_uniform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
