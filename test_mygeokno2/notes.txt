
PS C:\Users\15461\Desktop\mygithub2\test_mygeokno2> cd "c:\Users\15461\Desktop\mygithub2\test_mygeokno2"
PS C:\Users\15461\Desktop\mygithub2\test_mygeokno2> python -u "c:\Users\15461\Desktop\mygithub2\test_mygeokno2\geokno_learningmodes_car_test.py"
Casting to tensor
In GeoKNO_train, ndims =  3
L: [2.0, 2.0, 5.0] L_grad:  [-0.0024, 0.0005, 0.0008]
L: [2.0002, 1.9999, 4.9999] L_grad:  [0.0091, -0.0404, -0.0068]
L: [1.9993, 2.004, 5.0006] L_grad:  [0.0967, 0.0162, 0.0295]
L: [1.9897, 2.0024, 4.9977] L_grad:  [-0.0878, -0.1921, -0.0631]
L: [1.9984, 2.0216, 5.004] L_grad:  [0.2398, -0.1222, -0.0281]
L: [1.9745, 2.0338, 5.0068] L_grad:  [0.3155, -0.5379, 0.0151]
L: [1.9429, 2.0876, 5.0053] L_grad:  [-0.8384, -0.9964, -0.1805]
L: [2.0267, 2.1872, 5.0233] L_grad:  [-1.396, -0.1741, -2.0942]
L: [2.1663, 2.2046, 5.2327] L_grad:  [0.8042, 2.0483, 0.7967]
L: [2.0859, 1.9998, 5.1531] L_grad:  [0.3627, 1.3864, -1.6725]
Epoch :  0  time:  16.988  Rel. Train L2 Loss :  0.2633640289306641  Rel. Test L2 Loss :  0.15643081068992615  Test L2 Loss :  9.588587074279785
L: [2.4808, 2.1863, 5.8282] L_grad:  [-0.0302, -0.3729, 0.2466]
L: [2.4839, 2.2236, 5.8036] L_grad:  [-0.1633, 0.2168, 0.2488]
L: [2.5002, 2.2019, 5.7787] L_grad:  [0.0196, -0.1893, -0.4758]
L: [2.4982, 2.2208, 5.8263] L_grad:  [0.0167, 0.0026, -0.2251]
L: [2.4966, 2.2206, 5.8488] L_grad:  [0.3033, 0.044, -0.1345]
L: [2.4662, 2.2162, 5.8622] L_grad:  [-0.166, 0.1546, -0.0543]
L: [2.4828, 2.2007, 5.8677] L_grad:  [0.2661, 0.1108, 0.2406]
L: [2.4562, 2.1896, 5.8436] L_grad:  [0.0769, 0.2334, 0.0798]
L: [2.4485, 2.1663, 5.8356] L_grad:  [-0.4311, -0.0572, -0.0622]
L: [2.4916, 2.172, 5.8418] L_grad:  [-0.1422, -0.0282, 0.0487]
Epoch :  1  time:  17.033  Rel. Train L2 Loss :  0.12480730253458024  Rel. Test L2 Loss :  0.11388960719108582  Test L2 Loss :  7.008291072845459
L: [2.405, 2.1142, 6.0072] L_grad:  [-0.0214, -0.0772, 0.2193]
L: [2.4071, 2.1219, 5.9853] L_grad:  [0.2184, 0.4375, -0.2309]
L: [2.3853, 2.0781, 6.0084] L_grad:  [-0.3631, -0.6622, -0.048]
L: [2.4216, 2.1444, 6.0132] L_grad:  [0.2593, 0.5081, 0.2494]
L: [2.3957, 2.0936, 5.9882] L_grad:  [-0.2204, -0.5729, 0.085]
L: [2.4177, 2.1508, 5.9797] L_grad:  [-0.0839, -0.3914, -0.2226]
L: [2.4261, 2.19, 6.002] L_grad:  [-0.09, 0.3938, 0.042]
L: [2.4351, 2.1506, 5.9978] L_grad:  [-0.0299, -0.5108, 0.1552]
L: [2.4381, 2.2017, 5.9823] L_grad:  [0.2634, 0.1433, -0.078]
L: [2.4117, 2.1874, 5.9901] L_grad:  [-0.2325, -0.2791, 0.1049]
Epoch :  2  time:  16.508  Rel. Train L2 Loss :  0.10165429472923279  Rel. Test L2 Loss :  0.10392802864313126  Test L2 Loss :  6.37572473526001
L: [2.39, 2.1827, 5.9547] L_grad:  [0.083, 0.3929, 0.0232]
L: [2.3817, 2.1434, 5.9524] L_grad:  [0.0684, -0.339, 0.0392]
L: [2.3749, 2.1773, 5.9484] L_grad:  [-0.1143, 0.1603, -0.0879]
L: [2.3863, 2.1613, 5.9572] L_grad:  [-0.1011, 0.1303, 0.0933]
L: [2.3964, 2.1483, 5.9479] L_grad:  [-0.0586, -0.0199, -0.0802]
L: [2.4023, 2.1503, 5.9559] L_grad:  [-0.3602, -0.084, -0.1442]
L: [2.4383, 2.1587, 5.9703] L_grad:  [0.2334, 0.256, 0.0201]
L: [2.4149, 2.1331, 5.9683] L_grad:  [0.0199, -0.1631, 0.0824]
L: [2.413, 2.1494, 5.9601] L_grad:  [-0.1168, -0.1277, -0.0667]
L: [2.4246, 2.1621, 5.9667] L_grad:  [-0.1888, -0.1223, -0.7073]
Epoch :  3  time:  16.732  Rel. Train L2 Loss :  0.09059234416484832  Rel. Test L2 Loss :  0.0965577694773674  Test L2 Loss :  5.9253839492797855
L: [2.3805, 2.1722, 6.1208] L_grad:  [-0.3149, -0.2923, -0.6597]
L: [2.412, 2.2014, 6.1867] L_grad:  [0.1556, 0.3698, 0.0859]
L: [2.3965, 2.1644, 6.1782] L_grad:  [-0.0981, 0.1379, 0.0194]
L: [2.4063, 2.1506, 6.1762] L_grad:  [0.0769, 0.1045, -0.0794]
L: [2.3986, 2.1402, 6.1842] L_grad:  [0.1592, -0.2015, 0.1686]
L: [2.3827, 2.1603, 6.1673] L_grad:  [-0.1345, -0.0025, 0.1281]
L: [2.3961, 2.1606, 6.1545] L_grad:  [0.1126, 0.0351, 0.1885]
L: [2.3849, 2.1571, 6.1356] L_grad:  [0.0237, 0.0368, 0.0255]
L: [2.3825, 2.1534, 6.1331] L_grad:  [-0.0813, 0.2064, -0.0351]
L: [2.3906, 2.1328, 6.1366] L_grad:  [-0.2962, -0.5502, 0.0547]
Epoch :  4  time:  16.601  Rel. Train L2 Loss :  0.08082294881343842  Rel. Test L2 Loss :  0.09582514643669128  Test L2 Loss :  5.874759864807129
L: [2.3881, 2.1533, 6.0656] L_grad:  [-0.3058, 0.2871, -0.1767]
L: [2.4187, 2.1246, 6.0833] L_grad:  [-0.0536, -0.4452, 0.2587]
L: [2.4241, 2.1691, 6.0574] L_grad:  [0.0479, 0.4315, 0.0827]
L: [2.4193, 2.126, 6.0491] L_grad:  [0.3378, 0.069, -0.1442]
L: [2.3855, 2.1191, 6.0635] L_grad:  [-0.161, -0.173, 0.0687]
L: [2.4016, 2.1364, 6.0567] L_grad:  [0.0979, 0.0514, 0.2363]
L: [2.3918, 2.1312, 6.033] L_grad:  [0.0001, 0.1321, 0.0369]
L: [2.3918, 2.118, 6.0293] L_grad:  [-0.0198, -0.2136, -0.125]
L: [2.3938, 2.1394, 6.0418] L_grad:  [0.1842, 0.1938, -0.1848]
L: [2.3754, 2.12, 6.0603] L_grad:  [-0.3588, -0.2241, 0.1898]
Epoch :  5  time:  16.813  Rel. Train L2 Loss :  0.0752856683731079  Rel. Test L2 Loss :  0.08610589236021042  Test L2 Loss :  5.284712829589844
L: [2.3752, 2.1446, 6.1272] L_grad:  [-0.1243, 0.0968, -0.1088]
L: [2.3876, 2.1349, 6.1381] L_grad:  [-0.1036, -0.4348, 0.2809]
L: [2.3979, 2.1784, 6.11] L_grad:  [0.1707, 0.3264, -0.01]
L: [2.3809, 2.1457, 6.111] L_grad:  [-0.1002, 0.0149, 0.0196]
L: [2.3909, 2.1442, 6.109] L_grad:  [-0.3439, 0.0889, 0.0973]
L: [2.4253, 2.1354, 6.0993] L_grad:  [0.5704, 0.1123, -0.1305]
L: [2.3682, 2.1241, 6.1124] L_grad:  [-0.5041, -0.6993, 0.1662]
L: [2.4187, 2.1941, 6.0957] L_grad:  [0.3095, 0.7714, -0.1719]
L: [2.3877, 2.1169, 6.1129] L_grad:  [-0.2601, -0.5657, 0.2871]
L: [2.4137, 2.1735, 6.0842] L_grad:  [0.3792, 0.5632, -0.2713]
Epoch :  6  time:  16.546  Rel. Train L2 Loss :  0.07101579332351685  Rel. Test L2 Loss :  0.08901417940855026  Test L2 Loss :  5.460063438415528
L: [2.3983, 2.1727, 6.0761] L_grad:  [0.1086, 0.7004, -0.237]
L: [2.3875, 2.1027, 6.0998] L_grad:  [0.1187, -0.631, -0.1027]
L: [2.3756, 2.1658, 6.11] L_grad:  [-0.3921, 0.0638, 0.0872]
L: [2.4148, 2.1594, 6.1013] L_grad:  [0.2503, 0.223, 0.017]
L: [2.3898, 2.1371, 6.0996] L_grad:  [-0.0604, -0.5917, 0.3745]
L: [2.3958, 2.1963, 6.0621] L_grad:  [0.2952, 0.9968, -0.1748]
L: [2.3663, 2.0966, 6.0796] L_grad:  [-0.3463, -1.2592, -0.0826]
L: [2.4009, 2.2225, 6.0879] L_grad:  [0.2914, 1.1235, -0.226]
L: [2.3718, 2.1102, 6.1105] L_grad:  [-0.3107, -0.6281, 0.2656]
L: [2.4028, 2.173, 6.0839] L_grad:  [0.0311, 0.1895, 0.0542]
Epoch :  7  time:  17.06  Rel. Train L2 Loss :  0.07127884721755981  Rel. Test L2 Loss :  0.08629021942615508  Test L2 Loss :  5.288446960449218
L: [2.392, 2.1225, 6.1207] L_grad:  [0.001, -0.5567, 0.1983]
L: [2.3919, 2.1782, 6.1008] L_grad:  [-0.1744, 0.2342, 0.0679]
L: [2.4093, 2.1548, 6.094] L_grad:  [0.3221, 0.2842, -0.1345]
L: [2.3771, 2.1263, 6.1075] L_grad:  [-0.3838, -0.1257, 0.0355]
L: [2.4155, 2.1389, 6.104] L_grad:  [0.19, 0.0676, 0.0871]
L: [2.3965, 2.1322, 6.0952] L_grad:  [-0.1337, -0.2192, 0.1006]
L: [2.4099, 2.1541, 6.0852] L_grad:  [0.3212, 0.3082, -0.2916]
L: [2.3778, 2.1233, 6.1143] L_grad:  [0.071, -0.1591, 0.0199]
L: [2.3707, 2.1392, 6.1124] L_grad:  [-0.25, -0.1981, 0.1829]
L: [2.3957, 2.159, 6.0941] L_grad:  [0.0927, 0.3345, -0.0927]
Epoch :  8  time:  17.016  Rel. Train L2 Loss :  0.06703539258241653  Rel. Test L2 Loss :  0.08243244588375091  Test L2 Loss :  5.063701400756836
L: [2.3635, 2.1204, 6.1332] L_grad:  [-0.317, -0.3206, 0.0991]
L: [2.3952, 2.1524, 6.1233] L_grad:  [0.1522, 0.5211, 0.0199]
L: [2.38, 2.1003, 6.1213] L_grad:  [-0.2578, -0.4152, 0.1563]
L: [2.4058, 2.1418, 6.1057] L_grad:  [0.5299, 0.5218, -0.3128]
L: [2.3528, 2.0897, 6.137] L_grad:  [-0.3918, -0.7812, -0.0018]
L: [2.392, 2.1678, 6.1371] L_grad:  [-0.0648, 0.3448, 0.0526]
L: [2.3984, 2.1333, 6.1319] L_grad:  [-0.513, -0.6025, 0.3745]
L: [2.4497, 2.1935, 6.0944] L_grad:  [0.8825, 0.8993, -0.3048]
L: [2.3615, 2.1036, 6.1249] L_grad:  [-0.233, -0.2862, -0.0082]
L: [2.3848, 2.1322, 6.1257] L_grad:  [-0.2312, 0.1096, 0.0172]
Epoch :  9  time:  17.415  Rel. Train L2 Loss :  0.0667166433930397  Rel. Test L2 Loss :  0.0846293169260025  Test L2 Loss :  5.1964718627929685
L: [2.389, 2.1271, 6.108] L_grad:  [0.1496, 0.2607, -0.1533]
L: [2.3741, 2.101, 6.1233] L_grad:  [0.0143, -0.5203, 0.2635]
L: [2.3726, 2.153, 6.0969] L_grad:  [0.1108, 0.5028, 0.0313]
L: [2.3616, 2.1028, 6.0938] L_grad:  [-0.2899, -0.5813, 0.0571]
L: [2.3905, 2.1609, 6.0881] L_grad:  [0.136, 1.0951, -0.2424]
L: [2.3769, 2.0514, 6.1123] L_grad:  [-0.1943, -1.3119, 0.1869]
L: [2.3964, 2.1826, 6.0937] L_grad:  [0.2178, 0.6797, -0.0259]
L: [2.3746, 2.1146, 6.0963] L_grad:  [-0.2475, -0.3234, -0.0818]
L: [2.3993, 2.1469, 6.1044] L_grad:  [0.3412, 0.098, -0.0524]
L: [2.3652, 2.1371, 6.1097] L_grad:  [-0.2405, -0.2346, 0.1571]
Epoch :  10  time:  17.509  Rel. Train L2 Loss :  0.06378988933563233  Rel. Test L2 Loss :  0.07835117638111115  Test L2 Loss :  4.805235080718994
L: [2.3917, 2.1282, 6.1213] L_grad:  [0.0378, -0.2848, -0.0193]
L: [2.3879, 2.1567, 6.1233] L_grad:  [0.0915, 0.2856, -0.0141]
L: [2.3788, 2.1281, 6.1247] L_grad:  [-0.1665, -0.7739, 0.211]
L: [2.3954, 2.2055, 6.1036] L_grad:  [0.4655, 0.5759, -0.0845]
L: [2.3489, 2.1479, 6.112] L_grad:  [-0.3394, 0.325, 0.1769]
L: [2.3828, 2.1154, 6.0943] L_grad:  [0.1125, -0.1325, -0.0991]
L: [2.3716, 2.1287, 6.1042] L_grad:  [0.104, 0.0194, -0.2367]
L: [2.3612, 2.1267, 6.1279] L_grad:  [-0.1272, -0.0854, 0.147]
L: [2.3739, 2.1353, 6.1132] L_grad:  [-0.2921, -0.0818, 0.2376]
L: [2.4031, 2.1434, 6.0894] L_grad:  [0.2792, 0.1857, -0.0493]
Epoch :  11  time:  17.724  Rel. Train L2 Loss :  0.06155530482530594  Rel. Test L2 Loss :  0.08272009760141373  Test L2 Loss :  5.075251445770264
L: [2.369, 2.1318, 6.188] L_grad:  [-0.051, 0.162, -0.3712]
L: [2.3741, 2.1156, 6.2251] L_grad:  [-0.1026, -0.2593, 0.0556]
L: [2.3843, 2.1415, 6.2195] L_grad:  [0.1306, 0.2703, 0.2723]
L: [2.3713, 2.1145, 6.1923] L_grad:  [-0.0346, -0.2908, 0.0386]
L: [2.3747, 2.1435, 6.1885] L_grad:  [0.0833, 0.0042, 0.0598]
L: [2.3664, 2.1431, 6.1825] L_grad:  [-0.0374, 0.5191, -0.1557]
L: [2.3701, 2.0912, 6.198] L_grad:  [-0.1293, -0.6759, 0.245]
L: [2.3831, 2.1588, 6.1735] L_grad:  [0.2346, 0.6724, 0.0463]
L: [2.3596, 2.0916, 6.1689] L_grad:  [-0.216, -0.7246, 0.1165]
L: [2.3812, 2.164, 6.1573] L_grad:  [0.1947, 0.7928, -0.1785]
Epoch :  12  time:  17.897  Rel. Train L2 Loss :  0.0636037782728672  Rel. Test L2 Loss :  0.0844826865196228  Test L2 Loss :  5.181309928894043
L: [2.3772, 2.1309, 6.1059] L_grad:  [0.016, 0.3843, -0.1319]
L: [2.3756, 2.0925, 6.119] L_grad:  [-0.4227, -0.7087, 0.1949]
L: [2.4178, 2.1634, 6.0995] L_grad:  [0.5845, 0.8433, -0.3335]
L: [2.3594, 2.079, 6.1329] L_grad:  [-0.3161, -0.8152, 0.2713]
L: [2.391, 2.1606, 6.1058] L_grad:  [0.3132, 0.6509, -0.1047]
L: [2.3597, 2.0955, 6.1162] L_grad:  [-0.3825, -0.6056, 0.3129]
L: [2.3979, 2.156, 6.085] L_grad:  [0.3961, 0.8009, -0.2029]
L: [2.3583, 2.076, 6.1052] L_grad:  [-0.1185, -0.4922, -0.0373]
L: [2.3702, 2.1252, 6.109] L_grad:  [0.0487, 0.2072, -0.2062]
L: [2.3653, 2.1045, 6.1296] L_grad:  [0.0011, -0.5161, 0.2685]
Epoch :  13  time:  18.09  Rel. Train L2 Loss :  0.061318312227725984  Rel. Test L2 Loss :  0.07840747565031052  Test L2 Loss :  4.80705717086792
L: [2.3744, 2.1109, 6.1735] L_grad:  [-0.1628, -0.7764, 0.1702]
L: [2.3907, 2.1885, 6.1565] L_grad:  [0.3593, 1.4771, -0.3504]
L: [2.3547, 2.0408, 6.1915] L_grad:  [0.0035, -1.2655, 0.3971]
L: [2.3544, 2.1673, 6.1518] L_grad:  [0.0401, 1.2482, -0.2473]
L: [2.3504, 2.0425, 6.1766] L_grad:  [-0.1679, -0.7553, 0.1081]
L: [2.3672, 2.1181, 6.1658] L_grad:  [-0.0498, 0.4047, -0.1244]
L: [2.3722, 2.0776, 6.1782] L_grad:  [-0.183, -0.3421, 0.0818]
L: [2.3905, 2.1118, 6.17] L_grad:  [0.2275, 0.1418, -0.0588]
L: [2.3677, 2.0976, 6.1759] L_grad:  [-0.0617, -0.5785, 0.1503]
L: [2.3739, 2.1555, 6.1609] L_grad:  [0.1175, 0.5123, 0.0364]
Epoch :  14  time:  18.558  Rel. Train L2 Loss :  0.05915493738651276  Rel. Test L2 Loss :  0.080525723695755  Test L2 Loss :  4.941384010314941
L: [2.3849, 2.1638, 6.1438] L_grad:  [0.1659, -0.139, 0.1821]
L: [2.3683, 2.1777, 6.1256] L_grad:  [-0.0726, 0.38, -0.0319]
L: [2.3756, 2.1397, 6.1288] L_grad:  [-0.0655, 0.2762, -0.1363]
L: [2.3822, 2.1121, 6.1424] L_grad:  [0.224, -0.6748, 0.1348]
L: [2.3598, 2.1796, 6.1289] L_grad:  [0.0023, 0.9772, -0.0668]
L: [2.3595, 2.0818, 6.1356] L_grad:  [-0.1938, -1.1014, 0.3504]
L: [2.3789, 2.192, 6.1006] L_grad:  [0.398, 1.3325, -0.3123]
L: [2.3391, 2.0587, 6.1318] L_grad:  [-0.4073, -0.7063, -0.0252]
L: [2.3798, 2.1294, 6.1343] L_grad:  [0.1359, 0.0646, -0.1069]
L: [2.3662, 2.1229, 6.145] L_grad:  [0.0584, -0.3991, 0.2701]
Epoch :  15  time:  18.34  Rel. Train L2 Loss :  0.0594897985458374  Rel. Test L2 Loss :  0.07643677890300751  Test L2 Loss :  4.69041820526123
L: [2.3664, 2.1275, 6.1481] L_grad:  [-0.0232, -0.064, 0.0301]
L: [2.3687, 2.1339, 6.1451] L_grad:  [0.191, 0.2641, -0.0668]
L: [2.3496, 2.1075, 6.1517] L_grad:  [-0.3606, -0.5954, 0.1604]
L: [2.3857, 2.1671, 6.1357] L_grad:  [0.2359, 0.7679, -0.0899]
L: [2.3621, 2.0903, 6.1447] L_grad:  [-0.0833, -0.7707, 0.1913]
L: [2.3704, 2.1673, 6.1256] L_grad:  [0.2678, 0.7095, -0.0909]
L: [2.3436, 2.0964, 6.1346] L_grad:  [-0.3141, -0.2749, 0.0237]
L: [2.375, 2.1239, 6.1323] L_grad:  [0.0905, -0.2002, -0.0401]
L: [2.366, 2.1439, 6.1363] L_grad:  [0.2194, 0.3099, -0.0293]
L: [2.3441, 2.1129, 6.1392] L_grad:  [-0.2466, -0.3539, 0.3148]
Epoch :  16  time:  18.621  Rel. Train L2 Loss :  0.05801898008584976  Rel. Test L2 Loss :  0.07778897136449814  Test L2 Loss :  4.771173076629639
L: [2.3708, 2.146, 6.1109] L_grad:  [0.2407, 0.4635, -0.2164]
L: [2.3468, 2.0996, 6.1325] L_grad:  [-0.2148, -0.6526, 0.2193]
L: [2.3682, 2.1649, 6.1106] L_grad:  [0.3329, 0.5731, -0.1955]
L: [2.335, 2.1076, 6.1301] L_grad:  [-0.2809, -0.0605, 0.053]
L: [2.363, 2.1136, 6.1248] L_grad:  [-0.0921, -0.0007, 0.2179]
L: [2.3722, 2.1137, 6.103] L_grad:  [0.0244, -0.0743, 0.0632]
L: [2.3698, 2.1211, 6.0967] L_grad:  [0.1054, 0.1693, -0.2834]
L: [2.3593, 2.1042, 6.1251] L_grad:  [-0.1692, -0.3075, -0.0154]
L: [2.3762, 2.135, 6.1266] L_grad:  [0.089, 0.3661, 0.0411]
L: [2.3673, 2.0983, 6.1225] L_grad:  [-0.4936, -0.5608, 0.2236]
Epoch :  17  time:  18.764  Rel. Train L2 Loss :  0.057593602418899534  Rel. Test L2 Loss :  0.07812786191701888  Test L2 Loss :  4.788870429992675
L: [2.3603, 2.1232, 6.1446] L_grad:  [-0.0224, 0.1244, -0.0438]
L: [2.3625, 2.1108, 6.149] L_grad:  [-0.0983, -0.2874, 0.0563]
L: [2.3723, 2.1395, 6.1433] L_grad:  [0.1644, 0.2963, 0.0246]
L: [2.3559, 2.1099, 6.1409] L_grad:  [0.1146, 0.3008, -0.1567]
L: [2.3444, 2.0798, 6.1565] L_grad:  [-0.3251, -0.3508, 0.0358]
L: [2.3769, 2.1149, 6.153] L_grad:  [0.0506, 0.0795, 0.0789]
L: [2.3719, 2.1069, 6.1451] L_grad:  [0.1602, 0.2541, -0.0453]
L: [2.3559, 2.0815, 6.1496] L_grad:  [-0.2294, -0.6444, 0.0172]
L: [2.3788, 2.146, 6.1479] L_grad:  [0.2421, 0.4127, 0.0247]
L: [2.3546, 2.1047, 6.1454] L_grad:  [-0.1328, -0.1171, 0.0976]
Epoch :  18  time:  18.673  Rel. Train L2 Loss :  0.056399951189756395  Rel. Test L2 Loss :  0.07840236485004425  Test L2 Loss :  4.8075339889526365
L: [2.3675, 2.1165, 6.2488] L_grad:  [0.1493, 0.0905, 0.141]
L: [2.3525, 2.1074, 6.2347] L_grad:  [-0.1545, -0.7151, 0.3425]
L: [2.368, 2.1789, 6.2005] L_grad:  [0.0698, 0.8159, -0.0998]
L: [2.361, 2.0974, 6.2105] L_grad:  [0.1462, 0.0011, -0.1148]
L: [2.3464, 2.0972, 6.2219] L_grad:  [-0.1026, 0.0351, -0.1046]
L: [2.3566, 2.0937, 6.2324] L_grad:  [-0.1532, -0.2307, 0.2532]
L: [2.372, 2.1168, 6.2071] L_grad:  [0.2135, 0.1344, 0.068]
L: [2.3506, 2.1034, 6.2003] L_grad:  [-0.291, -0.074, -0.0086]
L: [2.3797, 2.1108, 6.2011] L_grad:  [0.1298, -0.083, 0.084]
L: [2.3667, 2.119, 6.1927] L_grad:  [0.1994, -0.0332, 0.1256]
Epoch :  19  time:  18.878  Rel. Train L2 Loss :  0.05541673895716667  Rel. Test L2 Loss :  0.07572102129459381  Test L2 Loss :  4.644292907714844
L: [2.3668, 2.1125, 6.1585] L_grad:  [0.1804, 0.2704, -0.0536]
L: [2.3488, 2.0855, 6.1639] L_grad:  [-0.3029, -0.2526, 0.0669]
L: [2.3791, 2.1107, 6.1572] L_grad:  [0.2332, 0.1005, -0.1437]
L: [2.3558, 2.1007, 6.1716] L_grad:  [0.0349, -0.0254, -0.0984]
L: [2.3523, 2.1032, 6.1814] L_grad:  [-0.1804, -0.0971, 0.1728]
L: [2.3703, 2.1129, 6.1641] L_grad:  [0.1311, 0.3972, -0.1492]
L: [2.3572, 2.0732, 6.1791] L_grad:  [-0.171, -0.7312, 0.0168]
L: [2.3743, 2.1463, 6.1774] L_grad:  [0.059, 0.3716, -0.0379]
L: [2.3684, 2.1092, 6.1812] L_grad:  [-0.0434, -0.3344, 0.339]
L: [2.3727, 2.1426, 6.1473] L_grad:  [0.1257, 0.3017, -0.1259]
Epoch :  20  time:  18.837  Rel. Train L2 Loss :  0.05411565566062927  Rel. Test L2 Loss :  0.07453037351369858  Test L2 Loss :  4.5731159114837645
L: [2.344, 2.1198, 6.1557] L_grad:  [-0.0186, 0.317, -0.0708]
L: [2.3458, 2.0881, 6.1628] L_grad:  [-0.2429, -0.5035, 0.0968]
L: [2.3701, 2.1385, 6.1531] L_grad:  [0.0677, 0.305, -0.0549]
L: [2.3633, 2.108, 6.1586] L_grad:  [-0.0403, -0.2139, 0.0204]
L: [2.3674, 2.1294, 6.1565] L_grad:  [0.1269, 0.4608, -0.1498]
L: [2.3547, 2.0833, 6.1715] L_grad:  [-0.0349, -0.411, 0.1708]
L: [2.3582, 2.1244, 6.1544] L_grad:  [0.1218, 0.5053, -0.0997]
L: [2.346, 2.0739, 6.1644] L_grad:  [-0.0534, -0.5651, 0.2697]
L: [2.3513, 2.1304, 6.1374] L_grad:  [-0.0622, 0.4135, -0.2778]
L: [2.3575, 2.089, 6.1652] L_grad:  [0.1261, -0.2448, -0.0513]
Epoch :  21  time:  18.807  Rel. Train L2 Loss :  0.05386122733354568  Rel. Test L2 Loss :  0.07696703344583511  Test L2 Loss :  4.716071243286133
L: [2.3532, 2.1068, 6.1633] L_grad:  [0.016, -0.3092, 0.0257]
L: [2.3516, 2.1377, 6.1608] L_grad:  [-0.1926, 0.2684, 0.1234]
L: [2.3709, 2.1108, 6.1484] L_grad:  [0.1097, 0.1551, 0.0872]
L: [2.3599, 2.0953, 6.1397] L_grad:  [0.0685, -0.3776, -0.0511]
L: [2.353, 2.1331, 6.1448] L_grad:  [-0.0347, 0.1884, -0.0865]
L: [2.3565, 2.1142, 6.1535] L_grad:  [0.0479, 0.0591, -0.1322]
L: [2.3517, 2.1083, 6.1667] L_grad:  [0.0769, -0.3007, 0.2643]
L: [2.344, 2.1384, 6.1402] L_grad:  [-0.0001, 0.2867, -0.0433]
L: [2.344, 2.1097, 6.1446] L_grad:  [0.0895, -0.252, -0.0022]
L: [2.3351, 2.1349, 6.1448] L_grad:  [-0.1648, 0.0847, -0.0499]
Epoch :  22  time:  18.783  Rel. Train L2 Loss :  0.05407583057880402  Rel. Test L2 Loss :  0.07795027017593384  Test L2 Loss :  4.7814728546142575
L: [2.3684, 2.1241, 6.162] L_grad:  [-0.1767, -0.1612, 0.1904]
L: [2.3861, 2.1402, 6.143] L_grad:  [0.4781, 0.6125, -0.2094]
L: [2.3383, 2.079, 6.1639] L_grad:  [-0.0487, -0.3158, -0.1465]
L: [2.3431, 2.1106, 6.1786] L_grad:  [-0.1226, -0.1728, -0.0105]
L: [2.3554, 2.1278, 6.1796] L_grad:  [-0.2646, -0.0546, 0.257]
L: [2.3819, 2.1333, 6.1539] L_grad:  [0.2201, 0.3654, 0.0085]
L: [2.3598, 2.0968, 6.1531] L_grad:  [0.1866, -0.2061, 0.0133]
L: [2.3412, 2.1174, 6.1518] L_grad:  [-0.0592, 0.3061, -0.2947]
L: [2.3471, 2.0868, 6.1812] L_grad:  [-0.1486, -0.4142, 0.0836]
L: [2.362, 2.1282, 6.1729] L_grad:  [0.0362, 0.1859, 0.1102]
Epoch :  23  time:  18.979  Rel. Train L2 Loss :  0.05500455951690674  Rel. Test L2 Loss :  0.07735175848007202  Test L2 Loss :  4.746074943542481
L: [2.3615, 2.0928, 6.1889] L_grad:  [0.0786, 0.3207, -0.1467]
L: [2.3536, 2.0607, 6.2036] L_grad:  [0.0069, -0.7438, 0.1638]
L: [2.3529, 2.1351, 6.1872] L_grad:  [0.132, 0.5094, -0.1015]
L: [2.3397, 2.0842, 6.1973] L_grad:  [-0.0855, -0.1802, -0.0157]
L: [2.3483, 2.1022, 6.1989] L_grad:  [0.0078, -0.0229, -0.0209]
L: [2.3475, 2.1045, 6.201] L_grad:  [0.0125, -0.2301, 0.1706]
L: [2.3462, 2.1275, 6.1839] L_grad:  [0.0725, 0.2614, 0.1078]
L: [2.339, 2.1013, 6.1731] L_grad:  [-0.4965, 0.0335, -0.2653]
L: [2.3886, 2.098, 6.1997] L_grad:  [0.0185, -0.0896, -0.1847]
L: [2.3868, 2.1069, 6.2181] L_grad:  [0.0867, -0.435, 0.255]
Epoch :  24  time:  18.985  Rel. Train L2 Loss :  0.05305206161737442  Rel. Test L2 Loss :  0.07570255845785141  Test L2 Loss :  4.640963592529297
L: [2.3534, 2.135, 6.1739] L_grad:  [-0.0436, 0.6784, -0.0257]
L: [2.3577, 2.0672, 6.1765] L_grad:  [0.1109, -0.5192, 0.1316]
L: [2.3467, 2.1191, 6.1633] L_grad:  [0.0581, 0.1222, -0.0287]
L: [2.3408, 2.1069, 6.1662] L_grad:  [0.0646, -0.1571, -0.1483]
L: [2.3344, 2.1226, 6.181] L_grad:  [-0.0068, -0.0072, 0.0619]
L: [2.3351, 2.1233, 6.1748] L_grad:  [-0.3358, 0.116, 0.2456]
L: [2.3686, 2.1117, 6.1503] L_grad:  [0.2972, 0.2787, -0.0302]
L: [2.3389, 2.0839, 6.1533] L_grad:  [-0.0897, -0.2088, -0.1585]
L: [2.3479, 2.1048, 6.1691] L_grad:  [-0.141, 0.094, -0.0836]
L: [2.362, 2.0954, 6.1775] L_grad:  [-0.0221, 0.0199, 0.1546]
Epoch :  25  time:  19.178  Rel. Train L2 Loss :  0.05435054695606232  Rel. Test L2 Loss :  0.07570828765630722  Test L2 Loss :  4.645158882141113
L: [2.3476, 2.1057, 6.1549] L_grad:  [0.2013, 0.2582, -0.1611]
L: [2.3275, 2.0799, 6.171] L_grad:  [-0.1995, -0.3234, 0.0062]
L: [2.3474, 2.1123, 6.1703] L_grad:  [-0.0268, -0.0226, 0.1136]
L: [2.3501, 2.1145, 6.159] L_grad:  [-0.0554, 0.1396, 0.1077]
L: [2.3557, 2.1006, 6.1482] L_grad:  [-0.0763, -0.0512, 0.0705]
L: [2.3633, 2.1057, 6.1412] L_grad:  [0.1601, 0.4346, -0.206]
L: [2.3473, 2.0622, 6.1618] L_grad:  [-0.3117, -1.0613, 0.1107]
L: [2.3784, 2.1683, 6.1507] L_grad:  [0.3845, 1.3726, -0.3108]
L: [2.34, 2.0311, 6.1818] L_grad:  [-0.4649, -1.0901, 0.3304]
L: [2.3865, 2.1401, 6.1487] L_grad:  [0.3832, 0.7191, -0.1511]
Epoch :  26  time:  19.181  Rel. Train L2 Loss :  0.051675253868103024  Rel. Test L2 Loss :  0.07335280478000641  Test L2 Loss :  4.500777807235718
L: [2.3492, 2.109, 6.1488] L_grad:  [-0.0962, -0.1771, -0.0139]
L: [2.3589, 2.1267, 6.1502] L_grad:  [0.1582, 0.3866, -0.0487]
L: [2.343, 2.0881, 6.1551] L_grad:  [-0.0056, -0.4373, 0.2206]
L: [2.3436, 2.1318, 6.133] L_grad:  [0.0765, 0.5574, -0.0001]
L: [2.3359, 2.076, 6.133] L_grad:  [-0.0026, -0.453, -0.0821]
L: [2.3362, 2.1213, 6.1412] L_grad:  [-0.1857, 0.4922, -0.2254]
L: [2.3548, 2.0721, 6.1638] L_grad:  [0.0594, -0.5776, 0.1971]
L: [2.3488, 2.1299, 6.1441] L_grad:  [0.2104, 0.0101, 0.2183]
L: [2.3278, 2.1289, 6.1222] L_grad:  [-0.084, 0.441, -0.1529]
L: [2.3362, 2.0848, 6.1375] L_grad:  [-0.1214, -0.1972, -0.1029]
Epoch :  27  time:  19.196  Rel. Train L2 Loss :  0.05126178205013275  Rel. Test L2 Loss :  0.07677862852811813  Test L2 Loss :  4.710138397216797
L: [2.3595, 2.1216, 6.1486] L_grad:  [0.0432, 0.4668, -0.2951]
L: [2.3552, 2.0749, 6.1781] L_grad:  [-0.1212, -0.5277, 0.2011]
L: [2.3673, 2.1276, 6.158] L_grad:  [0.2264, 0.3043, 0.0152]
L: [2.3447, 2.0972, 6.1564] L_grad:  [-0.1212, -0.1316, 0.0205]
L: [2.3568, 2.1104, 6.1544] L_grad:  [0.0255, 0.1332, -0.0914]
L: [2.3542, 2.097, 6.1635] L_grad:  [-0.0766, -0.1839, 0.2013]
L: [2.3619, 2.1154, 6.1434] L_grad:  [0.3959, 0.1518, -0.1033]
L: [2.3223, 2.1003, 6.1537] L_grad:  [-0.1526, -0.0581, -0.0501]
L: [2.3376, 2.1061, 6.1587] L_grad:  [-0.1881, -0.1405, 0.0958]
L: [2.3564, 2.1201, 6.1492] L_grad:  [0.1245, 0.4077, -0.1189]
Epoch :  28  time:  19.202  Rel. Train L2 Loss :  0.051474119126796725  Rel. Test L2 Loss :  0.07740410208702088  Test L2 Loss :  4.748025493621826
L: [2.3376, 2.1219, 6.1651] L_grad:  [0.0, 0.0016, 0.1496]
L: [2.3376, 2.1217, 6.1502] L_grad:  [-0.4279, 0.3687, -0.2021]
L: [2.3804, 2.0849, 6.1704] L_grad:  [0.306, -0.2964, -0.1235]
L: [2.3498, 2.1145, 6.1828] L_grad:  [-0.0087, -0.1245, -0.0184]
L: [2.3506, 2.127, 6.1846] L_grad:  [0.3364, -0.0018, 0.221]
L: [2.317, 2.1271, 6.1625] L_grad:  [-0.0906, 0.1663, 0.1266]
L: [2.3261, 2.1105, 6.1498] L_grad:  [-0.214, -0.029, -0.0476]
L: [2.3475, 2.1134, 6.1546] L_grad:  [-0.1677, 0.2647, -0.1079]
L: [2.3642, 2.0869, 6.1654] L_grad:  [0.1633, -0.3469, -0.0031]
L: [2.3479, 2.1216, 6.1657] L_grad:  [0.0834, 0.3089, 0.1159]
Epoch :  29  time:  19.662  Rel. Train L2 Loss :  0.050857026636600494  Rel. Test L2 Loss :  0.07372863173484802  Test L2 Loss :  4.519926691055298
L: [2.3397, 2.1162, 6.1475] L_grad:  [-0.085, 0.0105, 0.0292]
L: [2.3482, 2.1151, 6.1446] L_grad:  [0.0826, -0.2374, -0.1299]
L: [2.34, 2.1389, 6.1575] L_grad:  [-0.0863, 0.5789, -0.1305]
L: [2.3486, 2.081, 6.1706] L_grad:  [-0.2382, -0.3334, 0.0918]
L: [2.3724, 2.1143, 6.1614] L_grad:  [0.248, -0.0162, 0.204]
L: [2.3476, 2.1159, 6.141] L_grad:  [0.1187, 0.3593, -0.2583]
L: [2.3357, 2.08, 6.1668] L_grad:  [-0.1468, -0.5755, 0.118]
L: [2.3504, 2.1376, 6.155] L_grad:  [0.2574, 0.6489, -0.0999]
L: [2.3247, 2.0727, 6.165] L_grad:  [-0.2528, -0.4086, 0.1731]
L: [2.3499, 2.1135, 6.1477] L_grad:  [0.21, 0.3178, -0.2168]
Epoch :  30  time:  19.776  Rel. Train L2 Loss :  0.05001631075143814  Rel. Test L2 Loss :  0.07432438969612122  Test L2 Loss :  4.5570018863677975
L: [2.3467, 2.0981, 6.1513] L_grad:  [0.0949, 0.3454, 0.0663]
L: [2.3372, 2.0636, 6.1447] L_grad:  [-0.1577, -0.7342, 0.0454]
L: [2.3529, 2.137, 6.1402] L_grad:  [0.1666, 0.7522, -0.2321]
L: [2.3363, 2.0618, 6.1634] L_grad:  [-0.1759, -0.5355, 0.0634]
L: [2.3539, 2.1153, 6.157] L_grad:  [0.0399, 0.4942, 0.0706]
L: [2.3499, 2.0659, 6.15] L_grad:  [-0.0504, -0.8292, 0.199]
L: [2.3549, 2.1488, 6.1301] L_grad:  [0.1086, 0.4494, -0.3298]
L: [2.3441, 2.1039, 6.1631] L_grad:  [-0.0133, -0.3478, 0.0318]
L: [2.3454, 2.1387, 6.1599] L_grad:  [0.0149, 0.1028, 0.2487]
L: [2.3439, 2.1284, 6.135] L_grad:  [0.2332, 0.3229, -0.0109]
Epoch :  31  time:  19.458  Rel. Train L2 Loss :  0.05030350285768509  Rel. Test L2 Loss :  0.07311727136373519  Test L2 Loss :  4.482731723785401
L: [2.3619, 2.0952, 6.1498] L_grad:  [0.2603, 0.0852, -0.1177]
L: [2.3359, 2.0867, 6.1615] L_grad:  [0.0064, 0.0783, 0.0247]
L: [2.3353, 2.0789, 6.159] L_grad:  [-0.0387, -0.2224, -0.0034]
L: [2.3391, 2.1011, 6.1594] L_grad:  [0.0711, 0.1125, -0.1075]
L: [2.332, 2.0899, 6.1701] L_grad:  [-0.1011, 0.0047, 0.0537]
L: [2.3421, 2.0894, 6.1648] L_grad:  [0.1894, -0.0293, 0.1915]
L: [2.3232, 2.0923, 6.1456] L_grad:  [-0.2734, 0.1313, -0.1376]
L: [2.3505, 2.0792, 6.1594] L_grad:  [0.1334, -0.2969, 0.0303]
L: [2.3372, 2.1089, 6.1564] L_grad:  [-0.0529, -0.0344, -0.0446]
L: [2.3425, 2.1123, 6.1608] L_grad:  [0.1736, 0.1832, -0.0224]
Epoch :  32  time:  19.225  Rel. Train L2 Loss :  0.05032002979516983  Rel. Test L2 Loss :  0.07456143200397491  Test L2 Loss :  4.568965044021606
L: [2.3217, 2.0942, 6.1867] L_grad:  [-0.0265, 0.0664, 0.0043]
L: [2.3243, 2.0876, 6.1863] L_grad:  [-0.1564, -0.6267, 0.1364]
L: [2.34, 2.1502, 6.1726] L_grad:  [-0.2175, 0.3431, 0.0496]
L: [2.3617, 2.1159, 6.1677] L_grad:  [0.0165, 0.3246, -0.031]
L: [2.3601, 2.0835, 6.1708] L_grad:  [0.4837, -0.0042, -0.0311]
L: [2.3117, 2.0839, 6.1739] L_grad:  [-0.2033, 0.1215, -0.0857]
L: [2.332, 2.0717, 6.1825] L_grad:  [0.1124, -0.5793, 0.2161]
L: [2.3208, 2.1297, 6.1608] L_grad:  [-0.4358, 0.8576, -0.1775]
L: [2.3644, 2.0439, 6.1786] L_grad:  [0.2809, -0.835, 0.1396]
L: [2.3363, 2.1274, 6.1646] L_grad:  [-0.4098, 0.639, -0.0435]
Epoch :  33  time:  19.243  Rel. Train L2 Loss :  0.04953648716211319  Rel. Test L2 Loss :  0.07341997683048249  Test L2 Loss :  4.502529850006104
L: [2.3365, 2.0806, 6.1681] L_grad:  [-0.1206, -0.6671, 0.1719]
L: [2.3485, 2.1473, 6.1509] L_grad:  [0.1934, 1.0188, -0.2256]
L: [2.3292, 2.0454, 6.1735] L_grad:  [-0.2673, -1.0651, 0.2186]
L: [2.3559, 2.1519, 6.1516] L_grad:  [0.0562, 0.7203, -0.0859]
L: [2.3503, 2.0799, 6.1602] L_grad:  [0.2182, -0.238, 0.0567]
L: [2.3285, 2.1037, 6.1546] L_grad:  [-0.1567, 0.0147, -0.1515]
L: [2.3441, 2.1022, 6.1697] L_grad:  [-0.0405, 0.1371, 0.0271]
L: [2.3482, 2.0885, 6.167] L_grad:  [-0.0558, -0.2457, 0.1338]
L: [2.3538, 2.1131, 6.1536] L_grad:  [0.0075, 0.2962, 0.0602]
L: [2.353, 2.0835, 6.1476] L_grad:  [0.0374, -0.191, -0.0247]
Epoch :  34  time:  19.226  Rel. Train L2 Loss :  0.05048785787820816  Rel. Test L2 Loss :  0.07810778200626373  Test L2 Loss :  4.79286304473877
L: [2.3356, 2.0719, 6.1793] L_grad:  [-0.1364, -0.6026, 0.0257]
L: [2.3492, 2.1321, 6.1768] L_grad:  [-0.038, 0.3983, 0.0858]
L: [2.353, 2.0923, 6.1682] L_grad:  [0.0135, -0.1769, 0.1626]
L: [2.3517, 2.11, 6.1519] L_grad:  [0.0215, 0.3483, -0.0096]
L: [2.3495, 2.0752, 6.1529] L_grad:  [0.1259, -0.1669, 0.1691]
L: [2.3369, 2.0919, 6.136] L_grad:  [-0.1214, -0.0072, -0.0327]
L: [2.3491, 2.0926, 6.1392] L_grad:  [0.1798, 0.0872, -0.0739]
L: [2.3311, 2.0839, 6.1466] L_grad:  [-0.2009, -0.2924, -0.008]
L: [2.3512, 2.1131, 6.1474] L_grad:  [-0.1034, 0.2064, 0.0642]
L: [2.3615, 2.0925, 6.141] L_grad:  [0.3815, -0.1265, -0.1258]
Epoch :  35  time:  19.187  Rel. Train L2 Loss :  0.05007874682545662  Rel. Test L2 Loss :  0.07395783841609954  Test L2 Loss :  4.537227125167846
L: [2.3529, 2.0971, 6.1842] L_grad:  [0.1723, 0.3565, 0.1323]
L: [2.3357, 2.0614, 6.171] L_grad:  [-0.104, -0.4114, 0.1548]
L: [2.3461, 2.1026, 6.1555] L_grad:  [0.0791, 0.2291, -0.012]
L: [2.3382, 2.0796, 6.1567] L_grad:  [0.0477, -0.1896, 0.0545]
L: [2.3334, 2.0986, 6.1512] L_grad:  [-0.0778, 0.3494, -0.1047]
L: [2.3412, 2.0637, 6.1617] L_grad:  [0.0131, -0.4725, -0.0823]
L: [2.3399, 2.1109, 6.1699] L_grad:  [0.0411, 0.3234, -0.0715]
L: [2.3358, 2.0786, 6.1771] L_grad:  [-0.1519, -0.4989, 0.3681]
L: [2.351, 2.1285, 6.1403] L_grad:  [0.1247, 0.4882, 0.0326]
L: [2.3385, 2.0796, 6.137] L_grad:  [0.067, -0.2382, -0.1974]
Epoch :  36  time:  19.585  Rel. Train L2 Loss :  0.04994026058912277  Rel. Test L2 Loss :  0.07673456609249114  Test L2 Loss :  4.7005775451660154
L: [2.3462, 2.0971, 6.1335] L_grad:  [0.1702, 0.1346, -0.119]
L: [2.3292, 2.0837, 6.1454] L_grad:  [-0.159, -0.5666, 0.0447]
L: [2.3451, 2.1403, 6.1409] L_grad:  [0.1391, 0.5689, -0.0648]
L: [2.3312, 2.0834, 6.1474] L_grad:  [0.1014, 0.1138, 0.0518]
L: [2.3211, 2.0721, 6.1422] L_grad:  [-0.2207, -0.2378, 0.1228]
L: [2.3431, 2.0958, 6.1299] L_grad:  [0.0742, 0.2035, -0.0699]
L: [2.3357, 2.0755, 6.1369] L_grad:  [0.256, -0.385, 0.0258]
L: [2.3101, 2.114, 6.1343] L_grad:  [-0.2467, 0.3912, -0.1263]
L: [2.3348, 2.0749, 6.147] L_grad:  [-0.0534, -0.6945, 0.132]
L: [2.3401, 2.1443, 6.1338] L_grad:  [0.1918, 1.011, -0.3028]
Epoch :  37  time:  19.563  Rel. Train L2 Loss :  0.049410056293010714  Rel. Test L2 Loss :  0.07447937250137329  Test L2 Loss :  4.561207733154297
L: [2.3374, 2.1049, 6.1757] L_grad:  [-0.042, 0.4767, -0.0792]
L: [2.3416, 2.0573, 6.1836] L_grad:  [-0.2029, -0.438, 0.2468]
L: [2.3619, 2.1011, 6.1589] L_grad:  [0.4903, 0.531, -0.168]
L: [2.3129, 2.048, 6.1757] L_grad:  [-0.2445, -0.7816, 0.1721]
L: [2.3373, 2.1261, 6.1585] L_grad:  [0.0577, 0.6436, -0.1027]
L: [2.3316, 2.0617, 6.1688] L_grad:  [-0.0392, -0.8187, 0.234]
L: [2.3355, 2.1436, 6.1454] L_grad:  [0.0345, 1.0557, -0.3576]
L: [2.332, 2.038, 6.1811] L_grad:  [-0.0935, -0.8109, 0.1158]
L: [2.3414, 2.1191, 6.1696] L_grad:  [0.07, 0.2231, 0.0526]
L: [2.3344, 2.0968, 6.1643] L_grad:  [-0.2146, 0.382, -0.1054]
Epoch :  38  time:  19.662  Rel. Train L2 Loss :  0.04978443232178688  Rel. Test L2 Loss :  0.0727604791522026  Test L2 Loss :  4.464331007003784
L: [2.346, 2.1018, 6.1671] L_grad:  [0.2406, 0.2501, -0.0154]
L: [2.3219, 2.0768, 6.1686] L_grad:  [-0.174, -0.3882, -0.0226]
L: [2.3393, 2.1156, 6.1709] L_grad:  [-0.0377, 0.2534, 0.015]
L: [2.3431, 2.0903, 6.1694] L_grad:  [-0.0626, -0.2101, -0.0106]
L: [2.3494, 2.1113, 6.1705] L_grad:  [0.28, 0.2075, 0.0142]
L: [2.3214, 2.0906, 6.169] L_grad:  [0.0193, -0.2555, -0.0222]
L: [2.3194, 2.1161, 6.1713] L_grad:  [-0.0781, 0.4305, -0.1653]
L: [2.3273, 2.0731, 6.1878] L_grad:  [-0.3372, -0.9846, 0.3921]
L: [2.361, 2.1715, 6.1486] L_grad:  [0.5521, 1.1224, -0.2891]
L: [2.3058, 2.0593, 6.1775] L_grad:  [-0.198, -0.4202, 0.1234]
Epoch :  39  time:  19.786  Rel. Train L2 Loss :  0.047860025823116305  Rel. Test L2 Loss :  0.07222237974405289  Test L2 Loss :  4.433057117462158
L: [2.3437, 2.1074, 6.1826] L_grad:  [0.0412, 0.7488, -0.1514]
L: [2.3396, 2.0325, 6.1977] L_grad:  [-0.2566, -1.5641, 0.4255]
L: [2.3652, 2.1889, 6.1552] L_grad:  [0.4304, 1.3433, -0.2297]
L: [2.3222, 2.0546, 6.1781] L_grad:  [-0.1381, -0.7003, 0.3342]
L: [2.336, 2.1246, 6.1447] L_grad:  [0.082, 0.3542, -0.1362]
L: [2.3278, 2.0892, 6.1583] L_grad:  [-0.0999, -0.1581, -0.0449]
L: [2.3378, 2.105, 6.1628] L_grad:  [-0.0426, 0.2723, 0.0079]
L: [2.3421, 2.0777, 6.162] L_grad:  [-0.0693, -0.2428, -0.0094]
L: [2.349, 2.102, 6.163] L_grad:  [0.1565, 0.3544, -0.2109]
L: [2.3333, 2.0666, 6.184] L_grad:  [-0.2186, -0.4193, 0.1242]
Epoch :  40  time:  19.862  Rel. Train L2 Loss :  0.04884510236978531  Rel. Test L2 Loss :  0.0732257443666458  Test L2 Loss :  4.4905108451843265
L: [2.3333, 2.1048, 6.159] L_grad:  [0.1058, 0.0936, -0.0746]
L: [2.3227, 2.0955, 6.1665] L_grad:  [-0.1364, 0.2412, -0.0295]
L: [2.3363, 2.0713, 6.1694] L_grad:  [0.0341, -0.2382, 0.0228]
L: [2.3329, 2.0952, 6.1672] L_grad:  [0.1033, -0.1128, -0.3626]
L: [2.3226, 2.1064, 6.2034] L_grad:  [-0.1232, 0.0645, -0.0242]
L: [2.3349, 2.1, 6.2058] L_grad:  [0.0581, -0.1138, 0.0999]
L: [2.3291, 2.1114, 6.1959] L_grad:  [0.0904, 0.1024, -0.0018]
L: [2.3201, 2.1011, 6.196] L_grad:  [0.049, 0.1022, -0.0261]
L: [2.3152, 2.0909, 6.1986] L_grad:  [-0.1559, -0.3974, 0.285]
L: [2.3308, 2.1306, 6.1701] L_grad:  [0.1932, 0.5921, -0.0429]
Epoch :  41  time:  19.596  Rel. Train L2 Loss :  0.047213672786951065  Rel. Test L2 Loss :  0.07432860940694809  Test L2 Loss :  4.558737392425537
L: [2.337, 2.0881, 6.1502] L_grad:  [0.1347, 0.1538, -0.0146]
L: [2.3235, 2.0727, 6.1516] L_grad:  [-0.1015, -0.3421, -0.0023]
L: [2.3336, 2.1069, 6.1518] L_grad:  [0.1222, 0.445, -0.0902]
L: [2.3214, 2.0624, 6.1609] L_grad:  [-0.1572, -0.4889, 0.1113]
L: [2.3371, 2.1113, 6.1497] L_grad:  [-0.0137, 0.4825, -0.0985]
L: [2.3385, 2.063, 6.1596] L_grad:  [0.0304, -0.3928, -0.0227]
L: [2.3355, 2.1023, 6.1618] L_grad:  [-0.0826, 0.2377, 0.0029]
L: [2.3437, 2.0786, 6.1616] L_grad:  [-0.1224, -0.081, 0.1123]
L: [2.356, 2.0867, 6.1503] L_grad:  [-0.0494, -0.1133, -0.0413]
L: [2.3609, 2.098, 6.1545] L_grad:  [0.2334, 0.2158, 0.0222]
Epoch :  42  time:  19.65  Rel. Train L2 Loss :  0.04844613906741142  Rel. Test L2 Loss :  0.07611487776041032  Test L2 Loss :  4.669615707397461
L: [2.3243, 2.0813, 6.1617] L_grad:  [0.1154, 0.3545, -0.1821]
L: [2.3128, 2.0459, 6.1799] L_grad:  [-0.263, -0.7527, 0.1123]
L: [2.3391, 2.1212, 6.1687] L_grad:  [0.2387, 0.5575, -0.0434]
L: [2.3152, 2.0654, 6.173] L_grad:  [-0.3066, -0.7642, 0.1973]
L: [2.3459, 2.1418, 6.1533] L_grad:  [0.2334, 0.9184, -0.1869]
L: [2.3225, 2.05, 6.172] L_grad:  [-0.1442, -0.587, 0.0904]
L: [2.337, 2.1087, 6.1629] L_grad:  [0.1985, 0.2608, 0.0506]
L: [2.3171, 2.0826, 6.1579] L_grad:  [-0.0687, -0.3397, 0.1446]
L: [2.324, 2.1166, 6.1434] L_grad:  [0.0772, 0.3361, 0.0072]
L: [2.3162, 2.083, 6.1427] L_grad:  [-0.2571, -0.2829, -0.0108]
Epoch :  43  time:  19.641  Rel. Train L2 Loss :  0.04778233703970909  Rel. Test L2 Loss :  0.073071571290493  Test L2 Loss :  4.480187158584595
L: [2.3215, 2.0851, 6.1583] L_grad:  [-0.1479, -0.002, 0.1261]
L: [2.3363, 2.0853, 6.1457] L_grad:  [0.326, 0.1482, -0.0126]
L: [2.3037, 2.0705, 6.147] L_grad:  [-0.1098, 0.245, -0.1866]
L: [2.3147, 2.046, 6.1656] L_grad:  [-0.2077, -0.9282, 0.335]
L: [2.3354, 2.1388, 6.1321] L_grad:  [0.333, 0.9353, -0.2079]
L: [2.3021, 2.0453, 6.1529] L_grad:  [-0.3898, -0.7552, -0.044]
L: [2.3411, 2.1208, 6.1573] L_grad:  [0.1441, 0.2495, -0.0576]
L: [2.3267, 2.0959, 6.1631] L_grad:  [-0.2426, -0.3413, 0.0662]
L: [2.351, 2.13, 6.1565] L_grad:  [0.3108, 0.7337, -0.1416]
L: [2.3199, 2.0566, 6.1706] L_grad:  [-0.0869, -0.7362, 0.3271]
Epoch :  44  time:  19.581  Rel. Train L2 Loss :  0.04798906308412552  Rel. Test L2 Loss :  0.07297275394201279  Test L2 Loss :  4.4755490779876705
L: [2.3231, 2.0792, 6.1987] L_grad:  [-0.1424, -0.0496, 0.1082]
L: [2.3373, 2.0841, 6.1879] L_grad:  [-0.022, -0.0687, -0.0134]
L: [2.3395, 2.091, 6.1892] L_grad:  [0.0905, 0.1578, -0.0859]
L: [2.3305, 2.0752, 6.1978] L_grad:  [0.0166, -0.1652, 0.0156]
L: [2.3288, 2.0917, 6.1963] L_grad:  [-0.0169, 0.0478, 0.3417]
L: [2.3305, 2.087, 6.1621] L_grad:  [0.0197, -0.1411, -0.027]
L: [2.3285, 2.1011, 6.1648] L_grad:  [-0.2464, 0.059, -0.1485]
L: [2.3531, 2.0952, 6.1796] L_grad:  [0.2532, -0.0216, -0.0212]
L: [2.3278, 2.0973, 6.1818] L_grad:  [-0.0359, -0.02, 0.3615]
L: [2.3314, 2.0993, 6.1456] L_grad:  [-0.0254, 0.1307, 0.1283]
Epoch :  45  time:  19.573  Rel. Train L2 Loss :  0.049855976819992064  Rel. Test L2 Loss :  0.07412794321775436  Test L2 Loss :  4.545024518966675
L: [2.3339, 2.0793, 6.1652] L_grad:  [-0.0843, -0.1561, 0.1211]
L: [2.3423, 2.0949, 6.1531] L_grad:  [-0.1716, 0.1639, -0.0289]
L: [2.3595, 2.0785, 6.156] L_grad:  [0.0059, 0.1759, -0.0548]
L: [2.3589, 2.0609, 6.1615] L_grad:  [0.2999, -0.0228, 0.0236]
L: [2.3289, 2.0632, 6.1591] L_grad:  [-0.0175, -0.318, 0.042]
L: [2.3306, 2.095, 6.1549] L_grad:  [-0.104, 0.2433, -0.0074]
L: [2.341, 2.0707, 6.1557] L_grad:  [0.0004, -0.1446, 0.0158]
L: [2.341, 2.0851, 6.1541] L_grad:  [-0.0428, 0.21, -0.4445]
L: [2.3453, 2.0641, 6.1986] L_grad:  [0.0236, -0.4832, 0.0386]
L: [2.3429, 2.1125, 6.1947] L_grad:  [0.387, 0.2313, -0.0503]
Epoch :  46  time:  19.022  Rel. Train L2 Loss :  0.04834399706125259  Rel. Test L2 Loss :  0.07243517398834229  Test L2 Loss :  4.441017055511475
L: [2.3415, 2.0725, 6.16] L_grad:  [-0.0427, -0.2502, 0.0846]
L: [2.3457, 2.0975, 6.1515] L_grad:  [0.4932, 0.7775, -0.2765]
L: [2.2964, 2.0198, 6.1792] L_grad:  [-0.4957, -1.4775, 0.3063]
L: [2.346, 2.1675, 6.1485] L_grad:  [0.1477, 1.218, -0.3117]
L: [2.3312, 2.0457, 6.1797] L_grad:  [0.0304, -1.0187, 0.301]
L: [2.3282, 2.1476, 6.1496] L_grad:  [0.0474, 0.7486, -0.0884]
L: [2.3234, 2.0727, 6.1584] L_grad:  [-0.1333, -0.3926, -0.0399]
L: [2.3367, 2.112, 6.1624] L_grad:  [0.0546, 0.4158, -0.0839]
L: [2.3313, 2.0704, 6.1708] L_grad:  [0.0255, -0.3764, 0.2667]
L: [2.3287, 2.108, 6.1442] L_grad:  [-0.0662, 0.3651, -0.1472]
Epoch :  47  time:  18.925  Rel. Train L2 Loss :  0.04729318618774414  Rel. Test L2 Loss :  0.07400855243206024  Test L2 Loss :  4.538445606231689
L: [2.3232, 2.0865, 6.1894] L_grad:  [0.1419, 0.0483, -0.0283]
L: [2.309, 2.0816, 6.1922] L_grad:  [-0.1369, -0.2116, 0.1214]
L: [2.3227, 2.1028, 6.1801] L_grad:  [-0.1073, 0.217, 0.1666]
L: [2.3334, 2.0811, 6.1634] L_grad:  [-0.0288, -0.129, -0.0386]
L: [2.3363, 2.094, 6.1673] L_grad:  [-0.1382, 0.1067, -0.1601]
L: [2.3501, 2.0833, 6.1833] L_grad:  [0.2239, -0.1493, 0.0033]
L: [2.3278, 2.0983, 6.183] L_grad:  [0.0714, 0.2316, 0.0635]
L: [2.3206, 2.0751, 6.1766] L_grad:  [-0.1741, -0.3213, 0.2297]
L: [2.338, 2.1072, 6.1536] L_grad:  [0.343, 0.752, -0.3043]
L: [2.3037, 2.032, 6.1841] L_grad:  [-0.4715, -1.0095, 0.1455]
Epoch :  48  time:  18.933  Rel. Train L2 Loss :  0.047420527696609496  Rel. Test L2 Loss :  0.07409388929605484  Test L2 Loss :  4.543543815612793
L: [2.339, 2.0959, 6.1625] L_grad:  [0.2729, 0.4085, -0.1971]
L: [2.3117, 2.055, 6.1822] L_grad:  [-0.3513, -0.3936, 0.1051]
L: [2.3469, 2.0944, 6.1717] L_grad:  [0.1241, 0.4449, -0.1275]
L: [2.3345, 2.0499, 6.1844] L_grad:  [-0.2758, -0.5204, 0.2161]
L: [2.362, 2.1019, 6.1628] L_grad:  [0.4204, 0.6049, -0.0722]
L: [2.32, 2.0414, 6.1701] L_grad:  [-0.1924, -0.8528, 0.1631]
L: [2.3392, 2.1267, 6.1537] L_grad:  [0.4063, 0.5882, -0.2082]
L: [2.2986, 2.0679, 6.1746] L_grad:  [-0.3053, -0.4457, 0.0758]
L: [2.3291, 2.1125, 6.167] L_grad:  [0.197, 0.403, -0.038]
L: [2.3094, 2.0722, 6.1708] L_grad:  [-0.1924, 0.2055, -0.0252]
Epoch :  49  time:  18.886  Rel. Train L2 Loss :  0.04837464174628258  Rel. Test L2 Loss :  0.07324756890535354  Test L2 Loss :  4.488189296722412
L: [2.3129, 2.0808, 6.1652] L_grad:  [-0.0777, 0.0301, 0.0423]
L: [2.3207, 2.0778, 6.161] L_grad:  [-0.0473, 0.1521, -0.1241]
L: [2.3254, 2.0625, 6.1734] L_grad:  [-0.1839, -0.605, 0.2295]
L: [2.3438, 2.123, 6.1505] L_grad:  [0.3935, 0.6178, -0.1141]
L: [2.3044, 2.0613, 6.1619] L_grad:  [-0.1756, -0.5627, 0.1087]
L: [2.322, 2.1175, 6.151] L_grad:  [0.053, 0.2933, -0.1709]
L: [2.3167, 2.0882, 6.1681] L_grad:  [-0.0948, -0.1135, -0.0907]
L: [2.3262, 2.0996, 6.1772] L_grad:  [0.0037, -0.0475, 0.1656]
L: [2.3258, 2.1043, 6.1606] L_grad:  [0.0586, -0.0722, 0.1978]
L: [2.3199, 2.1115, 6.1408] L_grad:  [-0.0504, 0.6128, -0.0596]
Epoch :  50  time:  20.276  Rel. Train L2 Loss :  0.04919983553886414  Rel. Test L2 Loss :  0.07285684406757355  Test L2 Loss :  4.465676136016846
L: [2.328, 2.07, 6.1749] L_grad:  [0.0495, 0.155, -0.0305]
L: [2.3231, 2.0545, 6.178] L_grad:  [0.0209, -0.2821, 0.0629]
L: [2.321, 2.0827, 6.1717] L_grad:  [0.1594, 0.0398, -0.0787]
L: [2.3051, 2.0787, 6.1795] L_grad:  [-0.1014, 0.3217, -0.1882]
L: [2.3152, 2.0465, 6.1984] L_grad:  [-0.2065, -0.7383, 0.2252]
L: [2.3358, 2.1204, 6.1758] L_grad:  [0.1992, 0.7596, 0.0279]
L: [2.3159, 2.0444, 6.1731] L_grad:  [-0.0045, -0.6603, 0.0749]
L: [2.3164, 2.1104, 6.1656] L_grad:  [0.0216, 0.5261, -0.1904]
L: [2.3142, 2.0578, 6.1846] L_grad:  [-0.1988, -0.4477, 0.0806]
L: [2.3341, 2.1026, 6.1765] L_grad:  [0.0757, 0.0806, 0.0663]
Epoch :  51  time:  19.049  Rel. Train L2 Loss :  0.0472703018784523  Rel. Test L2 Loss :  0.07343546837568284  Test L2 Loss :  4.501600847244263
L: [2.328, 2.0927, 6.1827] L_grad:  [0.0717, 0.5932, -0.1903]
L: [2.3208, 2.0334, 6.2018] L_grad:  [0.0098, -0.8103, 0.315]
L: [2.3198, 2.1144, 6.1702] L_grad:  [0.0132, 0.5099, 0.1429]
L: [2.3185, 2.0634, 6.156] L_grad:  [-0.0894, -0.0947, -0.2083]
L: [2.3274, 2.0729, 6.1768] L_grad:  [0.107, -0.1076, -0.0593]
L: [2.3167, 2.0836, 6.1827] L_grad:  [-0.0736, -0.1802, 0.0555]
L: [2.3241, 2.1016, 6.1772] L_grad:  [-0.1212, -0.1199, 0.1522]
L: [2.3362, 2.1136, 6.162] L_grad:  [0.0755, 0.4016, -0.1374]
L: [2.3287, 2.0735, 6.1757] L_grad:  [-0.1174, -0.6379, 0.1761]
L: [2.3404, 2.1373, 6.1581] L_grad:  [0.1579, 0.4643, -0.0378]
Epoch :  52  time:  19.041  Rel. Train L2 Loss :  0.046191409945487975  Rel. Test L2 Loss :  0.07437894701957702  Test L2 Loss :  4.559361209869385
L: [2.3276, 2.1002, 6.167] L_grad:  [0.1315, 0.2995, -0.0522]
L: [2.3145, 2.0702, 6.1722] L_grad:  [-0.123, -0.3692, 0.15]
L: [2.3268, 2.1071, 6.1572] L_grad:  [-0.043, 0.8175, -0.1559]
L: [2.3311, 2.0254, 6.1728] L_grad:  [-0.1699, -0.8582, 0.1237]
L: [2.3481, 2.1112, 6.1604] L_grad:  [0.3827, 0.6478, -0.171]
L: [2.3098, 2.0464, 6.1775] L_grad:  [-0.0812, -0.7158, -0.2693]
L: [2.3179, 2.118, 6.2044] L_grad:  [0.1337, 0.2445, 0.1966]
L: [2.3046, 2.0936, 6.1848] L_grad:  [-0.1521, 0.3531, -0.1687]
L: [2.3198, 2.0582, 6.2016] L_grad:  [0.1652, 0.1886, -0.1294]
L: [2.3033, 2.0394, 6.2146] L_grad:  [-0.0089, -0.5266, 0.0759]
Epoch :  53  time:  19.089  Rel. Train L2 Loss :  0.04662459653615952  Rel. Test L2 Loss :  0.07251060545444489  Test L2 Loss :  4.44996826171875
L: [2.3356, 2.0842, 6.1902] L_grad:  [-0.1518, -0.1383, 0.0864]
L: [2.3508, 2.098, 6.1815] L_grad:  [0.3017, 0.2074, 0.0876]
L: [2.3206, 2.0773, 6.1728] L_grad:  [0.1536, -0.1379, 0.1322]
L: [2.3053, 2.0911, 6.1596] L_grad:  [-0.393, 0.2929, -0.2058]
L: [2.3446, 2.0618, 6.1801] L_grad:  [0.3174, -0.4781, 0.0051]
L: [2.3128, 2.1096, 6.1796] L_grad:  [-0.1587, 0.2269, 0.0607]
L: [2.3287, 2.0869, 6.1736] L_grad:  [0.1433, 0.1188, 0.1225]
L: [2.3143, 2.0751, 6.1613] L_grad:  [-0.0589, 0.2238, -0.0021]
L: [2.3202, 2.0527, 6.1615] L_grad:  [-0.0671, -0.3804, 0.0616]
L: [2.3269, 2.0907, 6.1554] L_grad:  [0.1309, 0.42, -0.2173]
Epoch :  54  time:  19.081  Rel. Train L2 Loss :  0.04703125071525574  Rel. Test L2 Loss :  0.07275807350873947  Test L2 Loss :  4.461289367675781
L: [2.3018, 2.0994, 6.1609] L_grad:  [-0.2051, 0.3631, -0.1241]
L: [2.3223, 2.0631, 6.1734] L_grad:  [0.2373, -0.2629, 0.0467]
L: [2.2986, 2.0894, 6.1687] L_grad:  [-0.2748, 0.0485, -0.0542]
L: [2.3261, 2.0845, 6.1741] L_grad:  [0.0335, 0.0895, -0.0627]
L: [2.3227, 2.0756, 6.1804] L_grad:  [0.1171, -0.0963, 0.0236]
L: [2.311, 2.0852, 6.178] L_grad:  [-0.1152, 0.1158, -0.0375]
L: [2.3225, 2.0736, 6.1818] L_grad:  [0.0641, -0.2003, 0.0597]
L: [2.3161, 2.0937, 6.1758] L_grad:  [-0.211, 0.061, -0.0164]
L: [2.3372, 2.0876, 6.1774] L_grad:  [0.1425, -0.1596, 0.1332]
L: [2.323, 2.1035, 6.1641] L_grad:  [-0.201, 0.4054, 0.0885]
Epoch :  55  time:  19.117  Rel. Train L2 Loss :  0.04749181663990021  Rel. Test L2 Loss :  0.0769270321726799  Test L2 Loss :  4.721155853271484
L: [2.3308, 2.0743, 6.1773] L_grad:  [-0.2347, -0.1753, 0.3185]
L: [2.3542, 2.0918, 6.1454] L_grad:  [0.1919, 0.1208, 0.1596]
L: [2.335, 2.0797, 6.1295] L_grad:  [0.1093, 0.1551, -0.1571]
L: [2.3241, 2.0642, 6.1452] L_grad:  [0.1599, 0.0094, -0.142]
L: [2.3081, 2.0633, 6.1594] L_grad:  [-0.2316, -0.6683, 0.0921]
L: [2.3313, 2.1301, 6.1502] L_grad:  [0.0842, 0.4345, -0.0329]
L: [2.3228, 2.0867, 6.1535] L_grad:  [-0.0318, -0.122, 0.1035]
L: [2.326, 2.0989, 6.1431] L_grad:  [0.2458, 0.544, -0.0581]
L: [2.3014, 2.0445, 6.1489] L_grad:  [-0.1574, -0.6367, 0.1819]
L: [2.3172, 2.1082, 6.1307] L_grad:  [-0.1151, 0.295, -0.1007]
Epoch :  56  time:  19.114  Rel. Train L2 Loss :  0.04653669759631157  Rel. Test L2 Loss :  0.07365636706352234  Test L2 Loss :  4.517374544143677
L: [2.324, 2.0656, 6.1835] L_grad:  [-0.0726, -0.183, 0.068]
L: [2.3313, 2.0839, 6.1767] L_grad:  [0.3102, 0.0475, -0.0867]
L: [2.3002, 2.0792, 6.1854] L_grad:  [-0.3428, -0.1143, -0.0347]
L: [2.3345, 2.0906, 6.1889] L_grad:  [0.2492, -0.0336, -0.0239]
L: [2.3096, 2.094, 6.1913] L_grad:  [-0.0504, 0.0985, 0.0934]
L: [2.3146, 2.0841, 6.1819] L_grad:  [-0.0592, 0.1795, 0.101]
L: [2.3205, 2.0662, 6.1718] L_grad:  [-0.0968, -0.0287, 0.4184]
L: [2.3302, 2.069, 6.13] L_grad:  [0.0706, 0.1669, -0.3001]
L: [2.3232, 2.0523, 6.16] L_grad:  [-0.0928, -0.4865, 0.0227]
L: [2.3325, 2.101, 6.1577] L_grad:  [0.3428, 0.3916, -0.0481]
Epoch :  57  time:  19.091  Rel. Train L2 Loss :  0.04566338968276978  Rel. Test L2 Loss :  0.07200446575880051  Test L2 Loss :  4.41961142539978
L: [2.3127, 2.101, 6.165] L_grad:  [-0.0484, 0.3052, -0.0569]
L: [2.3175, 2.0704, 6.1707] L_grad:  [-0.0662, -0.0888, 0.0224]
L: [2.3241, 2.0793, 6.1685] L_grad:  [0.0025, -0.1619, 0.0206]
L: [2.3239, 2.0955, 6.1664] L_grad:  [0.0033, 0.3965, -0.0193]
L: [2.3236, 2.0559, 6.1684] L_grad:  [0.0335, -0.5259, 0.1366]
L: [2.3202, 2.1084, 6.1547] L_grad:  [0.1685, 0.73, -0.5158]
L: [2.3034, 2.0354, 6.2063] L_grad:  [-0.24, -1.1767, 0.2716]
L: [2.3274, 2.1531, 6.1791] L_grad:  [0.1395, 0.7889, -0.052]
L: [2.3134, 2.0742, 6.1843] L_grad:  [0.0211, -0.4157, 0.2562]
L: [2.3113, 2.1158, 6.1587] L_grad:  [0.083, 0.7356, -0.2502]
Epoch :  58  time:  19.055  Rel. Train L2 Loss :  0.04544889181852341  Rel. Test L2 Loss :  0.07298212558031082  Test L2 Loss :  4.472158260345459
L: [2.3182, 2.0698, 6.1762] L_grad:  [0.1074, -0.0372, 0.1505]
L: [2.3075, 2.0735, 6.1611] L_grad:  [-0.2099, 0.1198, -0.1445]
L: [2.3285, 2.0615, 6.1756] L_grad:  [0.0092, -0.2495, -0.1279]
L: [2.3276, 2.0864, 6.1884] L_grad:  [-0.0357, -0.0768, 0.0265]
L: [2.3311, 2.0941, 6.1857] L_grad:  [0.1734, 0.0783, 0.089]
L: [2.3138, 2.0863, 6.1768] L_grad:  [-0.1808, 0.1181, 0.0518]
L: [2.3319, 2.0745, 6.1716] L_grad:  [0.236, 0.0117, 0.0186]
L: [2.3083, 2.0733, 6.1698] L_grad:  [-0.0517, -0.0263, -0.0523]
L: [2.3134, 2.0759, 6.175] L_grad:  [0.0133, -0.5463, 0.1818]
L: [2.3121, 2.1306, 6.1568] L_grad:  [-0.0646, 0.9584, -0.2823]
Epoch :  59  time:  19.138  Rel. Train L2 Loss :  0.04473066508769989  Rel. Test L2 Loss :  0.07262433618307114  Test L2 Loss :  4.454143772125244
L: [2.3353, 2.0917, 6.1579] L_grad:  [0.1155, 0.3581, -0.1069]
L: [2.3238, 2.0559, 6.1686] L_grad:  [0.1735, -0.1921, 0.0523]
L: [2.3064, 2.0751, 6.1633] L_grad:  [-0.0641, 0.0699, -0.0387]
L: [2.3128, 2.0681, 6.1672] L_grad:  [-0.1594, -0.3611, 0.1316]
L: [2.3288, 2.1042, 6.154] L_grad:  [0.1777, 0.5979, -0.1431]
L: [2.311, 2.0444, 6.1684] L_grad:  [-0.1765, -0.557, 0.0996]
L: [2.3287, 2.1001, 6.1584] L_grad:  [0.1804, 0.4707, -0.0875]
L: [2.3106, 2.053, 6.1671] L_grad:  [-0.2126, -0.5682, 0.1004]
L: [2.3319, 2.1099, 6.1571] L_grad:  [0.3571, 0.5287, -0.1807]
L: [2.2962, 2.057, 6.1752] L_grad:  [-0.281, -0.7038, 0.1771]









2,2,5初始L

Casting to tensor
In GeoKNO_train, ndims =  3
L: [2.0, 2.0, 5.0] L_grad:  [-0.0024, 0.0005, 0.0008]
L: [2.0012, 1.9997, 4.9996] L_grad:  [0.0092, -0.04, -0.0067]
L: [1.9966, 2.0197, 5.0029] L_grad:  [0.0931, 0.0243, 0.0322]
L: [1.9501, 2.0076, 4.9868] L_grad:  [-0.1054, -0.2004, -0.0694]
Epoch :  0  time:  17.47  Rel. Train L2 Loss :  0.2956433386802673  Rel. Test L2 Loss :  0.16293056845664977  Test L2 Loss :  10.02695873260498
L: [3.3708, 2.1235, 6.1614] L_grad:  [0.0282, -0.6615, -0.0515]
L: [3.3567, 2.4542, 6.1872] L_grad:  [0.049, 0.4881, 0.0704]
L: [3.3322, 2.2102, 6.152] L_grad:  [0.1412, -0.3588, -0.0277]
L: [3.2617, 2.3896, 6.1658] L_grad:  [0.0282, 0.0192, -0.0012]
Epoch :  1  time:  17.054  Rel. Train L2 Loss :  0.13640533685684203  Rel. Test L2 Loss :  0.12014824867248536  Test L2 Loss :  7.3879138946533205
L: [3.1435, 2.189, 6.507] L_grad:  [-0.08, 0.0508, 0.1372]
L: [3.1834, 2.1636, 6.4384] L_grad:  [0.1146, 0.2945, -0.0393]
L: [3.1261, 2.0164, 6.458] L_grad:  [-0.13, -0.9916, 0.1032]
L: [3.1911, 2.5122, 6.4064] L_grad:  [0.137, 0.6994, -0.0202]
Epoch :  2  time:  16.769  Rel. Train L2 Loss :  0.11083552896976472  Rel. Test L2 Loss :  0.10912240773439408  Test L2 Loss :  6.682932510375976
L: [3.1048, 2.3137, 6.2484] L_grad:  [0.0344, -0.0437, 0.0857]
L: [3.0876, 2.3355, 6.2056] L_grad:  [0.0073, 0.0422, 0.0649]
L: [3.084, 2.3144, 6.1732] L_grad:  [-0.0521, 0.2106, 0.0607]
L: [3.11, 2.2091, 6.1428] L_grad:  [-0.0952, 0.1192, -0.0801]
Epoch :  3  time:  16.875  Rel. Train L2 Loss :  0.09967762076854705  Rel. Test L2 Loss :  0.09852766454219818  Test L2 Loss :  6.039884185791015
L: [3.2547, 2.2541, 6.5518] L_grad:  [-0.1118, -0.0017, 0.0996]
L: [3.3106, 2.2549, 6.502] L_grad:  [0.3252, 0.459, -0.0422]
L: [3.148, 2.0254, 6.5231] L_grad:  [-0.7251, -0.9475, 0.3363]
L: [3.5106, 2.4992, 6.3549] L_grad:  [0.3544, 0.6407, -0.2744]
Epoch :  4  time:  16.836  Rel. Train L2 Loss :  0.09259009110927582  Rel. Test L2 Loss :  0.10035556256771087  Test L2 Loss :  6.1639878463745115
L: [3.1978, 2.2385, 6.1737] L_grad:  [-0.159, 0.1159, -0.1942]
L: [3.2773, 2.1805, 6.2708] L_grad:  [0.0972, 0.1926, 0.1321]
L: [3.2287, 2.0842, 6.2047] L_grad:  [0.0784, -0.2647, -0.0425]
L: [3.1895, 2.2166, 6.226] L_grad:  [0.0132, 0.0893, 0.0401]
Epoch :  5  time:  17.461  Rel. Train L2 Loss :  0.0864751415848732  Rel. Test L2 Loss :  0.09440144300460815  Test L2 Loss :  5.7923406219482425
L: [3.1323, 2.1588, 6.3653] L_grad:  [-0.0375, -0.3367, 0.2567]
L: [3.1511, 2.3272, 6.237] L_grad:  [0.0055, 0.3731, 0.0813]
L: [3.1484, 2.1406, 6.1963] L_grad:  [-0.0747, -0.0944, -0.1731]
L: [3.1857, 2.1878, 6.2829] L_grad:  [0.1062, -0.1592, -0.1495]
Epoch :  6  time:  17.608  Rel. Train L2 Loss :  0.08250936996936799  Rel. Test L2 Loss :  0.0875340062379837  Test L2 Loss :  5.374718093872071
L: [3.1337, 2.1735, 6.3448] L_grad:  [-0.0363, -0.1145, 0.1106]
L: [3.1519, 2.2308, 6.2895] L_grad:  [-0.0599, 0.1791, -0.0515]
L: [3.1818, 2.1412, 6.3153] L_grad:  [0.1408, 0.0175, -0.0315]
L: [3.1114, 2.1325, 6.331] L_grad:  [0.0539, 0.0943, 0.0848]
Epoch :  7  time:  18.066  Rel. Train L2 Loss :  0.07887770354747772  Rel. Test L2 Loss :  0.09559977531433106  Test L2 Loss :  5.8467082595825195
L: [3.039, 2.2888, 6.3065] L_grad:  [-0.0987, 0.4534, 0.1195]
L: [3.0884, 2.062, 6.2467] L_grad:  [0.0942, -0.7514, 0.0477]
L: [3.0413, 2.4377, 6.2228] L_grad:  [-0.4203, 0.243, -0.1958]
L: [3.2514, 2.3162, 6.3208] L_grad:  [0.2985, 0.3417, -0.1675]
Epoch :  8  time:  18.099  Rel. Train L2 Loss :  0.0761864628791809  Rel. Test L2 Loss :  0.08520061194896698  Test L2 Loss :  5.2241560745239255
L: [3.1446, 2.1785, 6.3276] L_grad:  [0.0623, -0.0727, -0.013]
L: [3.1134, 2.2149, 6.3341] L_grad:  [-0.0685, 0.1811, -0.0173]
L: [3.1477, 2.1243, 6.3428] L_grad:  [0.0387, -0.2165, 0.0189]
L: [3.1283, 2.2326, 6.3333] L_grad:  [0.0145, 0.2934, -0.1165]
Epoch :  9  time:  18.23  Rel. Train L2 Loss :  0.07472686970233917  Rel. Test L2 Loss :  0.08865042924880981  Test L2 Loss :  5.4321226882934575
L: [3.3935, 2.1537, 6.3707] L_grad:  [0.1372, 0.0351, 0.1601]
L: [3.3249, 2.1362, 6.2906] L_grad:  [0.0951, -0.0888, -0.0447]
L: [3.2773, 2.1806, 6.3129] L_grad:  [0.1591, 0.1195, -0.0108]
L: [3.1978, 2.1209, 6.3183] L_grad:  [-0.3323, -0.2401, 0.0757]
Epoch :  10  time:  18.445  Rel. Train L2 Loss :  0.07194943416118622  Rel. Test L2 Loss :  0.08363348066806793  Test L2 Loss :  5.128524627685547
L: [3.1133, 2.2284, 6.3717] L_grad:  [-0.2578, -0.057, -0.0998]
L: [3.2422, 2.2569, 6.4216] L_grad:  [0.1008, 0.2603, 0.234]
L: [3.1918, 2.1268, 6.3046] L_grad:  [-0.039, -0.1747, 0.334]
L: [3.2113, 2.2141, 6.1376] L_grad:  [0.2951, 0.1999, -0.2401]
Epoch :  11  time:  18.722  Rel. Train L2 Loss :  0.06965315663814545  Rel. Test L2 Loss :  0.08122047185897827  Test L2 Loss :  4.980302639007569
L: [3.0807, 2.1893, 6.2962] L_grad:  [-0.0343, 0.0691, -0.2882]
L: [3.0978, 2.1547, 6.4403] L_grad:  [-0.0419, -0.1748, 0.1266]
L: [3.1188, 2.2422, 6.377] L_grad:  [0.0237, 0.1565, -0.0452]
L: [3.1069, 2.1639, 6.3996] L_grad:  [-0.0632, -0.2206, 0.0469]
Epoch :  12  time:  18.492  Rel. Train L2 Loss :  0.06822907394170762  Rel. Test L2 Loss :  0.08024449050426483  Test L2 Loss :  4.919908828735352
L: [3.0655, 2.1718, 6.2918] L_grad:  [-0.1424, -0.1897, -0.0407]
L: [3.1367, 2.2666, 6.3121] L_grad:  [0.1822, 0.2934, -0.2107]
L: [3.0456, 2.1199, 6.4175] L_grad:  [-0.1328, -0.4958, 0.1468]
L: [3.112, 2.3678, 6.3441] L_grad:  [0.0084, 0.4921, 0.1751]
Epoch :  13  time:  18.583  Rel. Train L2 Loss :  0.06585435032844543  Rel. Test L2 Loss :  0.07979369461536408  Test L2 Loss :  4.8922460174560545
L: [3.0808, 2.1038, 6.2548] L_grad:  [-0.0759, -0.2071, -0.0822]
L: [3.1187, 2.2074, 6.2959] L_grad:  [0.0208, 0.2905, -0.0784]
L: [3.1083, 2.0621, 6.3351] L_grad:  [0.0117, -0.3634, 0.0869]
L: [3.1025, 2.2438, 6.2917] L_grad:  [0.0095, 0.3541, -0.158]
Epoch :  14  time:  18.522  Rel. Train L2 Loss :  0.06355055916309357  Rel. Test L2 Loss :  0.07902860224246978  Test L2 Loss :  4.845385761260986
L: [3.0987, 2.3165, 6.3484] L_grad:  [-0.0628, 0.4346, 0.2748]
L: [3.1301, 2.0992, 6.211] L_grad:  [0.0477, -0.4831, -0.0773]
L: [3.1063, 2.3407, 6.2496] L_grad:  [-0.0599, 0.7719, -0.1344]
L: [3.1362, 1.9548, 6.3168] L_grad:  [0.0258, -0.8609, 0.1621]
Epoch :  15  time:  18.485  Rel. Train L2 Loss :  0.06355019867420196  Rel. Test L2 Loss :  0.08458745539188385  Test L2 Loss :  5.1809517288208005
L: [3.0906, 2.2764, 6.3139] L_grad:  [-0.0141, 0.4074, -0.0605]
L: [3.0976, 2.0727, 6.3441] L_grad:  [0.1215, -0.6026, -0.0445]
L: [3.0369, 2.374, 6.3664] L_grad:  [-0.1014, 0.3711, -0.0071]
L: [3.0876, 2.1885, 6.37] L_grad:  [0.1279, -0.1163, 0.2032]
Epoch :  16  time:  18.546  Rel. Train L2 Loss :  0.06348116481304168  Rel. Test L2 Loss :  0.07896896749734879  Test L2 Loss :  4.84957857131958
L: [3.1528, 2.1879, 6.2732] L_grad:  [0.1889, 0.1698, -0.1769]
L: [3.0584, 2.103, 6.3616] L_grad:  [-0.166, -0.3954, 0.2264]
L: [3.1414, 2.3007, 6.2484] L_grad:  [0.2881, 0.576, -0.1044]
L: [2.9974, 2.0127, 6.3006] L_grad:  [-0.2425, -0.6882, -0.006]
Epoch :  17  time:  18.554  Rel. Train L2 Loss :  0.06499799555540085  Rel. Test L2 Loss :  0.08059103369712829  Test L2 Loss :  4.938438930511475
L: [3.1301, 2.1895, 6.4319] L_grad:  [0.183, 0.3329, 0.2465]
L: [3.0386, 2.023, 6.3087] L_grad:  [-0.1569, -0.3733, -0.039]
L: [3.1171, 2.2097, 6.3282] L_grad:  [0.0101, 0.2709, -0.0045]
L: [3.112, 2.0742, 6.3304] L_grad:  [0.0787, -0.0273, -0.101]
Epoch :  18  time:  18.557  Rel. Train L2 Loss :  0.0651082063317299  Rel. Test L2 Loss :  0.08208957016468048  Test L2 Loss :  5.041997928619384
L: [3.0976, 2.0715, 6.3899] L_grad:  [-0.075, -0.2857, -0.0493]
L: [3.1351, 2.2144, 6.4145] L_grad:  [0.0379, 0.0369, -0.0594]
L: [3.1161, 2.1959, 6.4442] L_grad:  [0.185, 0.2086, 0.1007]
L: [3.0236, 2.0916, 6.3939] L_grad:  [-0.242, 0.0434, 0.0132]
Epoch :  19  time:  18.648  Rel. Train L2 Loss :  0.06229566815495491  Rel. Test L2 Loss :  0.07549500733613967  Test L2 Loss :  4.633564348220825
L: [3.0747, 2.1634, 6.3315] L_grad:  [-0.0937, -0.037, 0.0592]
L: [3.1216, 2.1819, 6.3019] L_grad:  [0.0754, 0.0691, 0.0187]
L: [3.0839, 2.1473, 6.2925] L_grad:  [-0.0685, -0.1088, 0.1231]
L: [3.1181, 2.2017, 6.231] L_grad:  [0.1884, 0.3089, -0.1849]
Epoch :  20  time:  18.644  Rel. Train L2 Loss :  0.060467491209506986  Rel. Test L2 Loss :  0.08224178314208984  Test L2 Loss :  5.041387729644775
L: [3.1944, 2.0608, 6.3817] L_grad:  [-0.1483, -0.3139, 0.2058]
L: [3.2685, 2.2177, 6.2788] L_grad:  [0.476, 0.2666, -0.1966]
L: [3.0306, 2.0844, 6.3771] L_grad:  [-0.0965, -0.2986, 0.2521]
L: [3.0788, 2.2337, 6.251] L_grad:  [0.105, 0.3313, -0.1304]
Epoch :  21  time:  18.595  Rel. Train L2 Loss :  0.06094210016727448  Rel. Test L2 Loss :  0.0950979644060135  Test L2 Loss :  5.820069274902344
L: [3.0197, 2.1242, 6.3134] L_grad:  [-0.0568, -0.1042, -0.1002]
L: [3.0481, 2.1763, 6.3635] L_grad:  [-0.1097, -0.0888, 0.0826]
L: [3.103, 2.2207, 6.3223] L_grad:  [-0.0761, 0.1681, 0.0871]
L: [3.141, 2.1366, 6.2787] L_grad:  [-0.1551, -0.1273, 0.1842]
Epoch :  22  time:  18.804  Rel. Train L2 Loss :  0.06480743211507797  Rel. Test L2 Loss :  0.07785514950752258  Test L2 Loss :  4.774426193237304
L: [3.1132, 2.1268, 6.2607] L_grad:  [-0.0158, -0.1364, -0.0726]
L: [3.1211, 2.195, 6.297] L_grad:  [0.1231, 0.0837, -0.0907]
L: [3.0596, 2.1531, 6.3423] L_grad:  [-0.0819, -0.1394, -0.0259]
L: [3.1005, 2.2228, 6.3553] L_grad:  [0.0645, 0.1439, 0.1134]
Epoch :  23  time:  18.76  Rel. Train L2 Loss :  0.058775571525096895  Rel. Test L2 Loss :  0.07687491923570633  Test L2 Loss :  4.712856140136719
L: [3.0342, 2.1684, 6.3632] L_grad:  [-0.0393, -0.077, -0.0176]
L: [3.0538, 2.2069, 6.372] L_grad:  [0.0179, 0.1907, 0.1478]
L: [3.0449, 2.1115, 6.2981] L_grad:  [-0.0928, -0.2929, -0.1029]
L: [3.0913, 2.258, 6.3495] L_grad:  [0.1546, 0.1036, -0.018]
Epoch :  24  time:  18.676  Rel. Train L2 Loss :  0.05814378082752228  Rel. Test L2 Loss :  0.07462570309638977  Test L2 Loss :  4.570363073348999
L: [3.0751, 2.1752, 6.314] L_grad:  [-0.006, 0.2202, 0.1073]
L: [3.0781, 2.0651, 6.2604] L_grad:  [0.09, -0.0642, -0.1782]
L: [3.0331, 2.0972, 6.3495] L_grad:  [-0.0214, -0.2044, 0.0514]
L: [3.0438, 2.1994, 6.3238] L_grad:  [0.0045, 0.0094, -0.0382]
Epoch :  25  time:  18.626  Rel. Train L2 Loss :  0.05823985150456429  Rel. Test L2 Loss :  0.07676319599151611  Test L2 Loss :  4.704128971099854
L: [3.0548, 2.1747, 6.3312] L_grad:  [-0.0186, 0.1419, -0.0105]
L: [3.0641, 2.1038, 6.3365] L_grad:  [-0.0771, -0.1565, 0.0861]
L: [3.1027, 2.182, 6.2934] L_grad:  [0.3131, 0.3274, -0.0505]
L: [2.9461, 2.0183, 6.3187] L_grad:  [-0.208, -0.5978, -0.0012]
Epoch :  26  time:  18.651  Rel. Train L2 Loss :  0.06053213030099869  Rel. Test L2 Loss :  0.07749178558588028  Test L2 Loss :  4.7515308380126955
L: [3.0087, 2.1464, 6.3314] L_grad:  [-0.2439, -0.0642, 0.1162]
L: [3.1307, 2.1785, 6.2733] L_grad:  [0.2214, 0.0995, -0.0905]
L: [3.0199, 2.1288, 6.3186] L_grad:  [-0.0185, -0.0252, -0.0246]
L: [3.0292, 2.1414, 6.3309] L_grad:  [-0.1844, -0.0124, 0.0684]
Epoch :  27  time:  18.658  Rel. Train L2 Loss :  0.0576447132229805  Rel. Test L2 Loss :  0.07720974147319794  Test L2 Loss :  4.734180488586426
L: [3.0431, 2.0854, 6.3239] L_grad:  [0.0101, -0.1303, -0.0911]
L: [3.038, 2.1505, 6.3695] L_grad:  [-0.1327, 0.0764, 0.0636]
L: [3.1044, 2.1123, 6.3377] L_grad:  [0.1085, -0.163, 0.0828]
L: [3.0501, 2.1939, 6.2963] L_grad:  [-0.0015, 0.1584, -0.1049]
Epoch :  28  time:  18.643  Rel. Train L2 Loss :  0.05625369966030121  Rel. Test L2 Loss :  0.07497646600008011  Test L2 Loss :  4.599243831634522
L: [3.0273, 2.1758, 6.3547] L_grad:  [-0.0682, 0.016, 0.0081]
L: [3.0614, 2.1678, 6.3507] L_grad:  [0.145, 0.0545, 0.0286]
L: [2.9889, 2.1406, 6.3364] L_grad:  [-0.2354, 0.0496, -0.0101]
L: [3.1066, 2.1158, 6.3414] L_grad:  [0.1214, -0.0917, 0.0294]
Epoch :  29  time:  18.658  Rel. Train L2 Loss :  0.06376337844133377  Rel. Test L2 Loss :  0.07670967400074005  Test L2 Loss :  4.701736507415771
L: [3.0351, 2.1686, 6.3195] L_grad:  [-0.0077, 0.1767, -0.0416]
L: [3.039, 2.0803, 6.3403] L_grad:  [-0.0308, -0.1584, 0.1698]
L: [3.0544, 2.1595, 6.2554] L_grad:  [0.0915, 0.1146, -0.1779]
L: [3.0087, 2.1022, 6.3444] L_grad:  [-0.0026, -0.0819, -0.0482]
Epoch :  30  time:  18.608  Rel. Train L2 Loss :  0.05706742697954178  Rel. Test L2 Loss :  0.07770707428455353  Test L2 Loss :  4.7642104530334475
L: [3.0563, 2.135, 6.3946] L_grad:  [-0.0247, -0.1999, 0.0038]
L: [3.0687, 2.2349, 6.3927] L_grad:  [0.2751, 0.2617, 0.2922]
L: [2.9312, 2.1041, 6.2466] L_grad:  [-0.3182, 0.0281, -0.0535]
L: [3.0903, 2.0901, 6.2734] L_grad:  [0.1584, 0.0248, -0.0702]
Epoch :  31  time:  18.639  Rel. Train L2 Loss :  0.056374996811151504  Rel. Test L2 Loss :  0.07327005296945571  Test L2 Loss :  4.493134593963623
L: [3.0561, 2.0939, 6.3147] L_grad:  [0.0787, 0.0079, -0.224]
L: [3.0167, 2.0899, 6.4267] L_grad:  [-0.0396, -0.1876, 0.1529]
L: [3.0365, 2.1837, 6.3502] L_grad:  [0.0001, 0.2115, -0.0925]
L: [3.0365, 2.0779, 6.3965] L_grad:  [-0.0409, -0.286, 0.1332]
Epoch :  32  time:  18.683  Rel. Train L2 Loss :  0.05962547942996025  Rel. Test L2 Loss :  0.08202972054481507  Test L2 Loss :  5.026936359405518
L: [2.9383, 2.1167, 6.2859] L_grad:  [-0.0339, -0.1744, -0.0941]
L: [2.9552, 2.2039, 6.333] L_grad:  [-0.2505, 0.3605, -0.117]
L: [3.0805, 2.0236, 6.3915] L_grad:  [0.0863, -0.4085, 0.0176]
L: [3.0373, 2.2278, 6.3827] L_grad:  [-0.1025, 0.2863, 0.1482]
Epoch :  33  time:  18.599  Rel. Train L2 Loss :  0.056723529994487765  Rel. Test L2 Loss :  0.07473820984363556  Test L2 Loss :  4.585512313842774
L: [3.02, 2.0978, 6.3033] L_grad:  [-0.0417, -0.0647, 0.0516]
L: [3.0409, 2.1302, 6.2775] L_grad:  [0.205, 0.1263, -0.1002]
L: [2.9384, 2.067, 6.3276] L_grad:  [-0.5281, -0.5113, 0.1124]
L: [3.2025, 2.3227, 6.2714] L_grad:  [0.4987, 0.4082, -0.181]
Epoch :  34  time:  18.658  Rel. Train L2 Loss :  0.05890254241228104  Rel. Test L2 Loss :  0.07761249482631684  Test L2 Loss :  4.755434875488281
L: [3.0479, 2.111, 6.3374] L_grad:  [0.0016, -0.1894, 0.0258]
L: [3.0471, 2.2057, 6.3245] L_grad:  [0.0563, 0.3202, 0.0146]
L: [3.019, 2.0456, 6.3172] L_grad:  [-0.0527, -0.4292, 0.079]
L: [3.0453, 2.2602, 6.2777] L_grad:  [-0.1455, 0.5655, -0.0963]
Epoch :  35  time:  18.621  Rel. Train L2 Loss :  0.05532355710864067  Rel. Test L2 Loss :  0.07678927958011628  Test L2 Loss :  4.71036810874939
L: [3.0116, 2.1267, 6.4114] L_grad:  [-0.0269, -0.0349, 0.0025]
L: [3.0251, 2.1442, 6.4102] L_grad:  [-0.0438, 0.0942, 0.1128]
L: [3.047, 2.0971, 6.3538] L_grad:  [0.0361, -0.0217, 0.1683]
L: [3.0289, 2.1079, 6.2696] L_grad:  [0.007, 0.1682, 0.1063]
Epoch :  36  time:  18.644  Rel. Train L2 Loss :  0.055133337378501894  Rel. Test L2 Loss :  0.0742825049161911  Test L2 Loss :  4.553905925750732
L: [3.0182, 2.1268, 6.3164] L_grad:  [0.024, -0.0811, 0.0381]
L: [3.0063, 2.1673, 6.2973] L_grad:  [0.0626, 0.255, -0.114]
L: [2.975, 2.0399, 6.3543] L_grad:  [-0.0674, -0.5127, 0.0678]
L: [3.0087, 2.2962, 6.3204] L_grad:  [-0.1203, 0.5521, 0.0214]
Epoch :  37  time:  18.632  Rel. Train L2 Loss :  0.05341113787889481  Rel. Test L2 Loss :  0.07761198341846466  Test L2 Loss :  4.752283153533935
L: [2.9742, 2.2795, 6.3761] L_grad:  [0.0425, 0.0535, 0.1889]
L: [2.9529, 2.2528, 6.2816] L_grad:  [-0.2319, 0.2727, -0.2528]
L: [3.0689, 2.1165, 6.408] L_grad:  [0.0787, -0.1272, 0.1444]
L: [3.0295, 2.1801, 6.3358] L_grad:  [-0.028, 0.2023, -0.0737]
Epoch :  38  time:  18.585  Rel. Train L2 Loss :  0.05590123927593231  Rel. Test L2 Loss :  0.07921510398387908  Test L2 Loss :  4.86681001663208
L: [3.0536, 2.1593, 6.3233] L_grad:  [0.1431, 0.2927, -0.0866]
L: [2.9821, 2.0129, 6.3666] L_grad:  [-0.0876, -0.4326, 0.0421]
L: [3.0259, 2.2292, 6.3455] L_grad:  [0.0578, 0.3089, 0.0642]
L: [2.997, 2.0747, 6.3134] L_grad:  [-0.1349, -0.2187, 0.0052]
Epoch :  39  time:  18.629  Rel. Train L2 Loss :  0.054949172765016555  Rel. Test L2 Loss :  0.07406629413366318  Test L2 Loss :  4.539394273757934
L: [3.0602, 2.1976, 6.3553] L_grad:  [-0.1557, 0.1462, -0.0322]
L: [3.138, 2.1245, 6.3714] L_grad:  [0.0301, -0.1399, 0.1781]
L: [3.123, 2.1945, 6.2824] L_grad:  [0.1657, 0.2746, -0.1069]
L: [3.0401, 2.0572, 6.3358] L_grad:  [0.076, -0.1168, 0.0515]
Epoch :  40  time:  18.67  Rel. Train L2 Loss :  0.052999317586421964  Rel. Test L2 Loss :  0.07592573404312133  Test L2 Loss :  4.645053596496582
L: [3.1087, 2.1317, 6.2615] L_grad:  [0.0307, 0.0837, 0.0149]
L: [3.0934, 2.0899, 6.2541] L_grad:  [0.1157, -0.0761, -0.2187]
L: [3.0355, 2.1279, 6.3634] L_grad:  [-0.0529, 0.0938, 0.0348]
L: [3.062, 2.081, 6.346] L_grad:  [0.0334, -0.1389, -0.4184]
Epoch :  41  time:  18.612  Rel. Train L2 Loss :  0.05325418546795845  Rel. Test L2 Loss :  0.07299302875995636  Test L2 Loss :  4.469869632720947
L: [3.0198, 2.1284, 6.272] L_grad:  [0.0401, 0.0315, -0.1614]
L: [2.9997, 2.1127, 6.3527] L_grad:  [0.0348, 0.0098, -0.0053]
L: [2.9823, 2.1078, 6.3553] L_grad:  [-0.0742, -0.0909, 0.0247]
L: [3.0194, 2.1532, 6.3429] L_grad:  [0.1117, 0.1124, 0.0265]
Epoch :  42  time:  18.773  Rel. Train L2 Loss :  0.05317040485143661  Rel. Test L2 Loss :  0.08915956079959869  Test L2 Loss :  5.467341594696045
L: [3.0182, 2.2096, 6.4447] L_grad:  [0.1183, 0.4177, -0.0771]
L: [2.959, 2.0007, 6.4833] L_grad:  [-0.0346, -0.3203, -0.0227]
L: [2.9763, 2.1609, 6.4947] L_grad:  [0.0012, 0.2083, 0.0361]
L: [2.9757, 2.0567, 6.4766] L_grad:  [-0.3056, -0.2068, 0.2292]



PS C:\Users\15461\Desktop\mygithub2\test_mygeokno2> cd "c:\Users\15461\Desktop\mygithub2\test_mygeokno2"
PS C:\Users\15461\Desktop\mygithub2\test_mygeokno2> python -u "c:\Users\15461\Desktop\mygithub2\test_mygeokno2\geokno_learningmodes_car_test.py"
Casting to tensor
In GeoKNO_train, ndims =  3
Epoch :  0  time:  17.278  Rel. Train L2 Loss :  0.2837498044967651  Rel. Test L2 Loss :  0.14985588014125825  Test L2 Loss :  9.201900863647461
Epoch :  1  time:  17.231  Rel. Train L2 Loss :  0.11897340416908264  Rel. Test L2 Loss :  0.114022778570652  Test L2 Loss :  7.003857002258301
Epoch :  2  time:  17.419  Rel. Train L2 Loss :  0.09646417689323425  Rel. Test L2 Loss :  0.10599780261516571  Test L2 Loss :  6.493884468078614
Epoch :  3  time:  16.55  Rel. Train L2 Loss :  0.0861667994260788  Rel. Test L2 Loss :  0.09681225895881652  Test L2 Loss :  5.935165195465088
Epoch :  4  time:  16.593  Rel. Train L2 Loss :  0.07644155639410019  Rel. Test L2 Loss :  0.09753370136022568  Test L2 Loss :  5.973764705657959
Epoch :  5  time:  16.728  Rel. Train L2 Loss :  0.07193825286626816  Rel. Test L2 Loss :  0.09160256147384643  Test L2 Loss :  5.61517105102539
Epoch :  6  time:  16.826  Rel. Train L2 Loss :  0.06785828977823258  Rel. Test L2 Loss :  0.08779162079095841  Test L2 Loss :  5.380769386291504
Epoch :  7  time:  17.346  Rel. Train L2 Loss :  0.06520787620544434  Rel. Test L2 Loss :  0.08652460157871246  Test L2 Loss :  5.301186790466309
Epoch :  8  time:  17.324  Rel. Train L2 Loss :  0.062232492327690125  Rel. Test L2 Loss :  0.08310004681348801  Test L2 Loss :  5.094619636535644
Epoch :  9  time:  17.025  Rel. Train L2 Loss :  0.06298526895046234  Rel. Test L2 Loss :  0.08827155411243438  Test L2 Loss :  5.402032661437988
Epoch :  10  time:  17.245  Rel. Train L2 Loss :  0.06213925734162331  Rel. Test L2 Loss :  0.08261402428150177  Test L2 Loss :  5.062629928588867
Epoch :  11  time:  17.217  Rel. Train L2 Loss :  0.06310645723342896  Rel. Test L2 Loss :  0.08348291397094726  Test L2 Loss :  5.124223251342773
Epoch :  12  time:  17.3  Rel. Train L2 Loss :  0.057540706753730776  Rel. Test L2 Loss :  0.08228151142597198  Test L2 Loss :  5.040807514190674
Epoch :  13  time:  17.445  Rel. Train L2 Loss :  0.05786734575033188  Rel. Test L2 Loss :  0.08478495001792907  Test L2 Loss :  5.192633247375488
Epoch :  14  time:  17.513  Rel. Train L2 Loss :  0.057168017417192456  Rel. Test L2 Loss :  0.08120072066783905  Test L2 Loss :  4.982290458679199
Epoch :  15  time:  17.484  Rel. Train L2 Loss :  0.05524412137269974  Rel. Test L2 Loss :  0.07977415174245835  Test L2 Loss :  4.893354339599609
Epoch :  16  time:  17.648  Rel. Train L2 Loss :  0.05313704618811607  Rel. Test L2 Loss :  0.07758534252643585  Test L2 Loss :  4.755111999511719
Epoch :  17  time:  17.762  Rel. Train L2 Loss :  0.053346004247665404  Rel. Test L2 Loss :  0.07910459637641906  Test L2 Loss :  4.847018547058106
Epoch :  18  time:  17.645  Rel. Train L2 Loss :  0.051743712246418  Rel. Test L2 Loss :  0.07757114470005036  Test L2 Loss :  4.7504401206970215
Epoch :  19  time:  17.696  Rel. Train L2 Loss :  0.052333844900131224  Rel. Test L2 Loss :  0.07846818774938584  Test L2 Loss :  4.808265037536621
Epoch :  20  time:  17.719  Rel. Train L2 Loss :  0.05218652355670929  Rel. Test L2 Loss :  0.07723628461360932  Test L2 Loss :  4.735176982879639
Epoch :  21  time:  18.077  Rel. Train L2 Loss :  0.050368101119995116  Rel. Test L2 Loss :  0.07668756306171418  Test L2 Loss :  4.702638568878174