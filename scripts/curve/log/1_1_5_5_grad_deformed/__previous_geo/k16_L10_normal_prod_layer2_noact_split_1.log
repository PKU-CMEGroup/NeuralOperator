x1 = speconv(fn, bases_c, bases_s, bases_0, wbases_c, wbases_s, wbases_0)
x2 = w(f)
x3 = gw(self.softsign(compute_gradient(f, directed_edges, edge_gradient_weights)))



(1000,) (1000, 1000, 1) (1000, 1000, 2)
use normalized raw measures
Casting to tensor
x_train shape torch.Size([900, 1000, 8]), y_train shape torch.Size([900, 1000, 1])
length of each dim:  tensor([6.5573987960815430, 6.1420288085937500])
kmax = 16
L = 10
geo_dims = [1, 2, 3, 4]
In PCNO_train, ndims =  2
Epoch :  0  Time:  1.967  Rel. Train L2 Loss :  0.5013655349943373  Rel. Test L2 Loss :  0.30411194801330566  Test L2 Loss :  0.43848814606666564  inv_L_scale:  [1.0, 1.0]
Epoch :  1  Time:  1.249  Rel. Train L2 Loss :  0.25005308614836796  Rel. Test L2 Loss :  0.20971998691558838  Test L2 Loss :  0.30140754222869875  inv_L_scale:  [1.0, 1.0]
Epoch :  2  Time:  1.239  Rel. Train L2 Loss :  0.20792169060972002  Rel. Test L2 Loss :  0.19022030830383302  Test L2 Loss :  0.2744140803813934  inv_L_scale:  [1.0, 1.0]
Epoch :  3  Time:  1.274  Rel. Train L2 Loss :  0.1950643351342943  Rel. Test L2 Loss :  0.18742980182170868  Test L2 Loss :  0.2698414492607117  inv_L_scale:  [1.0, 1.0]
Epoch :  4  Time:  1.27  Rel. Train L2 Loss :  0.18882742365201313  Rel. Test L2 Loss :  0.18189301013946532  Test L2 Loss :  0.2630780816078186  inv_L_scale:  [1.0, 1.0]
Epoch :  5  Time:  1.25  Rel. Train L2 Loss :  0.18305451770623524  Rel. Test L2 Loss :  0.17625173211097717  Test L2 Loss :  0.2543237805366516  inv_L_scale:  [1.0, 1.0]
Epoch :  6  Time:  1.263  Rel. Train L2 Loss :  0.18238363365332286  Rel. Test L2 Loss :  0.1808402121067047  Test L2 Loss :  0.2610048043727875  inv_L_scale:  [1.0, 1.0]
Epoch :  7  Time:  1.265  Rel. Train L2 Loss :  0.178060552544064  Rel. Test L2 Loss :  0.1746281337738037  Test L2 Loss :  0.2509600138664246  inv_L_scale:  [1.0, 1.0]
Epoch :  8  Time:  1.253  Rel. Train L2 Loss :  0.17480378813213773  Rel. Test L2 Loss :  0.17741010308265687  Test L2 Loss :  0.2549291479587555  inv_L_scale:  [1.0, 1.0]
Epoch :  9  Time:  1.279  Rel. Train L2 Loss :  0.17376314805613624  Rel. Test L2 Loss :  0.17470494151115418  Test L2 Loss :  0.2520327985286713  inv_L_scale:  [1.0, 1.0]
Epoch :  10  Time:  1.268  Rel. Train L2 Loss :  0.1734054035610623  Rel. Test L2 Loss :  0.17333646774291991  Test L2 Loss :  0.24937291502952574  inv_L_scale:  [1.0, 1.0]
Epoch :  11  Time:  1.24  Rel. Train L2 Loss :  0.17243762705061172  Rel. Test L2 Loss :  0.17293064653873444  Test L2 Loss :  0.2490321433544159  inv_L_scale:  [1.0, 1.0]
Epoch :  12  Time:  1.235  Rel. Train L2 Loss :  0.17344622506035698  Rel. Test L2 Loss :  0.1711053454875946  Test L2 Loss :  0.24613796830177306  inv_L_scale:  [1.0, 1.0]
Epoch :  13  Time:  1.234  Rel. Train L2 Loss :  0.1705927383237415  Rel. Test L2 Loss :  0.16920757472515105  Test L2 Loss :  0.24365049123764038  inv_L_scale:  [1.0, 1.0]
Epoch :  14  Time:  1.237  Rel. Train L2 Loss :  0.17042513284418317  Rel. Test L2 Loss :  0.17283772826194763  Test L2 Loss :  0.24847623229026794  inv_L_scale:  [1.0, 1.0]
Epoch :  15  Time:  1.233  Rel. Train L2 Loss :  0.16994713220331403  Rel. Test L2 Loss :  0.16603386163711548  Test L2 Loss :  0.24064725756645203  inv_L_scale:  [1.0, 1.0]
Epoch :  16  Time:  1.236  Rel. Train L2 Loss :  0.16879772120051914  Rel. Test L2 Loss :  0.16695685863494872  Test L2 Loss :  0.24049246311187744  inv_L_scale:  [1.0, 1.0]
Epoch :  17  Time:  1.233  Rel. Train L2 Loss :  0.1667257833480835  Rel. Test L2 Loss :  0.16778822660446166  Test L2 Loss :  0.24206363677978515  inv_L_scale:  [1.0, 1.0]
Epoch :  18  Time:  1.237  Rel. Train L2 Loss :  0.16650589413113065  Rel. Test L2 Loss :  0.1688810557126999  Test L2 Loss :  0.24423234581947326  inv_L_scale:  [1.0, 1.0]
Epoch :  19  Time:  1.235  Rel. Train L2 Loss :  0.16963978548844655  Rel. Test L2 Loss :  0.16837598383426666  Test L2 Loss :  0.24351166009902955  inv_L_scale:  [1.0, 1.0]
Epoch :  20  Time:  1.235  Rel. Train L2 Loss :  0.1669325680202908  Rel. Test L2 Loss :  0.1655024540424347  Test L2 Loss :  0.2387804126739502  inv_L_scale:  [1.0, 1.0]
Epoch :  21  Time:  1.235  Rel. Train L2 Loss :  0.16625952257050408  Rel. Test L2 Loss :  0.16793137431144714  Test L2 Loss :  0.24246687412261964  inv_L_scale:  [1.0, 1.0]
Epoch :  22  Time:  1.25  Rel. Train L2 Loss :  0.16551508800850975  Rel. Test L2 Loss :  0.16417501628398895  Test L2 Loss :  0.23748409867286682  inv_L_scale:  [1.0, 1.0]
Epoch :  23  Time:  1.244  Rel. Train L2 Loss :  0.16528341790040335  Rel. Test L2 Loss :  0.16592373788356782  Test L2 Loss :  0.24039093554019927  inv_L_scale:  [1.0, 1.0]
Epoch :  24  Time:  1.284  Rel. Train L2 Loss :  0.16538397411505382  Rel. Test L2 Loss :  0.162995388507843  Test L2 Loss :  0.2359531503915787  inv_L_scale:  [1.0, 1.0]
Epoch :  25  Time:  1.257  Rel. Train L2 Loss :  0.16500779390335082  Rel. Test L2 Loss :  0.1650247222185135  Test L2 Loss :  0.23850336253643037  inv_L_scale:  [1.0, 1.0]
Epoch :  26  Time:  1.275  Rel. Train L2 Loss :  0.16593011339505515  Rel. Test L2 Loss :  0.16418016195297241  Test L2 Loss :  0.23775314092636107  inv_L_scale:  [1.0, 1.0]
Epoch :  27  Time:  1.262  Rel. Train L2 Loss :  0.16516321784920163  Rel. Test L2 Loss :  0.16327433586120604  Test L2 Loss :  0.23504442512989043  inv_L_scale:  [1.0, 1.0]
Epoch :  28  Time:  1.264  Rel. Train L2 Loss :  0.16552461571163601  Rel. Test L2 Loss :  0.16046932756900786  Test L2 Loss :  0.23201734602451324  inv_L_scale:  [1.0, 1.0]
Epoch :  29  Time:  1.255  Rel. Train L2 Loss :  0.16526393367184533  Rel. Test L2 Loss :  0.1648002517223358  Test L2 Loss :  0.2373826515674591  inv_L_scale:  [1.0, 1.0]
Epoch :  30  Time:  1.238  Rel. Train L2 Loss :  0.16425225893656412  Rel. Test L2 Loss :  0.16524035692214967  Test L2 Loss :  0.23858737468719482  inv_L_scale:  [1.0, 1.0]
Epoch :  31  Time:  1.238  Rel. Train L2 Loss :  0.1643062342537774  Rel. Test L2 Loss :  0.1627297568321228  Test L2 Loss :  0.2353864723443985  inv_L_scale:  [1.0, 1.0]
Epoch :  32  Time:  1.239  Rel. Train L2 Loss :  0.1648394317759408  Rel. Test L2 Loss :  0.16569124639034272  Test L2 Loss :  0.23904614746570588  inv_L_scale:  [1.0, 1.0]
Epoch :  33  Time:  1.24  Rel. Train L2 Loss :  0.1631922980149587  Rel. Test L2 Loss :  0.16323179006576538  Test L2 Loss :  0.23557372093200685  inv_L_scale:  [1.0, 1.0]
Epoch :  34  Time:  1.236  Rel. Train L2 Loss :  0.16348543597592247  Rel. Test L2 Loss :  0.16313921332359313  Test L2 Loss :  0.23578649997711182  inv_L_scale:  [1.0, 1.0]
Epoch :  35  Time:  1.243  Rel. Train L2 Loss :  0.16403813474708134  Rel. Test L2 Loss :  0.1589939498901367  Test L2 Loss :  0.22971415698528289  inv_L_scale:  [1.0, 1.0]
Epoch :  36  Time:  1.258  Rel. Train L2 Loss :  0.16342284546958075  Rel. Test L2 Loss :  0.16476178228855132  Test L2 Loss :  0.23898668289184571  inv_L_scale:  [1.0, 1.0]
Epoch :  37  Time:  1.296  Rel. Train L2 Loss :  0.16357137355539533  Rel. Test L2 Loss :  0.16456432402133941  Test L2 Loss :  0.23744331121444703  inv_L_scale:  [1.0, 1.0]
Epoch :  38  Time:  1.275  Rel. Train L2 Loss :  0.16455774823824565  Rel. Test L2 Loss :  0.16321695685386658  Test L2 Loss :  0.2354861980676651  inv_L_scale:  [1.0, 1.0]
Epoch :  39  Time:  1.292  Rel. Train L2 Loss :  0.162904009686576  Rel. Test L2 Loss :  0.16438184261322022  Test L2 Loss :  0.2367216032743454  inv_L_scale:  [1.0, 1.0]
Epoch :  40  Time:  1.252  Rel. Train L2 Loss :  0.16331945286856758  Rel. Test L2 Loss :  0.16030069708824157  Test L2 Loss :  0.23157615840435028  inv_L_scale:  [1.0, 1.0]
Epoch :  41  Time:  1.234  Rel. Train L2 Loss :  0.162775506178538  Rel. Test L2 Loss :  0.16128469467163087  Test L2 Loss :  0.23321930646896363  inv_L_scale:  [1.0, 1.0]
Epoch :  42  Time:  1.234  Rel. Train L2 Loss :  0.1624072774251302  Rel. Test L2 Loss :  0.1605500054359436  Test L2 Loss :  0.2327531111240387  inv_L_scale:  [1.0, 1.0]
Epoch :  43  Time:  1.231  Rel. Train L2 Loss :  0.16267961329883998  Rel. Test L2 Loss :  0.161548929810524  Test L2 Loss :  0.2339182662963867  inv_L_scale:  [1.0, 1.0]
Epoch :  44  Time:  1.231  Rel. Train L2 Loss :  0.1627886833084954  Rel. Test L2 Loss :  0.16451223611831664  Test L2 Loss :  0.23717970728874208  inv_L_scale:  [1.0, 1.0]
Epoch :  45  Time:  1.23  Rel. Train L2 Loss :  0.16219554437531367  Rel. Test L2 Loss :  0.16331034719944001  Test L2 Loss :  0.23599886536598205  inv_L_scale:  [1.0, 1.0]
Epoch :  46  Time:  1.244  Rel. Train L2 Loss :  0.1619127408663432  Rel. Test L2 Loss :  0.16110967278480529  Test L2 Loss :  0.23241613507270814  inv_L_scale:  [1.0, 1.0]
Epoch :  47  Time:  1.278  Rel. Train L2 Loss :  0.16186766929096646  Rel. Test L2 Loss :  0.16142239212989806  Test L2 Loss :  0.2335292422771454  inv_L_scale:  [1.0, 1.0]
Epoch :  48  Time:  1.3  Rel. Train L2 Loss :  0.16243067218197715  Rel. Test L2 Loss :  0.16394545197486876  Test L2 Loss :  0.2377881646156311  inv_L_scale:  [1.0, 1.0]
Epoch :  49  Time:  1.274  Rel. Train L2 Loss :  0.161753997736507  Rel. Test L2 Loss :  0.16116030216217042  Test L2 Loss :  0.23325032174587249  inv_L_scale:  [1.0, 1.0]
Epoch :  50  Time:  1.253  Rel. Train L2 Loss :  0.16127961854139963  Rel. Test L2 Loss :  0.16127348065376282  Test L2 Loss :  0.23292621910572053  inv_L_scale:  [1.0, 1.0]
Epoch :  51  Time:  1.23  Rel. Train L2 Loss :  0.16105068842569986  Rel. Test L2 Loss :  0.15865902423858644  Test L2 Loss :  0.2300743842124939  inv_L_scale:  [1.0, 1.0]
Epoch :  52  Time:  1.23  Rel. Train L2 Loss :  0.1612759311331643  Rel. Test L2 Loss :  0.15802286565303802  Test L2 Loss :  0.2288520234823227  inv_L_scale:  [1.0, 1.0]
Epoch :  53  Time:  1.231  Rel. Train L2 Loss :  0.16086081955167983  Rel. Test L2 Loss :  0.16402053117752075  Test L2 Loss :  0.23824219584465026  inv_L_scale:  [1.0, 1.0]
Epoch :  54  Time:  1.232  Rel. Train L2 Loss :  0.16210857050286398  Rel. Test L2 Loss :  0.15830834150314332  Test L2 Loss :  0.22819724261760713  inv_L_scale:  [1.0, 1.0]
Epoch :  55  Time:  1.232  Rel. Train L2 Loss :  0.16074849830733406  Rel. Test L2 Loss :  0.1613467127084732  Test L2 Loss :  0.232917400598526  inv_L_scale:  [1.0, 1.0]
Epoch :  56  Time:  1.234  Rel. Train L2 Loss :  0.16094204478793675  Rel. Test L2 Loss :  0.16095548033714294  Test L2 Loss :  0.23335413932800292  inv_L_scale:  [1.0, 1.0]
Epoch :  57  Time:  1.228  Rel. Train L2 Loss :  0.1608239295085271  Rel. Test L2 Loss :  0.16138638854026793  Test L2 Loss :  0.23341469943523407  inv_L_scale:  [1.0, 1.0]
Epoch :  58  Time:  1.235  Rel. Train L2 Loss :  0.16071059352821773  Rel. Test L2 Loss :  0.157468581199646  Test L2 Loss :  0.2279367685317993  inv_L_scale:  [1.0, 1.0]
Epoch :  59  Time:  1.258  Rel. Train L2 Loss :  0.16079714486996333  Rel. Test L2 Loss :  0.1595096296072006  Test L2 Loss :  0.2304365873336792  inv_L_scale:  [1.0, 1.0]
Epoch :  60  Time:  1.254  Rel. Train L2 Loss :  0.1597432608736886  Rel. Test L2 Loss :  0.16137724936008455  Test L2 Loss :  0.23318907380104065  inv_L_scale:  [1.0, 1.0]
Epoch :  61  Time:  1.236  Rel. Train L2 Loss :  0.1612224766280916  Rel. Test L2 Loss :  0.16191446781158447  Test L2 Loss :  0.23315191268920898  inv_L_scale:  [1.0, 1.0]
Epoch :  62  Time:  1.229  Rel. Train L2 Loss :  0.16022282587157355  Rel. Test L2 Loss :  0.15834485590457917  Test L2 Loss :  0.22916198194026946  inv_L_scale:  [1.0, 1.0]