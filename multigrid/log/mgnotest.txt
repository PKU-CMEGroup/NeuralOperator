downsample_ratio = 10
n_train = 1000
n_test = 200

model = MgNO(
    num_layer=5,
    num_channel_u=32,
    num_channel_f=1,
    num_iteration=[1, 1, 1, 1, 2],
    activation="gelu",
).to(device)


config = {
    "train": {
        "base_lr": 5e-04,
        "weight_decay": 1.0e-4,
        "epochs": 500,
        "scheduler": "OneCycleLR",
        "batch_size": 8,
        "normalization_x": True,
        "normalization_y": True,
        "normalization_dim": [],
    }
}




(PyTorch) Z2339@LEGIONPC C:\Users\Z2339\Desktop\NeuralOperator % & C:/Users/Z2339/anaconda3/envs/PyTorch/python.exe c:/Users/Z2339/Desktop/NeuralOperator/baselines/mgno_darcy_test.py
dir now: C:\Users\Z2339\Desktop\NeuralOperator\baselines
data_in.shape: (2048, 421, 421)
data_out.shape (2048, 421, 421)
x_train.shape:  torch.Size([1000, 1, 43, 43])
y_train.shape:  torch.Size([1000, 1, 43, 43])
number of parameters is 1072484
Epoch :  0  rel.Train:  0.2615715836882591  rel.Test:  0.11074716359376907  abs.Test :  0.0007432409236207604
Epoch :  10  rel.Train:  0.04098323264718056  rel.Test:  0.04237609073519707  abs.Test :  0.000277974201599136
Epoch :  20  rel.Train:  0.026483513370156288  rel.Test:  0.032547279223799706  abs.Test :  0.0002136607957072556
Epoch :  30  rel.Train:  0.020453096121549608  rel.Test:  0.01929007612168789  abs.Test :  0.0001283972425153479
Epoch :  40  rel.Train:  0.020239974834024906  rel.Test:  0.01869633488357067  abs.Test :  0.00012439753219950945
Epoch :  50  rel.Train:  0.019997025273740292  rel.Test:  0.02266934111714363  abs.Test :  0.00015310920600313693
Epoch :  60  rel.Train:  0.019910067550837993  rel.Test:  0.028453468084335326  abs.Test :  0.00018854941823519765
Epoch :  70  rel.Train:  0.01651786681264639  rel.Test:  0.018153458200395108  abs.Test :  0.00012111359188565985
Epoch :  80  rel.Train:  0.017125406056642534  rel.Test:  0.01865925833582878  abs.Test :  0.00012392190488753842
Epoch :  90  rel.Train:  0.016291332110762596  rel.Test:  0.017651212252676487  abs.Test :  0.00011715981119778007
Epoch :  100  rel.Train:  0.015400501996278763  rel.Test:  0.025022833496332168  abs.Test :  0.00016838327515870333
Epoch :  110  rel.Train:  0.015094673685729504  rel.Test:  0.016074885204434394  abs.Test :  0.00010716469114413485
Epoch :  120  rel.Train:  0.014232021436095237  rel.Test:  0.014888807833194732  abs.Test :  9.891979570966213e-05
Epoch :  130  rel.Train:  0.014051103502511979  rel.Test:  0.016232660263776778  abs.Test :  0.00010899974760832265
Epoch :  140  rel.Train:  0.013154961362481118  rel.Test:  0.015194953866302967  abs.Test :  0.00010089208983117715
Epoch :  150  rel.Train:  0.013665836803615094  rel.Test:  0.01633686453104019  abs.Test :  0.00010957980994135142
Epoch :  160  rel.Train:  0.011625592350959777  rel.Test:  0.014083534367382527  abs.Test :  9.41676227375865e-05
Epoch :  170  rel.Train:  0.011668282099068165  rel.Test:  0.013661081194877625  abs.Test :  9.092590888030827e-05
Epoch :  180  rel.Train:  0.010456640236079694  rel.Test:  0.0128146917745471  abs.Test :  8.545867516659201e-05
Epoch :  190  rel.Train:  0.010422108717262745  rel.Test:  0.013331119008362292  abs.Test :  8.902249654056504e-05
Epoch :  200  rel.Train:  0.009913661163300275  rel.Test:  0.012723409831523895  abs.Test :  8.423499210039154e-05
Epoch :  210  rel.Train:  0.009613366320729256  rel.Test:  0.01227200597524643  abs.Test :  8.163032325683161e-05
Epoch :  220  rel.Train:  0.009629816368222237  rel.Test:  0.01750920668244362  abs.Test :  0.00011909245105925947
Epoch :  230  rel.Train:  0.008904152281582355  rel.Test:  0.012672521732747554  abs.Test :  8.452619978925213e-05
Epoch :  240  rel.Train:  0.007822154715657234  rel.Test:  0.010776490978896618  abs.Test :  7.164663882576861e-05
Epoch :  250  rel.Train:  0.00801784286648035  rel.Test:  0.011420774273574352  abs.Test :  7.550678885309025e-05
Epoch :  260  rel.Train:  0.008538483183830976  rel.Test:  0.011158859841525554  abs.Test :  7.401104841846972e-05
Epoch :  270  rel.Train:  0.0077814459837973115  rel.Test:  0.010909872204065323  abs.Test :  7.267123757628724e-05
Epoch :  280  rel.Train:  0.007592674899846315  rel.Test:  0.01131368514150381  abs.Test :  7.568297078250908e-05
Epoch :  290  rel.Train:  0.006860287230461836  rel.Test:  0.01073836300522089  abs.Test :  7.148852120735682e-05
Epoch :  300  rel.Train:  0.007260024163872004  rel.Test:  0.010600583143532277  abs.Test :  7.080310097080655e-05
Epoch :  310  rel.Train:  0.007428679000586271  rel.Test:  0.010792483687400817  abs.Test :  7.197237166110426e-05
Epoch :  320  rel.Train:  0.0060721559748053554  rel.Test:  0.01040251012891531  abs.Test :  6.963518491829745e-05
Epoch :  330  rel.Train:  0.0065994632765650745  rel.Test:  0.010841232798993588  abs.Test :  7.207984002889134e-05
Epoch :  340  rel.Train:  0.005941334269940853  rel.Test:  0.010219128839671611  abs.Test :  6.803303302149288e-05
Epoch :  350  rel.Train:  0.005256700240075588  rel.Test:  0.01029859222471714  abs.Test :  6.863703340059146e-05
Epoch :  360  rel.Train:  0.005185390546917915  rel.Test:  0.009925189651548863  abs.Test :  6.624104949878528e-05
Epoch :  370  rel.Train:  0.00533011419326067  rel.Test:  0.010018197819590569  abs.Test :  6.66693710081745e-05
Epoch :  380  rel.Train:  0.004821545358747244  rel.Test:  0.009779841117560863  abs.Test :  6.521303366753272e-05
Epoch :  390  rel.Train:  0.005129664435982704  rel.Test:  0.00986750639975071  abs.Test :  6.572357611730694e-05
Epoch :  400  rel.Train:  0.0045711938329041  rel.Test:  0.009772281311452389  abs.Test :  6.506203542812728e-05
Epoch :  410  rel.Train:  0.00451550824008882  rel.Test:  0.009699158817529679  abs.Test :  6.459422249463387e-05
Epoch :  420  rel.Train:  0.004412150897085666  rel.Test:  0.00967847228050232  abs.Test :  6.44390520756133e-05
Epoch :  430  rel.Train:  0.0043136592246592045  rel.Test:  0.009697514921426773  abs.Test :  6.450644286815077e-05
Epoch :  440  rel.Train:  0.0042352664470672605  rel.Test:  0.009668600298464298  abs.Test :  6.43646482785698e-05
Epoch :  450  rel.Train:  0.004189634511247277  rel.Test:  0.009672201462090016  abs.Test :  6.432248366763815e-05
Epoch :  460  rel.Train:  0.004149028521031141  rel.Test:  0.009652428776025773  abs.Test :  6.421019570552744e-05
Epoch :  470  rel.Train:  0.004122272046282888  rel.Test:  0.009620551615953446  abs.Test :  6.402119965059683e-05
Epoch :  480  rel.Train:  0.004100060919299721  rel.Test:  0.009632782749831677  abs.Test :  6.411302936612629e-05
Epoch :  490  rel.Train:  0.004088304543867707  rel.Test:  0.00963343668729067  abs.Test :  6.41172009636648e-05
Epoch :  499  rel.Train:  0.004084914678707719  rel.Test:  0.009628002606332302  abs.Test :  6.40818799729459e-05





(PyTorch) Z2339@LEGIONPC C:\Users\Z2339\Desktop\NeuralOperator % & C:/Users/Z2339/anaconda3/envs/PyTorch/python.exe c:/Users/Z2339/Desktop/NeuralOperator/baselines/fno2d_transpose_test.py
dir now: C:\Users\Z2339\Desktop\NeuralOperator\baselines
data.shape: (2048, 421, 421)
x_train.shape:  torch.Size([1000, 43, 43, 3])
y_train.shape:  torch.Size([1000, 43, 43, 1])
Epoch :  0  Rel. Train L2 Loss :  0.4920846576690674  Rel. Test L2 Loss :  0.47839492201805117  Test L2 Loss :  4.167946834564209
Epoch :  10  Rel. Train L2 Loss :  0.11584802734851837  Rel. Test L2 Loss :  0.1560184133052826  Test L2 Loss :  1.3602108764648437
Epoch :  20  Rel. Train L2 Loss :  0.0628608695268631  Rel. Test L2 Loss :  0.14586262494325639  Test L2 Loss :  1.2716609859466552
Epoch :  30  Rel. Train L2 Loss :  0.04992434939742088  Rel. Test L2 Loss :  0.13895721286535262  Test L2 Loss :  1.2120653152465821
Epoch :  40  Rel. Train L2 Loss :  0.04363535659015179  Rel. Test L2 Loss :  0.13161232709884643  Test L2 Loss :  1.147593741416931
Epoch :  50  Rel. Train L2 Loss :  0.03910286070406437  Rel. Test L2 Loss :  0.12595054775476455  Test L2 Loss :  1.0976503920555114
Epoch :  60  Rel. Train L2 Loss :  0.03526298698782921  Rel. Test L2 Loss :  0.11688557326793671  Test L2 Loss :  1.0191329383850098
Epoch :  70  Rel. Train L2 Loss :  0.03233877347409725  Rel. Test L2 Loss :  0.10960162132978439  Test L2 Loss :  0.9558905029296875
Epoch :  80  Rel. Train L2 Loss :  0.02794401539862156  Rel. Test L2 Loss :  0.10099976003170014  Test L2 Loss :  0.8805206823348999
Epoch :  90  Rel. Train L2 Loss :  0.024735273949801923  Rel. Test L2 Loss :  0.09268132567405701  Test L2 Loss :  0.8079387354850769
Epoch :  100  Rel. Train L2 Loss :  0.022004749491810797  Rel. Test L2 Loss :  0.08645261555910111  Test L2 Loss :  0.753510205745697
Epoch :  110  Rel. Train L2 Loss :  0.019335958823561667  Rel. Test L2 Loss :  0.08047006458044052  Test L2 Loss :  0.7016472148895264
Epoch :  120  Rel. Train L2 Loss :  0.018337125547230243  Rel. Test L2 Loss :  0.07298073649406434  Test L2 Loss :  0.63569216132164
Epoch :  130  Rel. Train L2 Loss :  0.015851526625454426  Rel. Test L2 Loss :  0.07005304098129272  Test L2 Loss :  0.6109341025352478
Epoch :  140  Rel. Train L2 Loss :  0.01447745743766427  Rel. Test L2 Loss :  0.06600889295339585  Test L2 Loss :  0.5749469184875489
Epoch :  150  Rel. Train L2 Loss :  0.012471521146595478  Rel. Test L2 Loss :  0.05978644460439682  Test L2 Loss :  0.5207354891300201
Epoch :  160  Rel. Train L2 Loss :  0.010736984185874461  Rel. Test L2 Loss :  0.05804150283336639  Test L2 Loss :  0.5055756175518036
Epoch :  170  Rel. Train L2 Loss :  0.010851389121264219  Rel. Test L2 Loss :  0.05495108604431152  Test L2 Loss :  0.47851853370666503
Epoch :  180  Rel. Train L2 Loss :  0.010713151752948761  Rel. Test L2 Loss :  0.05424119874835014  Test L2 Loss :  0.4724019193649292
Epoch :  190  Rel. Train L2 Loss :  0.009626000737771392  Rel. Test L2 Loss :  0.049730740785598755  Test L2 Loss :  0.43362866759300234
Epoch :  200  Rel. Train L2 Loss :  0.008573047060519456  Rel. Test L2 Loss :  0.05073729030787945  Test L2 Loss :  0.441950101852417
Epoch :  210  Rel. Train L2 Loss :  0.00797462377883494  Rel. Test L2 Loss :  0.04968319997191429  Test L2 Loss :  0.4330242586135864
Epoch :  220  Rel. Train L2 Loss :  0.00702373855933547  Rel. Test L2 Loss :  0.047938306480646134  Test L2 Loss :  0.41791093349456787
Epoch :  230  Rel. Train L2 Loss :  0.0069070359990000725  Rel. Test L2 Loss :  0.04705548733472824  Test L2 Loss :  0.4098417198657989
Epoch :  240  Rel. Train L2 Loss :  0.006090249437838793  Rel. Test L2 Loss :  0.04759317860007286  Test L2 Loss :  0.4146789968013763
Epoch :  250  Rel. Train L2 Loss :  0.00621756730414927  Rel. Test L2 Loss :  0.045716872215270994  Test L2 Loss :  0.39821377515792844
Epoch :  260  Rel. Train L2 Loss :  0.0051377187166363  Rel. Test L2 Loss :  0.04551527500152588  Test L2 Loss :  0.39675130367279055
Epoch :  270  Rel. Train L2 Loss :  0.006769482336938381  Rel. Test L2 Loss :  0.04730960637331009  Test L2 Loss :  0.41235696196556093
Epoch :  280  Rel. Train L2 Loss :  0.004003558354452252  Rel. Test L2 Loss :  0.043055944219231604  Test L2 Loss :  0.37506439685821535
Epoch :  290  Rel. Train L2 Loss :  0.004286172885447741  Rel. Test L2 Loss :  0.044731506407260896  Test L2 Loss :  0.38984370946884156
Epoch :  300  Rel. Train L2 Loss :  0.0031847904492169616  Rel. Test L2 Loss :  0.045531170219182966  Test L2 Loss :  0.3964825654029846



dir now: /lustre/home/2200010870/works/NeuralOperator/baselines
data_in.shape: (2490, 221, 51, 2)
data_out.shape (2490, 221, 51)
x_train.shape:  torch.Size([1000, 2, 221, 51])
y_train.shape:  torch.Size([1000, 1, 221, 51])
number of parameters is 1315572
Epoch :  0  rel.Train:  0.15384954738616943  rel.Test:  0.11977811574935913  abs.Test :  0.09904107213020324
Epoch :  10  rel.Train:  0.10113999283313752  rel.Test:  0.09736273527145385  abs.Test :  0.08052563309669494
Epoch :  20  rel.Train:  0.10058368539810181  rel.Test:  0.09790922164916992  abs.Test :  0.08096394777297973
Epoch :  30  rel.Train:  0.10076810812950135  rel.Test:  0.09750025749206542  abs.Test :  0.0806687593460083
Epoch :  40  rel.Train:  0.10076275646686554  rel.Test:  0.09742092370986938  abs.Test :  0.08056562781333923
Epoch :  50  rel.Train:  0.09054725646972656  rel.Test:  0.10082406401634217  abs.Test :  0.08329556345939636
Epoch :  60  rel.Train:  0.07058160239458085  rel.Test:  0.07100800335407258  abs.Test :  0.05866881549358368
Epoch :  70  rel.Train:  0.0723787373304367  rel.Test:  0.06438177466392517  abs.Test :  0.05321952402591705
Epoch :  80  rel.Train:  0.061635202050209045  rel.Test:  0.05908171534538269  abs.Test :  0.048815605640411375
Epoch :  90  rel.Train:  0.05486247152090073  rel.Test:  0.07054896831512451  abs.Test :  0.05824252188205719
Epoch :  100  rel.Train:  0.04872690933942795  rel.Test:  0.045592720806598666  abs.Test :  0.037680740356445315
Epoch :  110  rel.Train:  0.0484115400314331  rel.Test:  0.04404393672943115  abs.Test :  0.03641373932361603
Epoch :  120  rel.Train:  0.0416546881198883  rel.Test:  0.034936456382274626  abs.Test :  0.028861959278583527
Epoch :  130  rel.Train:  0.040652197659015656  rel.Test:  0.0467245352268219  abs.Test :  0.03857781648635864
Epoch :  140  rel.Train:  0.043225990295410154  rel.Test:  0.050524539947509765  abs.Test :  0.041722621917724606
Epoch :  150  rel.Train:  0.041769069969654085  rel.Test:  0.05184135586023331  abs.Test :  0.04283537268638611
Epoch :  160  rel.Train:  0.03309019854664803  rel.Test:  0.040968818962574004  abs.Test :  0.033831631541252134
Epoch :  170  rel.Train:  0.031350317060947416  rel.Test:  0.046973894834518436  abs.Test :  0.03883260548114777
Epoch :  180  rel.Train:  0.032321373462677  rel.Test:  0.03436080515384674  abs.Test :  0.02838772714138031
Epoch :  190  rel.Train:  0.03432776167988777  rel.Test:  0.03883228719234467  abs.Test :  0.03208089679479599
Epoch :  200  rel.Train:  0.027220927625894547  rel.Test:  0.02936541199684143  abs.Test :  0.024247993528842927
Epoch :  210  rel.Train:  0.028892623126506804  rel.Test:  0.03523220092058182  abs.Test :  0.02911874622106552
Epoch :  220  rel.Train:  0.02839912474155426  rel.Test:  0.03664797186851501  abs.Test :  0.03030809700489044
Epoch :  230  rel.Train:  0.02636907133460045  rel.Test:  0.048741726279258726  abs.Test :  0.04024647772312164
Epoch :  240  rel.Train:  0.023333394408226012  rel.Test:  0.028510125279426576  abs.Test :  0.023590048700571062
Epoch :  250  rel.Train:  0.027188946187496184  rel.Test:  0.02774287909269333  abs.Test :  0.022910217344760894
Epoch :  260  rel.Train:  0.023157113194465638  rel.Test:  0.026153977811336517  abs.Test :  0.021602159440517424
Epoch :  270  rel.Train:  0.022329672873020172  rel.Test:  0.02265383169054985  abs.Test :  0.01869335174560547
Epoch :  280  rel.Train:  0.02130340690910816  rel.Test:  0.025225026160478593  abs.Test :  0.02084321901202202
Epoch :  290  rel.Train:  0.022984532684087754  rel.Test:  0.03849662244319916  abs.Test :  0.03180487334728241
Epoch :  300  rel.Train:  0.020871116816997527  rel.Test:  0.02338788002729416  abs.Test :  0.019313079416751863
Epoch :  310  rel.Train:  0.02091475906968117  rel.Test:  0.028942344188690187  abs.Test :  0.02391877204179764
Epoch :  320  rel.Train:  0.021291342496871948  rel.Test:  0.019964806586503982  abs.Test :  0.01649163842201233
Epoch :  330  rel.Train:  0.019996833264827728  rel.Test:  0.01697931617498398  abs.Test :  0.014031718522310256
Epoch :  340  rel.Train:  0.01652333016693592  rel.Test:  0.019765299260616303  abs.Test :  0.016346419304609297
Epoch :  350  rel.Train:  0.019727687433362007  rel.Test:  0.021607466489076615  abs.Test :  0.017851788103580474
Epoch :  360  rel.Train:  0.018619539096951486  rel.Test:  0.02632550835609436  abs.Test :  0.02173880726099014
Epoch :  370  rel.Train:  0.016015312880277632  rel.Test:  0.016171034574508667  abs.Test :  0.013359296545386314
Epoch :  380  rel.Train:  0.014890236109495162  rel.Test:  0.016110979914665223  abs.Test :  0.013305108845233917
Epoch :  390  rel.Train:  0.016129702806472777  rel.Test:  0.025445324033498765  abs.Test :  0.020998278558254244
Epoch :  400  rel.Train:  0.015002608269453049  rel.Test:  0.013614090830087662  abs.Test :  0.011245412528514862
Epoch :  410  rel.Train:  0.014706232964992523  rel.Test:  0.016287465542554856  abs.Test :  0.013448423072695732
Epoch :  420  rel.Train:  0.015442315146327018  rel.Test:  0.016628081798553466  abs.Test :  0.013738020434975623
Epoch :  430  rel.Train:  0.013438537865877152  rel.Test:  0.023790084421634675  abs.Test :  0.019660178124904632
Epoch :  440  rel.Train:  0.012702744536101818  rel.Test:  0.01436300739645958  abs.Test :  0.0118730628490448
Epoch :  450  rel.Train:  0.012876546129584313  rel.Test:  0.01536939576268196  abs.Test :  0.012699673026800156
Epoch :  460  rel.Train:  0.015581771582365036  rel.Test:  0.019086998105049133  abs.Test :  0.015774780660867693
Epoch :  470  rel.Train:  0.013609611697494984  rel.Test:  0.014819222539663314  abs.Test :  0.012226753160357475
Epoch :  480  rel.Train:  0.012303913414478302  rel.Test:  0.014069849699735642  abs.Test :  0.011626486778259277
Epoch :  490  rel.Train:  0.010683827981352806  rel.Test:  0.011380930244922639  abs.Test :  0.009403292611241341
Epoch :  500  rel.Train:  0.010601499676704406  rel.Test:  0.010668374598026276  abs.Test :  0.008811916634440422
Epoch :  510  rel.Train:  0.010685695260763168  rel.Test:  0.01244839183986187  abs.Test :  0.010286872386932372
Epoch :  520  rel.Train:  0.010525162883102894  rel.Test:  0.01192765787243843  abs.Test :  0.009854889586567878
Epoch :  530  rel.Train:  0.010099395036697387  rel.Test:  0.011996904090046882  abs.Test :  0.009912317395210266
Epoch :  540  rel.Train:  0.009619415447115897  rel.Test:  0.00961146280169487  abs.Test :  0.007940660864114761
Epoch :  550  rel.Train:  0.00988261790573597  rel.Test:  0.010025104805827141  abs.Test :  0.00827951580286026
Epoch :  560  rel.Train:  0.0106128169298172  rel.Test:  0.010572855025529862  abs.Test :  0.008738461807370186
Epoch :  570  rel.Train:  0.009490922808647155  rel.Test:  0.010856086760759354  abs.Test :  0.008975521102547646
Epoch :  580  rel.Train:  0.008488915495574473  rel.Test:  0.011629706770181656  abs.Test :  0.009615972340106964
Epoch :  590  rel.Train:  0.008622305303812026  rel.Test:  0.009728774651885033  abs.Test :  0.0080384012311697
Epoch :  600  rel.Train:  0.008907646529376506  rel.Test:  0.009738273322582244  abs.Test :  0.008043521419167518
Epoch :  610  rel.Train:  0.008199654154479503  rel.Test:  0.009069653004407882  abs.Test :  0.007493043914437294
Epoch :  620  rel.Train:  0.007952936880290508  rel.Test:  0.008371590301394463  abs.Test :  0.00691771037876606
Epoch :  630  rel.Train:  0.007568392857909202  rel.Test:  0.009728820025920869  abs.Test :  0.008038365170359612
Epoch :  640  rel.Train:  0.007271639734506607  rel.Test:  0.008741950914263726  abs.Test :  0.00722368061542511
Epoch :  650  rel.Train:  0.007491188362240791  rel.Test:  0.00793939247727394  abs.Test :  0.006558412238955497
Epoch :  660  rel.Train:  0.007118223898112774  rel.Test:  0.00792434811592102  abs.Test :  0.0065481381863355635
Epoch :  670  rel.Train:  0.006945186376571656  rel.Test:  0.007563832849264145  abs.Test :  0.006249118633568287
Epoch :  680  rel.Train:  0.006791393414139747  rel.Test:  0.00785568855702877  abs.Test :  0.006491431668400764
Epoch :  690  rel.Train:  0.006817682757973671  rel.Test:  0.007944299057126045  abs.Test :  0.006564974188804627
Epoch :  700  rel.Train:  0.006650393433868885  rel.Test:  0.007471621856093407  abs.Test :  0.006174692027270794
Epoch :  710  rel.Train:  0.006516539972275495  rel.Test:  0.007683793008327484  abs.Test :  0.006349407285451889
Epoch :  720  rel.Train:  0.006414261572062969  rel.Test:  0.007572382539510727  abs.Test :  0.006257235407829285
Epoch :  730  rel.Train:  0.006562905982136726  rel.Test:  0.007989319935441018  abs.Test :  0.006602060794830322
Epoch :  740  rel.Train:  0.006468564130365849  rel.Test:  0.007574300318956375  abs.Test :  0.006259482130408287
Epoch :  750  rel.Train:  0.006287196516990661  rel.Test:  0.007496945485472679  abs.Test :  0.006196461468935013
Epoch :  760  rel.Train:  0.006225709389895201  rel.Test:  0.0073558735847473146  abs.Test :  0.006078594401478767
Epoch :  770  rel.Train:  0.00621348524838686  rel.Test:  0.007351778075098991  abs.Test :  0.006075309701263905
Epoch :  780  rel.Train:  0.006205930456519127  rel.Test:  0.0073771834373474125  abs.Test :  0.006096599325537681
Epoch :  790  rel.Train:  0.006181208238005638  rel.Test:  0.007375767379999161  abs.Test :  0.006095217019319535
Epoch :  799  rel.Train:  0.006154257923364639  rel.Test:  0.00739258922636509  abs.Test :  0.006109296567738056






data.shape: (2048, 421, 421)
x_train.shape:  torch.Size([1000, 211, 211, 3])
y_train.shape:  torch.Size([1000, 211, 211, 1])
Epoch :  0  Rel. Train L2 Loss :  0.2853454154729843  Rel. Test L2 Loss :  0.24937955498695374  Test L2 Loss :  0.0016977975890040398
Epoch :  10  Rel. Train L2 Loss :  0.1329556968808174  Rel. Test L2 Loss :  0.14106677621603012  Test L2 Loss :  0.0009511513425968588
Epoch :  20  Rel. Train L2 Loss :  0.12262383317947388  Rel. Test L2 Loss :  0.13077465295791627  Test L2 Loss :  0.0008934922213666141
Epoch :  30  Rel. Train L2 Loss :  0.11142825692892075  Rel. Test L2 Loss :  0.11354667156934738  Test L2 Loss :  0.0007651500753127038
Epoch :  40  Rel. Train L2 Loss :  0.07044091966748238  Rel. Test L2 Loss :  0.07576938450336457  Test L2 Loss :  0.0005117159837391228
Epoch :  50  Rel. Train L2 Loss :  0.06393581381440162  Rel. Test L2 Loss :  0.06657164245843887  Test L2 Loss :  0.00045231370721012356
Epoch :  60  Rel. Train L2 Loss :  0.05908293044567108  Rel. Test L2 Loss :  0.06556758910417557  Test L2 Loss :  0.00043942430871538816
Epoch :  70  Rel. Train L2 Loss :  0.05584379157423973  Rel. Test L2 Loss :  0.060796578526496885  Test L2 Loss :  0.00041467762785032394
Epoch :  80  Rel. Train L2 Loss :  0.051997968524694446  Rel. Test L2 Loss :  0.05769524037837982  Test L2 Loss :  0.0003897637955378741
Epoch :  90  Rel. Train L2 Loss :  0.05026518559455872  Rel. Test L2 Loss :  0.05596878409385681  Test L2 Loss :  0.00037534648668952286
Epoch :  100  Rel. Train L2 Loss :  0.04904180788993835  Rel. Test L2 Loss :  0.0534599632024765  Test L2 Loss :  0.0003636217303574085
Epoch :  110  Rel. Train L2 Loss :  0.044450149416923525  Rel. Test L2 Loss :  0.047076579630374905  Test L2 Loss :  0.0003189164353534579
Epoch :  120  Rel. Train L2 Loss :  0.0420601053237915  Rel. Test L2 Loss :  0.043394686579704286  Test L2 Loss :  0.0002937866310821846
Epoch :  130  Rel. Train L2 Loss :  0.03954840663075447  Rel. Test L2 Loss :  0.04303631484508515  Test L2 Loss :  0.0002897666650824249
Epoch :  140  Rel. Train L2 Loss :  0.03711937826871872  Rel. Test L2 Loss :  0.04050176844000816  Test L2 Loss :  0.0002735542826121673
Epoch :  150  Rel. Train L2 Loss :  0.035870432108640674  Rel. Test L2 Loss :  0.04192933276295662  Test L2 Loss :  0.0002816997258923948
Epoch :  160  Rel. Train L2 Loss :  0.03521368470788002  Rel. Test L2 Loss :  0.039550061225891116  Test L2 Loss :  0.00026591503410600126
Epoch :  170  Rel. Train L2 Loss :  0.03294085745513439  Rel. Test L2 Loss :  0.03555057555437088  Test L2 Loss :  0.0002397931949235499
Epoch :  180  Rel. Train L2 Loss :  0.02932425218820572  Rel. Test L2 Loss :  0.03480244286358356  Test L2 Loss :  0.0002343927719630301
Epoch :  190  Rel. Train L2 Loss :  0.028338097631931305  Rel. Test L2 Loss :  0.03188690677285194  Test L2 Loss :  0.00021507072495296597
Epoch :  200  Rel. Train L2 Loss :  0.027428781136870386  Rel. Test L2 Loss :  0.032315335497260095  Test L2 Loss :  0.00021919105958659202
Epoch :  210  Rel. Train L2 Loss :  0.026728218957781793  Rel. Test L2 Loss :  0.03156028762459755  Test L2 Loss :  0.00021427791332826018
Epoch :  220  Rel. Train L2 Loss :  0.023983137488365173  Rel. Test L2 Loss :  0.030613604485988617  Test L2 Loss :  0.00020717664156109095
Epoch :  230  Rel. Train L2 Loss :  0.02516856352984905  Rel. Test L2 Loss :  0.027139703929424285  Test L2 Loss :  0.00018319116439670326
Epoch :  240  Rel. Train L2 Loss :  0.021812202498316766  Rel. Test L2 Loss :  0.024971280246973038  Test L2 Loss :  0.0001679789024638012
Epoch :  250  Rel. Train L2 Loss :  0.020034210845828057  Rel. Test L2 Loss :  0.023653163462877273  Test L2 Loss :  0.00015955792332533747
Epoch :  260  Rel. Train L2 Loss :  0.02056394686549902  Rel. Test L2 Loss :  0.02205207824707031  Test L2 Loss :  0.00014919749577529728
Epoch :  270  Rel. Train L2 Loss :  0.019306620240211486  Rel. Test L2 Loss :  0.029606452360749245  Test L2 Loss :  0.0002021024376153946
Epoch :  280  Rel. Train L2 Loss :  0.01852314592152834  Rel. Test L2 Loss :  0.02011409670114517  Test L2 Loss :  0.0001355308946222067
Epoch :  290  Rel. Train L2 Loss :  0.01665515249222517  Rel. Test L2 Loss :  0.01961884059011936  Test L2 Loss :  0.00013325639913091435
Epoch :  300  Rel. Train L2 Loss :  0.016038398951292037  Rel. Test L2 Loss :  0.02013345666229725  Test L2 Loss :  0.00013667849096236752
Epoch :  310  Rel. Train L2 Loss :  0.015111647970974446  Rel. Test L2 Loss :  0.018258017748594285  Test L2 Loss :  0.00012263841577805578
Epoch :  320  Rel. Train L2 Loss :  0.014239015802741051  Rel. Test L2 Loss :  0.01687604058533907  Test L2 Loss :  0.00011402395495679229
Epoch :  330  Rel. Train L2 Loss :  0.013069508053362369  Rel. Test L2 Loss :  0.015701316744089127  Test L2 Loss :  0.00010625167022226378
Epoch :  340  Rel. Train L2 Loss :  0.013934957429766654  Rel. Test L2 Loss :  0.01723742447793484  Test L2 Loss :  0.00011592787690460682
Epoch :  350  Rel. Train L2 Loss :  0.012463136985898017  Rel. Test L2 Loss :  0.015354659259319305  Test L2 Loss :  0.00010391412622993812
Epoch :  360  Rel. Train L2 Loss :  0.011592608466744423  Rel. Test L2 Loss :  0.019145522192120554  Test L2 Loss :  0.00013006859458982944
Epoch :  370  Rel. Train L2 Loss :  0.011542063362896442  Rel. Test L2 Loss :  0.013031896501779557  Test L2 Loss :  8.819713490083814e-05
Epoch :  380  Rel. Train L2 Loss :  0.009994625896215439  Rel. Test L2 Loss :  0.011881483569741249  Test L2 Loss :  8.020949404453859e-05
Epoch :  390  Rel. Train L2 Loss :  0.009683384135365486  Rel. Test L2 Loss :  0.011752061918377877  Test L2 Loss :  7.936218346003443e-05
Epoch :  400  Rel. Train L2 Loss :  0.009145638946443796  Rel. Test L2 Loss :  0.011050929464399815  Test L2 Loss :  7.465061324182898e-05
Epoch :  410  Rel. Train L2 Loss :  0.008431040946394205  Rel. Test L2 Loss :  0.010647509545087815  Test L2 Loss :  7.1924818912521e-05
Epoch :  420  Rel. Train L2 Loss :  0.008021612364798784  Rel. Test L2 Loss :  0.010417635925114155  Test L2 Loss :  7.05136226315517e-05
Epoch :  430  Rel. Train L2 Loss :  0.007849709447473287  Rel. Test L2 Loss :  0.009809115529060363  Test L2 Loss :  6.633081691688858e-05
Epoch :  440  Rel. Train L2 Loss :  0.007754216272383928  Rel. Test L2 Loss :  0.009717942997813225  Test L2 Loss :  6.569916164153255e-05
Epoch :  450  Rel. Train L2 Loss :  0.007232620369642973  Rel. Test L2 Loss :  0.009479239098727704  Test L2 Loss :  6.414995266823098e-05
Epoch :  460  Rel. Train L2 Loss :  0.007068316891789436  Rel. Test L2 Loss :  0.009259923361241817  Test L2 Loss :  6.262178445467725e-05
Epoch :  470  Rel. Train L2 Loss :  0.006993461571633816  Rel. Test L2 Loss :  0.009186068698763847  Test L2 Loss :  6.21454264910426e-05
Epoch :  480  Rel. Train L2 Loss :  0.006855458997189999  Rel. Test L2 Loss :  0.009180034976452589  Test L2 Loss :  6.211302548763343e-05
Epoch :  490  Rel. Train L2 Loss :  0.006760569162666798  Rel. Test L2 Loss :  0.009018158111721276  Test L2 Loss :  6.099263104260899e-05
Epoch :  499  Rel. Train L2 Loss :  0.006733589563518763  Rel. Test L2 Loss :  0.009013027064502238  Test L2 Loss :  6.094726923038252e-05
