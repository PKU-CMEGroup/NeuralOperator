{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from models import FNN1d, FNN1d_train, FNN1d_cost, UnitGaussianNormalizer, LpLoss\n",
    "\n",
    "\n",
    "def test(x_train, y_train, x_test, y_test, model_prefix, config, downsample_ratio, n_fno_layers, k_max, d_f):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    normalization, dim = config[\"train\"][\"normalization\"], config[\"train\"][\"dim\"]\n",
    "    \n",
    "    n_data = x_train.shape[0]\n",
    "    \n",
    "    # load model\n",
    "    # error mean/covariance on validataion set\n",
    "    # error mean/covariance on test set \n",
    "    setup_info=\"n_data_\"+str(n_data)+\"_k_max_\"+str(k_max)+\"_downsample_ratio_\"+str(downsample_ratio)+\"_n_fno_layers_\"+str(n_fno_layers)+\"_d_f_\"+str(d_f)\n",
    "    model = torch.load(model_prefix+setup_info, map_location=device)\n",
    "\n",
    "\n",
    "    n_test = x_test.shape[0]\n",
    "\n",
    "    test_rel_l2_losses = []\n",
    "    test_l2_losses =[]\n",
    "    \n",
    "    \n",
    "    if normalization:\n",
    "        x_normalizer = UnitGaussianNormalizer(x_train, dim=dim)\n",
    "        x_train = x_normalizer.encode(x_train)\n",
    "        x_test = x_normalizer.encode(x_test)\n",
    "        x_normalizer.to(device)\n",
    "\n",
    "        y_normalizer = UnitGaussianNormalizer(y_train, dim=dim)\n",
    "        y_train = y_normalizer.encode(y_train)\n",
    "        y_test = y_normalizer.encode(y_test)\n",
    "        y_normalizer.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    myloss = LpLoss(d=1, p=2, size_average=False)\n",
    "    \n",
    "    test_l2 = np.zeros(n_test)\n",
    "    test_rel_l2 = np.zeros(n_test)\n",
    "    \n",
    "    for i in range(n_test):\n",
    "        x, y = x_test[i:i+1,:, :], y_test[i:i+1,:, :]\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        out = model(x) #.reshape(1,  -1)\n",
    "\n",
    "        if normalization:\n",
    "            out = y_normalizer.decode(out)\n",
    "            y = y_normalizer.decode(y)\n",
    "\n",
    "        test_rel_l2[i] = myloss(out.view(1,-1), y.view(1,-1)).item()\n",
    "        test_l2[i] = myloss.abs(out.view(1,-1), y.view(1,-1)).item()\n",
    "\n",
    "    test_l2_mean, test_l2_cov = np.mean(test_l2), np.cov(test_l2)\n",
    "    test_rel_l2_mean, test_rel_l2_cov = np.mean(test_rel_l2), np.cov(test_rel_l2)\n",
    "\n",
    "    print(setup_info, test_l2_mean, test_l2_cov, test_rel_l2_mean, test_rel_l2_cov)\n",
    "    return test_l2_mean, test_l2_cov, test_rel_l2_mean, test_rel_l2_cov\n",
    "             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darcy flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_data_256_k_max_16_downsample_ratio_1_n_fno_layers_3_d_f_16 0.1279992140586046 0.004491773613030803 0.03540278993625634 0.0007415211296267925\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24628/3807343647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                         \u001b[0mdata_analysis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_data_analysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi_test_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_test_set\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownsample_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fno_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFNN1d_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24628/4022556587.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(x_train, y_train, x_test, y_test, model_prefix, config, downsample_ratio, n_fno_layers, k_max, d_f)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.reshape(1,  -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/qg/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/central/home/dzhuang/Code/NeuralOperator/models/fourier1d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mspeconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_convs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/qg/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/central/home/dzhuang/Code/NeuralOperator/models/basics.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mout_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mout_ft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodes1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompl_mul1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodes1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Return to physical space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prefix = \"/central/groups/esm/dzhuang/cost-accuracy-data/\"\n",
    "x_data = np.load(prefix+\"darcy_a.npy\")\n",
    "y_data = np.load(prefix+\"darcy_u.npy\")\n",
    "model_prefix = \"/central/groups/esm/dzhuang/cost-accuracy-data/models/darcy_FNO_0_\"\n",
    "\n",
    "n_data_array = [256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "k_max_array = [16, 32, 64, 128]\n",
    "d_f_array = [16, 32, 64, 128]\n",
    "n_fno_layers_array = [3, 4, 5]\n",
    "downsample_ratio_array = [1, 2, 4, 8]\n",
    "\n",
    "\n",
    "#optimization\n",
    "epochs = 1001\n",
    "base_lr = 0.001\n",
    "milestones = [200, 300, 400, 500, 800, 900]\n",
    "scheduler_gamma = 0.5\n",
    "batch_size=32\n",
    "normalization = True\n",
    "dim = []\n",
    "\n",
    "M = 2**15\n",
    "L, Ne_ref = 1.0, 2**12\n",
    "n_test = 8192\n",
    "\n",
    "\n",
    "n_test_sets = 2\n",
    "data_analysis = np.zeros((len(n_data_array)*len(downsample_ratio_array)*len(n_fno_layers_array)*len(k_max_array)*len(d_f_array), 4*n_test_sets + 6)) \n",
    "\n",
    "i_data_analysis = 0\n",
    "\n",
    "for n_train in n_data_array:\n",
    "    for downsample_ratio in downsample_ratio_array:\n",
    "        for n_fno_layers in n_fno_layers_array:\n",
    "            for k_max in k_max_array:\n",
    "                for d_f in d_f_array: \n",
    "                    for i_test_set in range(n_test_sets):\n",
    "                        \n",
    "                        \n",
    "                        Ne = Ne_ref//downsample_ratio\n",
    "                        grid = np.linspace(0, L, Ne+1)\n",
    "                        \n",
    "                        x_train = torch.from_numpy(np.stack((x_data[0:n_train, 0::downsample_ratio], np.tile(grid, (n_train,1))), axis=-1).astype(np.float32))\n",
    "                        y_train = torch.from_numpy(y_data[0:n_train, 0::downsample_ratio, np.newaxis].astype(np.float32))\n",
    "                        # x_train, y_train are [n_data, n_x, n_channel] arrays\n",
    "                        x_test = torch.from_numpy(np.stack((x_data[M//2+(i_test_set)*n_test:M//2+(i_test_set+1)*n_test, 0::downsample_ratio], np.tile(grid, (n_test,1))), axis=-1).astype(np.float32))\n",
    "                        y_test = torch.from_numpy(y_data[M//2+(i_test_set)*n_test:M//2+(i_test_set+1)*n_test, 0::downsample_ratio, np.newaxis].astype(np.float32))\n",
    "                        \n",
    "                        \n",
    "                        modes = [k_max] * n_fno_layers\n",
    "                        # channel d_f\n",
    "                        layers = [d_f] * (n_fno_layers + 1)\n",
    "                        fc_dim = d_f\n",
    "                        in_dim = 2\n",
    "                        out_dim = 1\n",
    "                        act = \"gelu\"\n",
    "                        pad_ratio = 0.05\n",
    "                        config = {\"model\" : {\"modes\": modes, \"fc_dim\": fc_dim, \"layers\": layers, \"in_dim\": in_dim, \"out_dim\":out_dim, \"act\": act, \"pad_ratio\":pad_ratio},\n",
    "                                  \"train\" : {\"base_lr\": base_lr, \"epochs\": epochs, \"milestones\": milestones, \"scheduler_gamma\": scheduler_gamma, \"batch_size\": batch_size, \n",
    "                                            \"normalization\": normalization, \"dim\": dim}}\n",
    "\n",
    "                        \n",
    "                        data_analysis[i_data_analysis, 4*i_test_set:4*(i_test_set+1)] = test(x_train, y_train, x_test, y_test, model_prefix, config, downsample_ratio, n_fno_layers, k_max, d_f)\n",
    "                        cost = FNN1d_cost(x_test.shape[1], config)\n",
    "    \n",
    "                    data_analysis[i_data_analysis, 4*n_test_sets:4*n_test_sets+6] =  n_train, downsample_ratio, n_fno_layers, k_max, d_f, cost\n",
    "                    i_data_analysis += 1\n",
    "            \n",
    "\n",
    "\n",
    "np.save(prefix+\"data/darcy_analysis_validation_test.npy\", data_analysis)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_data_256_k_max_16_downsample_ratio_1_n_fno_layers_3_d_f_16 0.0008529444739791003 1.8590540672194294e-06 0.02742019775533322 0.0011849994129223384\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24628/2294416430.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                         \u001b[0mdata_analysis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_data_analysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi_test_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_test_set\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownsample_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fno_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFNN1d_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24628/4022556587.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(x_train, y_train, x_test, y_test, model_prefix, config, downsample_ratio, n_fno_layers, k_max, d_f)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_normalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_normalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/central/home/dzhuang/Code/NeuralOperator/models/normalizer.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m \u001b[0;31m# n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# inplace function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prefix = \"/central/groups/esm/dzhuang/cost-accuracy-data/\"\n",
    "heat_u0s    = np.load(prefix+\"heat_u0.npy\")\n",
    "heat_fs     = np.load(prefix+\"heat_f.npy\")\n",
    "heat_us_ref = np.load(prefix+\"heat_u.npy\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_prefix = \"/central/groups/esm/dzhuang/cost-accuracy-data/models/heat_FNO_0_\"\n",
    "\n",
    "n_data_array = [256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "k_max_array = [16, 32, 64, 128]\n",
    "d_f_array = [16, 32, 64, 128]\n",
    "n_fno_layers_array = [3, 4, 5]\n",
    "downsample_ratio_array = [1, 2, 4, 8]\n",
    "\n",
    "\n",
    "#optimization\n",
    "epochs = 1001\n",
    "base_lr = 0.001\n",
    "milestones = [200, 300, 400, 500, 800, 900]\n",
    "scheduler_gamma = 0.5\n",
    "batch_size=32\n",
    "normalization = True\n",
    "dim = []\n",
    "\n",
    "M = 2**15\n",
    "L, Ne_ref = 1.0, 2**12\n",
    "n_test = 8192\n",
    "\n",
    "\n",
    "n_test_sets = 2\n",
    "data_analysis = np.zeros((len(n_data_array)*len(downsample_ratio_array)*len(n_fno_layers_array)*len(k_max_array)*len(d_f_array), 4*n_test_sets + 6)) \n",
    "\n",
    "i_data_analysis = 0\n",
    "\n",
    "for n_train in n_data_array:\n",
    "    for downsample_ratio in downsample_ratio_array:\n",
    "        for n_fno_layers in n_fno_layers_array:\n",
    "            for k_max in k_max_array:\n",
    "                for d_f in d_f_array: \n",
    "                    for i_test_set in range(n_test_sets):\n",
    "                        \n",
    "                        \n",
    "                        Ne = Ne_ref//downsample_ratio\n",
    "                        grid = np.linspace(0, L, Ne+1)\n",
    "                        \n",
    "                        \n",
    "                        x_train = torch.from_numpy(np.stack((heat_u0s[0:n_train, 0::downsample_ratio], heat_fs[0:n_train, 0::downsample_ratio], np.tile(grid, (n_train,1))), axis=-1).astype(np.float32))\n",
    "                        y_train = torch.from_numpy(heat_us_ref[0:n_train, 0::downsample_ratio, np.newaxis].astype(np.float32))\n",
    "                        x_test = torch.from_numpy(np.stack((heat_u0s[M//2+(i_test_set)*n_test:M//2+(i_test_set+1)*n_test, 0::downsample_ratio], heat_fs[M//2+(i_test_set)*n_test:M//2+(i_test_set+1)*n_test, 0::downsample_ratio], np.tile(grid, (n_test,1))), axis=-1).astype(np.float32))\n",
    "                        y_test = torch.from_numpy(heat_us_ref[M//2+(i_test_set)*n_test:M//2+(i_test_set+1)*n_test, 0::downsample_ratio, np.newaxis].astype(np.float32))\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        modes = [k_max] * n_fno_layers\n",
    "                        # channel d_f\n",
    "                        layers = [d_f] * (n_fno_layers + 1)\n",
    "                        fc_dim = d_f\n",
    "                        in_dim = 3\n",
    "                        out_dim = 1\n",
    "                        act = \"gelu\"\n",
    "                        pad_ratio = 0.05\n",
    "                        config = {\"model\" : {\"modes\": modes, \"fc_dim\": fc_dim, \"layers\": layers, \"in_dim\": in_dim, \"out_dim\":out_dim, \"act\": act, \"pad_ratio\":pad_ratio},\n",
    "                                  \"train\" : {\"base_lr\": base_lr, \"epochs\": epochs, \"milestones\": milestones, \"scheduler_gamma\": scheduler_gamma, \"batch_size\": batch_size, \n",
    "                                            \"normalization\": normalization, \"dim\": dim}}\n",
    "\n",
    "                        \n",
    "                        data_analysis[i_data_analysis, 4*i_test_set:4*(i_test_set+1)] = test(x_train, y_train, x_test, y_test, model_prefix, config, downsample_ratio, n_fno_layers, k_max, d_f)\n",
    "                        cost = FNN1d_cost(x_test.shape[1], config)\n",
    "    \n",
    "                    data_analysis[i_data_analysis, 4*n_test_sets:4*n_test_sets+6] =  n_train, downsample_ratio, n_fno_layers, k_max, d_f, cost\n",
    "                    i_data_analysis += 1\n",
    "            \n",
    "\n",
    "\n",
    "np.save(prefix+\"data/heat_analysis_validation_test.npy\", data_analysis)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 4097, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
