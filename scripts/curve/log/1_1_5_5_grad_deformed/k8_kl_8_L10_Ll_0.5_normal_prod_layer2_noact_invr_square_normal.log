--------------------------------------------------------------------------------------------------

target, source = directed_edges[...,0,m], directed_edges[...,1,m]  # target and source nodes of edges  (bsz, max_nedges)

diff_nodes = nodes[torch.arange(batch_size).unsqueeze(1),target] - nodes[torch.arange(batch_size).unsqueeze(1),source]  # (bsz, max_nedges, ndims)
normal_vectors_source = normal_vectors[torch.arange(batch_size).unsqueeze(1),source]  # (bsz, max_nedges, ndims)
gradn_logr = torch.sum(diff_nodes*normal_vectors_source, dim = -1, keepdim = True)/(torch.sum(diff_nodes**2, dim=-1, keepdim=True) + 1e-6)  # (bsz, max_nedges,1)

weights_c, weights_s, weights_0 = self.weights_c[...,m], self.weights_s[...,m], self.weights_0[...,0,m]

edge_local_weights = weights_0 + 2*torch.einsum('ik, bek->bei', weights_c, bases_c[...,m][torch.arange(batch_size).unsqueeze(1),target] * bases_c[...,m][torch.arange(batch_size).unsqueeze(1),source] + bases_s[...,m][torch.arange(batch_size).unsqueeze(1),target] * bases_s[...,m][torch.arange(batch_size).unsqueeze(1),source]) \
                                + 2*torch.einsum('ik, bek->bei', weights_s, bases_c[...,m][torch.arange(batch_size).unsqueeze(1),target] * bases_s[...,m][torch.arange(batch_size).unsqueeze(1),source] - bases_s[...,m][torch.arange(batch_size).unsqueeze(1),target] * bases_c[...,m][torch.arange(batch_size).unsqueeze(1),source])
message = torch.einsum('bei, bei, be->bei', edge_local_weights, f[torch.arange(batch_size).unsqueeze(1),source], node_weights[...,m])

f_out.scatter_add_(dim=1, src=message*gradn_logr, index=target.unsqueeze(2).repeat(1,1,in_channels))

--------------------------------------------------------------------------



(1000,) (1000, 1000, 1) (1000, 1000, 2)
use normalized raw measures
Preprocessing data : computing close_node_pairs
100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:15<00:00, 63.68it/s] 
maximum number of close node pairs is  10000
Casting to tensor
x train:torch.Size([900, 1000, 8]), y train:torch.Size([900, 1000, 1]), x test:torch.Size([100, 1000, 8]), y test:torch.Size([100, 1000, 1])
length of each dim:  tensor([6.5573987960815430, 6.1420288085937500])
kmax = 8, kmax_local = 8
L =  10  L_local =  0.5
In PCNO_train, ndims =  2
Epoch :  0  Time:  2.002  Rel. Train L2 Loss :  0.501427604092492  Rel. Test L2 Loss :  0.3556025004386902  Test L2 Loss :  0.5083443033695221  inv_L_scale:  [1.0, 1.0]
Epoch :  1  Time:  1.642  Rel. Train L2 Loss :  0.2994520006577174  Rel. Test L2 Loss :  0.2442997181415558  Test L2 Loss :  0.3502516758441925  inv_L_scale:  [1.0, 1.0]
Epoch :  2  Time:  1.631  Rel. Train L2 Loss :  0.2222814801004198  Rel. Test L2 Loss :  0.20563739001750947  Test L2 Loss :  0.29242756485939025  inv_L_scale:  [1.0, 1.0]
Epoch :  3  Time:  1.679  Rel. Train L2 Loss :  0.19412294056680468  Rel. Test L2 Loss :  0.1796739548444748  Test L2 Loss :  0.25898674070835115  inv_L_scale:  [1.0, 1.0]
Epoch :  4  Time:  1.639  Rel. Train L2 Loss :  0.17837658365567524  Rel. Test L2 Loss :  0.17535075068473815  Test L2 Loss :  0.25297346234321594  inv_L_scale:  [1.0, 1.0]
Epoch :  5  Time:  1.661  Rel. Train L2 Loss :  0.1718340343899197  Rel. Test L2 Loss :  0.17385563135147095  Test L2 Loss :  0.24786765694618226  inv_L_scale:  [1.0, 1.0]
Epoch :  6  Time:  1.636  Rel. Train L2 Loss :  0.16521112501621246  Rel. Test L2 Loss :  0.15599875688552856  Test L2 Loss :  0.225259450674057  inv_L_scale:  [1.0, 1.0]
Epoch :  7  Time:  1.644  Rel. Train L2 Loss :  0.15945741832256316  Rel. Test L2 Loss :  0.15570834398269653  Test L2 Loss :  0.2249351727962494  inv_L_scale:  [1.0, 1.0]
Epoch :  8  Time:  1.636  Rel. Train L2 Loss :  0.15816322525342305  Rel. Test L2 Loss :  0.15746583819389343  Test L2 Loss :  0.22712014198303224  inv_L_scale:  [1.0, 1.0]
Epoch :  9  Time:  1.64  Rel. Train L2 Loss :  0.1547550509373347  Rel. Test L2 Loss :  0.15420300304889678  Test L2 Loss :  0.22215332686901093  inv_L_scale:  [1.0, 1.0]
Epoch :  10  Time:  1.647  Rel. Train L2 Loss :  0.1514322225915061  Rel. Test L2 Loss :  0.14920709252357484  Test L2 Loss :  0.21469452381134033  inv_L_scale:  [1.0, 1.0]
Epoch :  11  Time:  1.641  Rel. Train L2 Loss :  0.14775639063782162  Rel. Test L2 Loss :  0.14348867475986482  Test L2 Loss :  0.20796955943107606  inv_L_scale:  [1.0, 1.0]
Epoch :  12  Time:  1.647  Rel. Train L2 Loss :  0.14849570234616596  Rel. Test L2 Loss :  0.1448621541261673  Test L2 Loss :  0.20948574423789978  inv_L_scale:  [1.0, 1.0]
Epoch :  13  Time:  1.679  Rel. Train L2 Loss :  0.14673023124535878  Rel. Test L2 Loss :  0.14128195643424987  Test L2 Loss :  0.20400615632534028  inv_L_scale:  [1.0, 1.0]
Epoch :  14  Time:  1.651  Rel. Train L2 Loss :  0.1460162471400367  Rel. Test L2 Loss :  0.14654769599437714  Test L2 Loss :  0.21128061413764954  inv_L_scale:  [1.0, 1.0]
Epoch :  15  Time:  1.655  Rel. Train L2 Loss :  0.14462049384911854  Rel. Test L2 Loss :  0.14750274479389192  Test L2 Loss :  0.2123819351196289  inv_L_scale:  [1.0, 1.0]
Epoch :  16  Time:  1.685  Rel. Train L2 Loss :  0.14226487139860788  Rel. Test L2 Loss :  0.13939984679222106  Test L2 Loss :  0.20097827196121215  inv_L_scale:  [1.0, 1.0]
Epoch :  17  Time:  1.651  Rel. Train L2 Loss :  0.14306014054351382  Rel. Test L2 Loss :  0.13826898574829102  Test L2 Loss :  0.19936771154403687  inv_L_scale:  [1.0, 1.0]
Epoch :  18  Time:  1.652  Rel. Train L2 Loss :  0.13959925558831957  Rel. Test L2 Loss :  0.1428675478696823  Test L2 Loss :  0.20671530365943908  inv_L_scale:  [1.0, 1.0]
Epoch :  19  Time:  1.671  Rel. Train L2 Loss :  0.14040950020154316  Rel. Test L2 Loss :  0.14354363024234773  Test L2 Loss :  0.2070400434732437  inv_L_scale:  [1.0, 1.0]
Epoch :  20  Time:  1.712  Rel. Train L2 Loss :  0.1408095653189553  Rel. Test L2 Loss :  0.1411923897266388  Test L2 Loss :  0.20351145625114442  inv_L_scale:  [1.0, 1.0]
Epoch :  21  Time:  1.692  Rel. Train L2 Loss :  0.1390563092629115  Rel. Test L2 Loss :  0.14419085025787354  Test L2 Loss :  0.20995231688022614  inv_L_scale:  [1.0, 1.0]
Epoch :  22  Time:  1.647  Rel. Train L2 Loss :  0.14149397710959116  Rel. Test L2 Loss :  0.13771139323711395  Test L2 Loss :  0.19942977964878084  inv_L_scale:  [1.0, 1.0]
Epoch :  23  Time:  1.64  Rel. Train L2 Loss :  0.13788517345984777  Rel. Test L2 Loss :  0.1335479176044464  Test L2 Loss :  0.19423857390880583  inv_L_scale:  [1.0, 1.0]
Epoch :  24  Time:  1.682  Rel. Train L2 Loss :  0.13938459290398492  Rel. Test L2 Loss :  0.1333736687898636  Test L2 Loss :  0.19323849081993102  inv_L_scale:  [1.0, 1.0]
Epoch :  25  Time:  1.644  Rel. Train L2 Loss :  0.13743341830041672  Rel. Test L2 Loss :  0.13902981579303741  Test L2 Loss :  0.20167280435562135  inv_L_scale:  [1.0, 1.0]
Epoch :  26  Time:  1.688  Rel. Train L2 Loss :  0.13882560716734993  Rel. Test L2 Loss :  0.13310365617275238  Test L2 Loss :  0.19286020755767821  inv_L_scale:  [1.0, 1.0]
Epoch :  27  Time:  1.641  Rel. Train L2 Loss :  0.1366693537765079  Rel. Test L2 Loss :  0.1350904458761215  Test L2 Loss :  0.19558902263641356  inv_L_scale:  [1.0, 1.0]
Epoch :  28  Time:  1.672  Rel. Train L2 Loss :  0.13663773500257068  Rel. Test L2 Loss :  0.1365947449207306  Test L2 Loss :  0.19828837871551513  inv_L_scale:  [1.0, 1.0]
Epoch :  29  Time:  1.64  Rel. Train L2 Loss :  0.1364652156829834  Rel. Test L2 Loss :  0.13736601829528808  Test L2 Loss :  0.19934388279914855  inv_L_scale:  [1.0, 1.0]
Epoch :  30  Time:  1.642  Rel. Train L2 Loss :  0.13519101950857373  Rel. Test L2 Loss :  0.13484313666820527  Test L2 Loss :  0.19525458216667174  inv_L_scale:  [1.0, 1.0]
Epoch :  31  Time:  1.657  Rel. Train L2 Loss :  0.13451636208428278  Rel. Test L2 Loss :  0.13514883160591126  Test L2 Loss :  0.19634295344352723  inv_L_scale:  [1.0, 1.0]
Epoch :  32  Time:  1.667  Rel. Train L2 Loss :  0.13602434078852335  Rel. Test L2 Loss :  0.13429029226303102  Test L2 Loss :  0.19463937044143675  inv_L_scale:  [1.0, 1.0]
Epoch :  33  Time:  1.66  Rel. Train L2 Loss :  0.13506245579984452  Rel. Test L2 Loss :  0.1428426420688629  Test L2 Loss :  0.2054879277944565  inv_L_scale:  [1.0, 1.0]
Epoch :  34  Time:  1.664  Rel. Train L2 Loss :  0.13507887383302053  Rel. Test L2 Loss :  0.1336650514602661  Test L2 Loss :  0.193845517039299  inv_L_scale:  [1.0, 1.0]
Epoch :  35  Time:  1.658  Rel. Train L2 Loss :  0.13394258081912994  Rel. Test L2 Loss :  0.13433171391487123  Test L2 Loss :  0.19580356955528258  inv_L_scale:  [1.0, 1.0]
Epoch :  36  Time:  1.655  Rel. Train L2 Loss :  0.13408068286048042  Rel. Test L2 Loss :  0.1292135226726532  Test L2 Loss :  0.18726406693458558  inv_L_scale:  [1.0, 1.0]
Epoch :  37  Time:  1.652  Rel. Train L2 Loss :  0.1347015819284651  Rel. Test L2 Loss :  0.13351596891880035  Test L2 Loss :  0.1932578080892563  inv_L_scale:  [1.0, 1.0]
Epoch :  38  Time:  1.65  Rel. Train L2 Loss :  0.13387252204948002  Rel. Test L2 Loss :  0.13185078561306  Test L2 Loss :  0.19129830121994018  inv_L_scale:  [1.0, 1.0]
Epoch :  39  Time:  1.648  Rel. Train L2 Loss :  0.1341459505425559  Rel. Test L2 Loss :  0.13304145395755768  Test L2 Loss :  0.19247276484966278  inv_L_scale:  [1.0, 1.0]
Epoch :  40  Time:  1.65  Rel. Train L2 Loss :  0.1340067454179128  Rel. Test L2 Loss :  0.13345986664295195  Test L2 Loss :  0.19315367221832275  inv_L_scale:  [1.0, 1.0]
Epoch :  41  Time:  1.677  Rel. Train L2 Loss :  0.1337744998269611  Rel. Test L2 Loss :  0.1331041169166565  Test L2 Loss :  0.19286310017108918  inv_L_scale:  [1.0, 1.0]
Epoch :  42  Time:  1.677  Rel. Train L2 Loss :  0.13335482749674055  Rel. Test L2 Loss :  0.13147593259811402  Test L2 Loss :  0.1906110441684723  inv_L_scale:  [1.0, 1.0]
Epoch :  43  Time:  1.642  Rel. Train L2 Loss :  0.13321659081512027  Rel. Test L2 Loss :  0.1373528468608856  Test L2 Loss :  0.19861080408096313  inv_L_scale:  [1.0, 1.0]
Epoch :  44  Time:  1.642  Rel. Train L2 Loss :  0.1336965294016732  Rel. Test L2 Loss :  0.1318015468120575  Test L2 Loss :  0.1913646078109741  inv_L_scale:  [1.0, 1.0]
Epoch :  45  Time:  1.639  Rel. Train L2 Loss :  0.1328664645883772  Rel. Test L2 Loss :  0.13122260987758635  Test L2 Loss :  0.19134356260299681  inv_L_scale:  [1.0, 1.0]
Epoch :  46  Time:  1.651  Rel. Train L2 Loss :  0.13239448765913645  Rel. Test L2 Loss :  0.13192855775356294  Test L2 Loss :  0.19142757296562196  inv_L_scale:  [1.0, 1.0]
Epoch :  47  Time:  1.65  Rel. Train L2 Loss :  0.1323890087339613  Rel. Test L2 Loss :  0.1307571116089821  Test L2 Loss :  0.19025127410888673  inv_L_scale:  [1.0, 1.0]
Epoch :  48  Time:  1.641  Rel. Train L2 Loss :  0.13282808038923474  Rel. Test L2 Loss :  0.13343475341796876  Test L2 Loss :  0.19378663182258607  inv_L_scale:  [1.0, 1.0]
Epoch :  49  Time:  1.686  Rel. Train L2 Loss :  0.13241973658402761  Rel. Test L2 Loss :  0.13418931245803833  Test L2 Loss :  0.19431191682815552  inv_L_scale:  [1.0, 1.0]
Epoch :  50  Time:  1.648  Rel. Train L2 Loss :  0.13243949635161295  Rel. Test L2 Loss :  0.13134040534496308  Test L2 Loss :  0.1904730713367462  inv_L_scale:  [1.0, 1.0]
Epoch :  51  Time:  1.666  Rel. Train L2 Loss :  0.13273805644777087  Rel. Test L2 Loss :  0.13065949201583862  Test L2 Loss :  0.18917499542236327  inv_L_scale:  [1.0, 1.0]
Epoch :  52  Time:  1.668  Rel. Train L2 Loss :  0.133784459233284  Rel. Test L2 Loss :  0.12970886886119842  Test L2 Loss :  0.18900765895843505  inv_L_scale:  [1.0, 1.0]
Epoch :  53  Time:  1.652  Rel. Train L2 Loss :  0.1323588358031379  Rel. Test L2 Loss :  0.1303044867515564  Test L2 Loss :  0.18924292981624602  inv_L_scale:  [1.0, 1.0]
Epoch :  54  Time:  1.668  Rel. Train L2 Loss :  0.1326994146241082  Rel. Test L2 Loss :  0.13216731309890747  Test L2 Loss :  0.19186897754669188  inv_L_scale:  [1.0, 1.0]
Epoch :  55  Time:  1.676  Rel. Train L2 Loss :  0.1323861082394918  Rel. Test L2 Loss :  0.12812438011169433  Test L2 Loss :  0.1866712212562561  inv_L_scale:  [1.0, 1.0]
Epoch :  56  Time:  1.685  Rel. Train L2 Loss :  0.13165460334883797  Rel. Test L2 Loss :  0.1317271375656128  Test L2 Loss :  0.19129590928554535  inv_L_scale:  [1.0, 1.0]
Epoch :  57  Time:  1.671  Rel. Train L2 Loss :  0.1313546030057801  Rel. Test L2 Loss :  0.13111167013645172  Test L2 Loss :  0.19018149733543396  inv_L_scale:  [1.0, 1.0]
Epoch :  58  Time:  1.657  Rel. Train L2 Loss :  0.13210650172498492  Rel. Test L2 Loss :  0.13070983827114105  Test L2 Loss :  0.1896099054813385  inv_L_scale:  [1.0, 1.0]
Epoch :  59  Time:  1.638  Rel. Train L2 Loss :  0.13080446402231852  Rel. Test L2 Loss :  0.1295931124687195  Test L2 Loss :  0.18818721532821656  inv_L_scale:  [1.0, 1.0]
Epoch :  60  Time:  1.685  Rel. Train L2 Loss :  0.13112714701228673  Rel. Test L2 Loss :  0.1301589924097061  Test L2 Loss :  0.18911710262298584  inv_L_scale:  [1.0, 1.0]
Epoch :  61  Time:  1.671  Rel. Train L2 Loss :  0.13117376307646433  Rel. Test L2 Loss :  0.13010402739048005  Test L2 Loss :  0.18897390365600586  inv_L_scale:  [1.0, 1.0]
Epoch :  62  Time:  1.651  Rel. Train L2 Loss :  0.13126808106899263  Rel. Test L2 Loss :  0.129421626329422  Test L2 Loss :  0.18894698441028596  inv_L_scale:  [1.0, 1.0]
Epoch :  63  Time:  1.654  Rel. Train L2 Loss :  0.1308433445956972  Rel. Test L2 Loss :  0.12732638239860536  Test L2 Loss :  0.18558021664619445  inv_L_scale:  [1.0, 1.0]
Epoch :  64  Time:  1.646  Rel. Train L2 Loss :  0.13012097163332834  Rel. Test L2 Loss :  0.12943098366260528  Test L2 Loss :  0.1881452000141144  inv_L_scale:  [1.0, 1.0]
Epoch :  65  Time:  1.645  Rel. Train L2 Loss :  0.1314936261375745  Rel. Test L2 Loss :  0.1284105521440506  Test L2 Loss :  0.18626458048820496  inv_L_scale:  [1.0, 1.0]
Epoch :  66  Time:  1.642  Rel. Train L2 Loss :  0.1305174807045195  Rel. Test L2 Loss :  0.12914549350738525  Test L2 Loss :  0.18834009230136872  inv_L_scale:  [1.0, 1.0]
Epoch :  67  Time:  1.667  Rel. Train L2 Loss :  0.12998766462008157  Rel. Test L2 Loss :  0.12828392088413237  Test L2 Loss :  0.186269633769989  inv_L_scale:  [1.0, 1.0]
Epoch :  68  Time:  1.656  Rel. Train L2 Loss :  0.13069271246592204  Rel. Test L2 Loss :  0.12748097360134125  Test L2 Loss :  0.18573651432991028  inv_L_scale:  [1.0, 1.0]
Epoch :  69  Time:  1.644  Rel. Train L2 Loss :  0.12984305640061697  Rel. Test L2 Loss :  0.1288476300239563  Test L2 Loss :  0.18775469958782195  inv_L_scale:  [1.0, 1.0]
Epoch :  70  Time:  1.642  Rel. Train L2 Loss :  0.12999120715591642  Rel. Test L2 Loss :  0.12922311902046205  Test L2 Loss :  0.18743160843849183  inv_L_scale:  [1.0, 1.0]
Epoch :  71  Time:  1.64  Rel. Train L2 Loss :  0.13026967737409803  Rel. Test L2 Loss :  0.12777822196483613  Test L2 Loss :  0.18616271018981934  inv_L_scale:  [1.0, 1.0]
Epoch :  72  Time:  1.646  Rel. Train L2 Loss :  0.13058674421575334  Rel. Test L2 Loss :  0.1289278668165207  Test L2 Loss :  0.18743981480598448  inv_L_scale:  [1.0, 1.0]
Epoch :  73  Time:  1.654  Rel. Train L2 Loss :  0.12983937892648909  Rel. Test L2 Loss :  0.12856903433799743  Test L2 Loss :  0.1864640998840332  inv_L_scale:  [1.0, 1.0]
Epoch :  74  Time:  1.648  Rel. Train L2 Loss :  0.13015666021241082  Rel. Test L2 Loss :  0.12854634761810302  Test L2 Loss :  0.18729447960853576  inv_L_scale:  [1.0, 1.0]
Epoch :  75  Time:  1.693  Rel. Train L2 Loss :  0.12923942824204762  Rel. Test L2 Loss :  0.13417010486125946  Test L2 Loss :  0.1947721290588379  inv_L_scale:  [1.0, 1.0]
Epoch :  76  Time:  1.656  Rel. Train L2 Loss :  0.12941814028554494  Rel. Test L2 Loss :  0.13003097653388976  Test L2 Loss :  0.18928556501865387  inv_L_scale:  [1.0, 1.0]
Epoch :  77  Time:  1.668  Rel. Train L2 Loss :  0.13018962297174666  Rel. Test L2 Loss :  0.1304357600212097  Test L2 Loss :  0.19035220980644227  inv_L_scale:  [1.0, 1.0]
Epoch :  78  Time:  1.663  Rel. Train L2 Loss :  0.12863304343488483  Rel. Test L2 Loss :  0.13386234283447265  Test L2 Loss :  0.19404365360736847  inv_L_scale:  [1.0, 1.0]
Epoch :  79  Time:  1.664  Rel. Train L2 Loss :  0.1292053335905075  Rel. Test L2 Loss :  0.12770803272724152  Test L2 Loss :  0.18602046251296997  inv_L_scale:  [1.0, 1.0]
Epoch :  80  Time:  1.786  Rel. Train L2 Loss :  0.1293831478887134  Rel. Test L2 Loss :  0.13153434932231903  Test L2 Loss :  0.1911846923828125  inv_L_scale:  [1.0, 1.0]
Epoch :  81  Time:  1.697  Rel. Train L2 Loss :  0.12911908434496985  Rel. Test L2 Loss :  0.12894879698753356  Test L2 Loss :  0.1882096701860428  inv_L_scale:  [1.0, 1.0]
Epoch :  82  Time:  1.738  Rel. Train L2 Loss :  0.12830695231755573  Rel. Test L2 Loss :  0.12765076577663423  Test L2 Loss :  0.1855119788646698  inv_L_scale:  [1.0, 1.0]
Epoch :  83  Time:  1.67  Rel. Train L2 Loss :  0.1293164602915446  Rel. Test L2 Loss :  0.12679925441741943  Test L2 Loss :  0.1845288836956024  inv_L_scale:  [1.0, 1.0]
Epoch :  84  Time:  1.707  Rel. Train L2 Loss :  0.1282268950674269  Rel. Test L2 Loss :  0.1296221387386322  Test L2 Loss :  0.188675097823143  inv_L_scale:  [1.0, 1.0]
Epoch :  85  Time:  1.775  Rel. Train L2 Loss :  0.12938512477609845  Rel. Test L2 Loss :  0.12916014075279236  Test L2 Loss :  0.18774498879909515  inv_L_scale:  [1.0, 1.0]
Epoch :  86  Time:  1.649  Rel. Train L2 Loss :  0.12806807935237885  Rel. Test L2 Loss :  0.12579511404037474  Test L2 Loss :  0.18345243692398072  inv_L_scale:  [1.0, 1.0]
Epoch :  87  Time:  1.654  Rel. Train L2 Loss :  0.12859337757031122  Rel. Test L2 Loss :  0.12599371135234833  Test L2 Loss :  0.1835879600048065  inv_L_scale:  [1.0, 1.0]
Epoch :  88  Time:  1.644  Rel. Train L2 Loss :  0.12891208734777237  Rel. Test L2 Loss :  0.1275591742992401  Test L2 Loss :  0.18634965538978576  inv_L_scale:  [1.0, 1.0]
Epoch :  89  Time:  1.717  Rel. Train L2 Loss :  0.1283324337667889  Rel. Test L2 Loss :  0.12655087172985077  Test L2 Loss :  0.1843632036447525  inv_L_scale:  [1.0, 1.0]
Epoch :  90  Time:  1.807  Rel. Train L2 Loss :  0.12845588839716382  Rel. Test L2 Loss :  0.12597640305757524  Test L2 Loss :  0.18290424346923828  inv_L_scale:  [1.0, 1.0]
Epoch :  91  Time:  1.846  Rel. Train L2 Loss :  0.12760872311062282  Rel. Test L2 Loss :  0.12761071860790252  Test L2 Loss :  0.18506598889827727  inv_L_scale:  [1.0, 1.0]
Epoch :  92  Time:  1.644  Rel. Train L2 Loss :  0.1290745755367809  Rel. Test L2 Loss :  0.12991218209266664  Test L2 Loss :  0.18956220984458924  inv_L_scale:  [1.0, 1.0]
Epoch :  93  Time:  1.689  Rel. Train L2 Loss :  0.12762791064050463  Rel. Test L2 Loss :  0.12884557157754897  Test L2 Loss :  0.18867162466049195  inv_L_scale:  [1.0, 1.0]
Epoch :  94  Time:  1.676  Rel. Train L2 Loss :  0.12747208959526485  Rel. Test L2 Loss :  0.12842415720224382  Test L2 Loss :  0.18782579362392426  inv_L_scale:  [1.0, 1.0]
Epoch :  95  Time:  1.699  Rel. Train L2 Loss :  0.12772549516624876  Rel. Test L2 Loss :  0.1266386604309082  Test L2 Loss :  0.1840675413608551  inv_L_scale:  [1.0, 1.0]
Epoch :  96  Time:  1.647  Rel. Train L2 Loss :  0.12754211803277335  Rel. Test L2 Loss :  0.12644699990749358  Test L2 Loss :  0.1834135717153549  inv_L_scale:  [1.0, 1.0]
Epoch :  97  Time:  1.644  Rel. Train L2 Loss :  0.12718561715549892  Rel. Test L2 Loss :  0.1260284861922264  Test L2 Loss :  0.18346964836120605  inv_L_scale:  [1.0, 1.0]
Epoch :  98  Time:  1.663  Rel. Train L2 Loss :  0.12756793253951604  Rel. Test L2 Loss :  0.12722823917865753  Test L2 Loss :  0.18467142105102538  inv_L_scale:  [1.0, 1.0]
Epoch :  99  Time:  1.645  Rel. Train L2 Loss :  0.1271721790234248  Rel. Test L2 Loss :  0.1245949012041092  Test L2 Loss :  0.18181775629520416  inv_L_scale:  [1.0, 1.0]
Epoch :  100  Time:  1.649  Rel. Train L2 Loss :  0.12711102932691573  Rel. Test L2 Loss :  0.12808515548706054  Test L2 Loss :  0.18631677508354186  inv_L_scale:  [1.0, 1.0]
Epoch :  101  Time:  1.668  Rel. Train L2 Loss :  0.12736303038067287  Rel. Test L2 Loss :  0.12801792621612548  Test L2 Loss :  0.18628498315811157  inv_L_scale:  [1.0, 1.0]
Epoch :  102  Time:  1.653  Rel. Train L2 Loss :  0.12740589446491665  Rel. Test L2 Loss :  0.12615077793598176  Test L2 Loss :  0.18377153992652892  inv_L_scale:  [1.0, 1.0]
Epoch :  103  Time:  1.652  Rel. Train L2 Loss :  0.1273596387770441  Rel. Test L2 Loss :  0.12655726313591004  Test L2 Loss :  0.18416035830974578  inv_L_scale:  [1.0, 1.0]
Epoch :  104  Time:  1.641  Rel. Train L2 Loss :  0.1273875470293893  Rel. Test L2 Loss :  0.12484624981880188  Test L2 Loss :  0.18193278074264527  inv_L_scale:  [1.0, 1.0]
Epoch :  105  Time:  1.647  Rel. Train L2 Loss :  0.1271425320042504  Rel. Test L2 Loss :  0.12500286221504212  Test L2 Loss :  0.1822442388534546  inv_L_scale:  [1.0, 1.0]
Epoch :  106  Time:  1.678  Rel. Train L2 Loss :  0.12638231522507137  Rel. Test L2 Loss :  0.12581090688705443  Test L2 Loss :  0.18321427345275879  inv_L_scale:  [1.0, 1.0]
Epoch :  107  Time:  1.648  Rel. Train L2 Loss :  0.12659313142299652  Rel. Test L2 Loss :  0.12483488351106643  Test L2 Loss :  0.18201002597808838  inv_L_scale:  [1.0, 1.0]
Epoch :  108  Time:  1.647  Rel. Train L2 Loss :  0.12614558978213206  Rel. Test L2 Loss :  0.12363064438104629  Test L2 Loss :  0.1801677131652832  inv_L_scale:  [1.0, 1.0]
Epoch :  109  Time:  1.646  Rel. Train L2 Loss :  0.12639474425050948  Rel. Test L2 Loss :  0.12571372330188751  Test L2 Loss :  0.1828506338596344  inv_L_scale:  [1.0, 1.0]
Epoch :  110  Time:  1.642  Rel. Train L2 Loss :  0.12632525854640536  Rel. Test L2 Loss :  0.12546443343162536  Test L2 Loss :  0.18249412953853608  inv_L_scale:  [1.0, 1.0]
Epoch :  111  Time:  1.641  Rel. Train L2 Loss :  0.1267251980966992  Rel. Test L2 Loss :  0.12372805207967758  Test L2 Loss :  0.18078820645809174  inv_L_scale:  [1.0, 1.0]
Epoch :  112  Time:  1.644  Rel. Train L2 Loss :  0.12645287129614088  Rel. Test L2 Loss :  0.12448368549346923  Test L2 Loss :  0.1818788731098175  inv_L_scale:  [1.0, 1.0]
Epoch :  113  Time:  1.664  Rel. Train L2 Loss :  0.1264670545856158  Rel. Test L2 Loss :  0.12411711215972901  Test L2 Loss :  0.1810881793498993  inv_L_scale:  [1.0, 1.0]
Epoch :  114  Time:  1.643  Rel. Train L2 Loss :  0.12651080714331733  Rel. Test L2 Loss :  0.12781925439834596  Test L2 Loss :  0.1864490044116974  inv_L_scale:  [1.0, 1.0]
Epoch :  115  Time:  1.666  Rel. Train L2 Loss :  0.12606059855884977  Rel. Test L2 Loss :  0.1279138618707657  Test L2 Loss :  0.18608206391334534  inv_L_scale:  [1.0, 1.0]
Epoch :  116  Time:  1.669  Rel. Train L2 Loss :  0.1260718016491996  Rel. Test L2 Loss :  0.12606319665908813  Test L2 Loss :  0.18355857133865355  inv_L_scale:  [1.0, 1.0]
Epoch :  117  Time:  1.656  Rel. Train L2 Loss :  0.12544215242067971  Rel. Test L2 Loss :  0.12692825496196747  Test L2 Loss :  0.18457577586174012  inv_L_scale:  [1.0, 1.0]
Epoch :  118  Time:  1.652  Rel. Train L2 Loss :  0.12611865454249913  Rel. Test L2 Loss :  0.12822558879852294  Test L2 Loss :  0.18654240608215333  inv_L_scale:  [1.0, 1.0]
Epoch :  119  Time:  1.692  Rel. Train L2 Loss :  0.12616032666630214  Rel. Test L2 Loss :  0.12665186196565628  Test L2 Loss :  0.18464018046855926  inv_L_scale:  [1.0, 1.0]
Epoch :  120  Time:  1.658  Rel. Train L2 Loss :  0.12556975861390432  Rel. Test L2 Loss :  0.12546718537807464  Test L2 Loss :  0.18249763309955597  inv_L_scale:  [1.0, 1.0]
Epoch :  121  Time:  1.651  Rel. Train L2 Loss :  0.12540967249208027  Rel. Test L2 Loss :  0.12443998694419861  Test L2 Loss :  0.18115408420562745  inv_L_scale:  [1.0, 1.0]
Epoch :  122  Time:  1.658  Rel. Train L2 Loss :  0.12510490046607123  Rel. Test L2 Loss :  0.12449092775583268  Test L2 Loss :  0.18211205840110778  inv_L_scale:  [1.0, 1.0]
Epoch :  123  Time:  1.641  Rel. Train L2 Loss :  0.12613637506961822  Rel. Test L2 Loss :  0.12555181950330735  Test L2 Loss :  0.18295004725456238  inv_L_scale:  [1.0, 1.0]
Epoch :  124  Time:  1.652  Rel. Train L2 Loss :  0.12517327980862725  Rel. Test L2 Loss :  0.12533751010894775  Test L2 Loss :  0.18328798055648804  inv_L_scale:  [1.0, 1.0]
Epoch :  125  Time:  1.647  Rel. Train L2 Loss :  0.12539746466610166  Rel. Test L2 Loss :  0.12436776846647263  Test L2 Loss :  0.18136552691459656  inv_L_scale:  [1.0, 1.0]
Epoch :  126  Time:  1.665  Rel. Train L2 Loss :  0.12590707725948758  Rel. Test L2 Loss :  0.12417949140071868  Test L2 Loss :  0.18078280806541444  inv_L_scale:  [1.0, 1.0]
Epoch :  127  Time:  1.648  Rel. Train L2 Loss :  0.1252826596630944  Rel. Test L2 Loss :  0.12340801507234574  Test L2 Loss :  0.17969306230545043  inv_L_scale:  [1.0, 1.0]
Epoch :  128  Time:  1.645  Rel. Train L2 Loss :  0.12558625274234347  Rel. Test L2 Loss :  0.1268954372406006  Test L2 Loss :  0.18564836502075197  inv_L_scale:  [1.0, 1.0]
Epoch :  129  Time:  1.656  Rel. Train L2 Loss :  0.12464120911227332  Rel. Test L2 Loss :  0.125944148004055  Test L2 Loss :  0.1836141264438629  inv_L_scale:  [1.0, 1.0]
Epoch :  130  Time:  1.704  Rel. Train L2 Loss :  0.12496975196732416  Rel. Test L2 Loss :  0.12409827649593354  Test L2 Loss :  0.18116536974906922  inv_L_scale:  [1.0, 1.0]