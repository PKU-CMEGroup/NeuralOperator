{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timeit import default_timer\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../../\")\n",
    "from utility.adam import Adam\n",
    "from utility.losses import LpLoss\n",
    "from utility.normalizer import UnitGaussianNormalizer\n",
    "from pcno.geo_utility import compute_edge_gradient_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test.shape:  torch.Size([7500, 7501, 5])  y_test.shape:  torch.Size([7500, 7501, 1]) node_mask.shape:  torch.Size([7500, 7501, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "equal_weights = False\n",
    "data_path = \"../../data/adv_diff_bvp/\"\n",
    "data = np.load(data_path+\"pcno_data.npz\")\n",
    "nnodes, node_mask, nodes = data[\"nnodes\"], data[\"node_mask\"], data[\"nodes\"]\n",
    "node_weights = data[\"node_equal_weights\"] if equal_weights else data[\"node_weights\"]\n",
    "directed_edges, edge_gradient_weights = data[\"directed_edges\"], data[\"edge_gradient_weights\"]\n",
    "features = data[\"features\"]\n",
    "node_measures = data[\"node_measures\"]\n",
    "node_measures_raw = data[\"node_measures_raw\"]\n",
    "indices = np.isfinite(node_measures_raw)\n",
    "node_rhos = np.copy(node_weights)\n",
    "node_rhos[indices] = node_rhos[indices]/node_measures[indices]\n",
    "\n",
    "\n",
    "nnodes = torch.from_numpy(nnodes)\n",
    "node_mask = torch.from_numpy(node_mask)\n",
    "nodes = torch.from_numpy(nodes.astype(np.float32))\n",
    "node_weights = torch.from_numpy(node_weights.astype(np.float32))\n",
    "node_rhos = torch.from_numpy(node_rhos.astype(np.float32))\n",
    "features = torch.from_numpy(features.astype(np.float32))\n",
    "directed_edges = torch.from_numpy(directed_edges.astype(np.int64))\n",
    "edge_gradient_weights = torch.from_numpy(edge_gradient_weights.astype(np.float32))\n",
    "\n",
    "nodes_input = nodes.clone()\n",
    "N = nnodes.shape[0]\n",
    "x_test = torch.cat((features[:,:,[0,2,3]],nodes_input, node_rhos), -1)\n",
    "aux_test = (node_mask,  nodes,  node_weights,  directed_edges,  edge_gradient_weights)\n",
    "y_test = features[:, :, [1]]\n",
    "print('x_test.shape: ',x_test.shape,' y_test.shape: ',y_test.shape , 'node_mask.shape: ',node_mask.shape)\n",
    "test_tuple = (x_test,y_test,node_mask,nodes,node_weights,directed_edges,edge_gradient_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from contextlib import redirect_stdout\n",
    "from pcno.pcno import compute_Fourier_modes, PCNO, PCNO_train\n",
    "\n",
    "def test_model(model,n_test,test_tuple):\n",
    "\n",
    "    x_test,y_test,node_mask,nodes,node_weights,directed_edges,edge_gradient_weights = test_tuple\n",
    "    rel_l2_uniform = []\n",
    "    index_uniform = []\n",
    "    rel_l2_exponential = []\n",
    "    index_exponential = []\n",
    "    rel_l2_linear = []\n",
    "    index_linear = []\n",
    "    myloss = LpLoss(d=1, p=2, size_average=False)\n",
    "    with torch.no_grad():\n",
    "        for i in range(N-n_test,N):\n",
    "            \n",
    "            x, y = x_test[i:i+1].to(device), y_test[i:i+1].to(device)\n",
    "            aux_batch = (\n",
    "            node_mask[i:i+1].to(device), nodes[i:i+1].to(device),\n",
    "            node_weights[i:i+1].to(device), directed_edges[i:i+1].to(device),\n",
    "            edge_gradient_weights[i:i+1].to(device)\n",
    "            )\n",
    "\n",
    "            out = model(x, aux_batch) #.reshape(batch_size_,  -1)\n",
    "\n",
    "            batch_size_ = x.shape[0]\n",
    "            out=out*node_mask[i:i+1].to(device) #mask the padded value with 0,(1 for node, 0 for padding)\n",
    "            test_rel_l2 = myloss(out.view(batch_size_,-1), y.view(batch_size_,-1)).item()\n",
    "            # test_l2 = myloss.abs(out.view(batch_size_,-1), y.view(batch_size_,-1)).item()\n",
    "            if i%3==0:\n",
    "                rel_l2_uniform.append(test_rel_l2)\n",
    "                index_uniform.append(i)\n",
    "            elif i%3 ==1:\n",
    "                rel_l2_exponential.append(test_rel_l2)\n",
    "                index_exponential.append(i)\n",
    "            else:\n",
    "                rel_l2_linear.append(test_rel_l2)\n",
    "                index_linear.append(i)            \n",
    "            # print(f'test index: {i}, test_l2: {test_l2}, test_rel_l2: {test_rel_l2}, test_type: {type_dict[i%3]}')\n",
    "    return   rel_l2_uniform,  rel_l2_exponential, rel_l2_linear, index_uniform, index_exponential, index_linear\n",
    "\n",
    "def sorted_result( rel_l2_uniform,  rel_l2_exponential, rel_l2_linear, index_uniform, index_exponential, index_linear):\n",
    "\n",
    "\n",
    "    uniform_sorted = sorted(enumerate(rel_l2_uniform), key=lambda x: x[1], reverse=True)\n",
    "    exponential_sorted = sorted(enumerate(rel_l2_exponential), key=lambda x: x[1], reverse=True)\n",
    "    linear_sorted = sorted(enumerate(rel_l2_linear), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    average_loss_list = [sum(rel_l2_uniform)/len(rel_l2_uniform),sum(rel_l2_exponential)/len(rel_l2_exponential),sum(rel_l2_linear)/len(rel_l2_linear)]\n",
    "    average_loss_list.append((sum(average_loss_list)/len(average_loss_list)))\n",
    "    print('average rel_l2 of uniform:  ',round(average_loss_list[0],5) ,flush = True)\n",
    "    print('average rel_l2 of exponential:  ',round(average_loss_list[1],5))\n",
    "    print('average rel_l2 of linear:  ',round(average_loss_list[2],5))\n",
    "    print('average rel_l2 of mixed:  ',round(average_loss_list[3],5))\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "    n = 1\n",
    "    index_uniform_3 = [index_uniform[uniform_sorted[0][0]],index_uniform[uniform_sorted[len(uniform_sorted)//2][0]],index_uniform[uniform_sorted[-1][0]]]\n",
    "    index_exponential_3 = [index_exponential[exponential_sorted[0][0]],index_exponential[exponential_sorted[len(exponential_sorted)//2][0]],index_exponential[exponential_sorted[-1][0]]]\n",
    "    index_linear_3 = [index_linear[linear_sorted[0][0]],index_linear[linear_sorted[len(linear_sorted)//2][0]],index_linear[linear_sorted[-1][0]]]\n",
    "    for j in range(n):\n",
    "        print(f'{j+1}th worst rel_l2 of uniform:  ',round(uniform_sorted[j][1],5), ' index: ',index_uniform[uniform_sorted[j][0]])\n",
    "        print(f'{j+1}th worst rel_l2 of exponential:  ',round(exponential_sorted[j][1],5), ' index: ',index_exponential[exponential_sorted[j][0]])\n",
    "        print(f'{j+1}th worst rel_l2 of linear:  ',round(linear_sorted[j][1],5), ' index: ',index_linear[linear_sorted[j][0]])\n",
    "        print()\n",
    "    print('medium rel_l2 of uniform: ',round(uniform_sorted[len(uniform_sorted)//2][1],5), ' index: ',index_uniform_3[1])\n",
    "    print('medium rel_l2 of exponential: ',round(exponential_sorted[len(exponential_sorted)//2][1],5), ' index: ',index_exponential_3[1])\n",
    "    print('medium rel_l2 of linear: ',round(linear_sorted[len(linear_sorted)//2][1],5), ' index: ',index_linear_3[1])\n",
    "    print()\n",
    "    for j in range(n):    \n",
    "        print(f'{j+1}th best rel_l2 of uniform:  ',round(uniform_sorted[-j-1][1],5), ' index: ',index_uniform[uniform_sorted[-j-1][0]])\n",
    "        print(f'{j+1}th best rel_l2 of exponential:  ',round(exponential_sorted[-j-1][1],5), ' index: ',index_exponential[exponential_sorted[-j-1][0]])\n",
    "        print(f'{j+1}th best rel_l2 of linear:  ',round(linear_sorted[-j-1][1],5), ' index: ',index_linear[linear_sorted[-j-1][0]],flush = True)\n",
    "\n",
    " \n",
    "        \n",
    "    return average_loss_list,index_uniform_3,index_exponential_3,index_linear_3\n",
    "\n",
    "def myplot(average_loss_list,index_uniform_3,index_exponential_3,index_linear_3,save_figure_path,\n",
    "           model,test_tuple):\n",
    "\n",
    "    x_test,y_test,node_mask,nodes,node_weights,directed_edges,edge_gradient_weights = test_tuple\n",
    "    myloss = LpLoss(d=1, p=2, size_average=False)\n",
    "    index_dict = {0:index_uniform_3,1:index_exponential_3,2:index_linear_3}\n",
    "    type_dict = {0:'uniform', 1:'exponential', 2:'linear'}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k in range(3):\n",
    "            index_list = index_dict[k]\n",
    "            test_type = type_dict[k]\n",
    "\n",
    "            fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "            for col, title in enumerate(['truth', 'prediction', f'error, average = {round(average_loss_list[k],4)}']):\n",
    "                axs[0, col].set_title(title)\n",
    "            for j in range(len(index_list)):\n",
    "                i = index_list[j]\n",
    "                x, y = x_test[i:i+1].to(device), y_test[i:i+1].to(device)\n",
    "                aux_batch = (\n",
    "                node_mask[i:i+1].to(device), nodes[i:i+1].to(device),\n",
    "                node_weights[i:i+1].to(device), directed_edges[i:i+1].to(device),\n",
    "                edge_gradient_weights[i:i+1].to(device)\n",
    "                )\n",
    "                out = model(x, aux_batch) #.reshape(batch_size_,  -1)\n",
    "\n",
    "                batch_size_ = x.shape[0]\n",
    "                out = out*node_mask[i:i+1].to(device) #mask the padded value with 0,(1 for node, 0 for padding)\n",
    "                test_rel_l2 = myloss(out.view(batch_size_,-1), y.view(batch_size_,-1)).item()\n",
    "\n",
    "\n",
    "\n",
    "                axs[j, 0].plot(nodes[i], y_test[i], \"o\", markersize=1)\n",
    "                # axs[j, 0].set_title('truth')\n",
    "                axs[j, 1].plot(nodes[i], out.reshape(-1).detach().cpu(), \"o\", markersize=1)\n",
    "                # axs[j, 1].set_title('prediction')\n",
    "\n",
    "                error = out.reshape(-1).detach().cpu()-y_test[i].reshape(-1)\n",
    "                axs[j, 2].plot(nodes[i], error, \"o\", markersize=1,label = f'loss={round(test_rel_l2,3)}')\n",
    "                axs[j, 2].legend()\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            if not os.path.exists(save_figure_path):\n",
    "                os.makedirs(save_figure_path)\n",
    "            fig.savefig(save_figure_path + f'test_{test_type}.png', format='png', bbox_inches='tight') \n",
    "            # fig.savefig(f'train_{model_train_type}_test_{test_type}.pdf', format='pdf', bbox_inches='tight') \n",
    "            plt.close(fig)\n",
    "\n",
    "n_test = 600\n",
    "device = 'cuda'\n",
    "# model_train_type_dict = ['uniform', 'exponential','linear', 'mixed']\n",
    "# model_n_train_dict =[500,1000,1500]\n",
    "model_train_sp_L_dict = [False, 'together', 'independently']\n",
    "model_train_type_dict = ['mixed']\n",
    "model_n_train_dict =[1000]\n",
    "# model_train_sp_L_dict = [False]\n",
    "equal_weight = False\n",
    "\n",
    "with open('notes/output.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        for model_train_type in model_train_type_dict:\n",
    "            for model_n_train in model_n_train_dict:\n",
    "                for model_train_sp_L in model_train_sp_L_dict:\n",
    "\n",
    "                    if equal_weight:\n",
    "                        model_path = f'model/pcno_adv_{model_n_train}_equal_weight/{model_train_type}_{model_train_sp_L}.pth'\n",
    "                    else:\n",
    "                        model_path = f'model/pcno_adv_{model_n_train}/{model_train_type}_{model_train_sp_L}.pth'\n",
    "                    k_max = 32\n",
    "                    ndim = 1\n",
    "\n",
    "\n",
    "                    modes = compute_Fourier_modes(ndim, [k_max], [15.0])\n",
    "                    modes = torch.tensor(modes, dtype=torch.float).to(device)\n",
    "                    model = PCNO(ndim, modes, nmeasures=1,\n",
    "                                layers=[128,128,128,128,128],\n",
    "                                fc_dim=128,\n",
    "                                in_dim=x_test.shape[-1], out_dim=y_test.shape[-1],\n",
    "                                train_sp_L = model_train_sp_L,\n",
    "                                act='gelu').to(device)\n",
    "                    checkpoint = torch.load(model_path, map_location=device)\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                    print('\\nload model state dict from ' + model_path, flush = True)\n",
    "                    save_figure_path = f'figure/pcno_adv_{model_n_train}/{model_train_type}_{model_train_sp_L}/'\n",
    "\n",
    "                    rel_l2_uniform,  rel_l2_exponential, rel_l2_linear, index_uniform, index_exponential, index_linear = test_model(model,n_test,test_tuple)\n",
    "                    average_loss_list,index_uniform_3,index_exponential_3,index_linear_3 = sorted_result( rel_l2_uniform,  rel_l2_exponential, rel_l2_linear, index_uniform, index_exponential, index_linear)\n",
    "                    myplot(average_loss_list,index_uniform_3,index_exponential_3,index_linear_3,save_figure_path,\n",
    "                            model,test_tuple)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{500: {'average': {}, 'worst': {}}, 1000: {'average': {'mixed_False_state_dict': {'uniform': 0.01727, 'exponential': 0.05875, 'linear': 0.02777, 'mixed': 0.0346}, 'mixed_together_state_dict': {'uniform': 0.01733, 'exponential': 0.06761, 'linear': 0.02898, 'mixed': 0.03798}, 'mixed_independently_state_dict': {'uniform': 0.01796, 'exponential': 0.06853, 'linear': 0.03107, 'mixed': 0.03918}}, 'worst': {'mixed_False_state_dict': {'uniform': {'1th_worst': 0.05978}, 'exponential': {'1th_worst': 0.14071}, 'linear': {'1th_worst': 0.0748}}, 'mixed_together_state_dict': {'uniform': {'1th_worst': 0.05874}, 'exponential': {'1th_worst': 0.15225}, 'linear': {'1th_worst': 0.07215}}, 'mixed_independently_state_dict': {'uniform': {'1th_worst': 0.06119}, 'exponential': {'1th_worst': 0.14775}, 'linear': {'1th_worst': 0.089}}}}, 1500: {'average': {}, 'worst': {}}}\n",
      "# average loss of datasize 500\n",
      "|train_type | uniform | exponential | linear | mixed |\n",
      "| :---: | :---: | :---: | :---: | :---: |\n",
      "\n",
      "\n",
      "\n",
      "# average loss of datasize 1000\n",
      "|train_type | uniform | exponential | linear | mixed |\n",
      "| :---: | :---: | :---: | :---: | :---: |\n",
      "| mixed_False_state_dict | 0.01727 | 0.05875 | 0.02777 | 0.0346 |\n",
      "| mixed_independently_state_dict | 0.01796 | 0.06853 | 0.03107 | 0.03918 |\n",
      "| mixed_together_state_dict | 0.01733 | 0.06761 | 0.02898 | 0.03798 |\n",
      "\n",
      "\n",
      "# average loss of datasize 1500\n",
      "|train_type | uniform | exponential | linear | mixed |\n",
      "| :---: | :---: | :---: | :---: | :---: |\n",
      "\n",
      "\n",
      "\n",
      "# biggest loss of datasize 500\n",
      "|train_type | uniform | exponential | linear | mixed |\n",
      "| :---: | :---: | :---: | :---: | :---: |\n",
      "\n",
      "\n",
      "\n",
      "# biggest loss of datasize 1000\n",
      "|train_type | uniform | exponential | linear | mixed |\n",
      "| :---: | :---: | :---: | :---: | :---: |\n",
      "| mixed_False_state_dict | 0.05978 | 0.14071 | 0.0748 | - |\n",
      "| mixed_independently_state_dict | 0.06119 | 0.14775 | 0.089 | - |\n",
      "| mixed_together_state_dict | 0.05874 | 0.15225 | 0.07215 | - |\n",
      "\n",
      "\n",
      "# biggest loss of datasize 1500\n",
      "|train_type | uniform | exponential | linear | mixed |\n",
      "| :---: | :---: | :---: | :---: | :---: |\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_file(file_path):\n",
    "    '''\n",
    "    data[datasize][loss_type][model_type]\n",
    "    datasize:  500, 1000, 1500\n",
    "    loss_type: average, worst\n",
    "    model_type: exponential_False , ...\n",
    "    '''\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    if \"equal_weight\" in lines[0]:\n",
    "        data = {\n",
    "            1000: \n",
    "            {\n",
    "            'average': {},\n",
    "            'worst': {}\n",
    "            },  \n",
    "            equal_weight: True\n",
    "        }\n",
    "    else:\n",
    "        data = {\n",
    "            500:\n",
    "            {\n",
    "            'average': {},\n",
    "            'worst': {}\n",
    "            },\n",
    "            1000:\n",
    "            {\n",
    "            'average': {},\n",
    "            'worst': {}\n",
    "            },\n",
    "            1500:\n",
    "            {\n",
    "            'average': {},\n",
    "            'worst': {}\n",
    "            },\n",
    "             equal_weight: False\n",
    "            \n",
    "        }\n",
    "    current_model = None\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('load model'):\n",
    "            # Extract the model name from the line.\n",
    "            match = re.search(r'load model state dict from (\\S+)', line)\n",
    "            if match:\n",
    "                current_model = match.group(1).split('/')[-1].replace('.pth', '')\n",
    "                datasize = int(next(item for item in match.group(1).split('/')[1].split('_') if item.isdigit()))\n",
    "                data[datasize]['average'][current_model] = {}\n",
    "                data[datasize]['worst'][current_model] = {}\n",
    "\n",
    "        elif current_model and 'average rel_l2 of ' in line:\n",
    "            key = line.split()[-2].replace(':', '')\n",
    "            value = line.split()[-1]\n",
    "            data[datasize]['average'][current_model][key.strip()] = float(value.strip())\n",
    "\n",
    "        elif current_model and '1th worst rel_l2 of ' in line:\n",
    "            key = line.split()[-4].replace(':', '')\n",
    "            value = line.split()[-3]\n",
    "            data[datasize]['worst'][current_model][key.strip()] = {'1th_worst': float(value.strip().split()[0])}\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_markdown_table(data, datasize, table_type):\n",
    "    def format_row(keys, row_data):\n",
    "        return \" | \".join([str(row_data.get(key, '-')) for key in keys])\n",
    "\n",
    "    header = \"|train_type | uniform | exponential | linear | mixed |\\n\"\n",
    "    format_header = \"| :---: | :---: | :---: | :---: | :---: |\\n\"\n",
    "\n",
    "    rows = []\n",
    "    train_types = sorted(data[datasize][table_type].keys())\n",
    "    columns = ['uniform', 'exponential', 'linear', 'mixed']\n",
    "\n",
    "    for train_type in train_types:\n",
    "        row_data = data[datasize][table_type][train_type]\n",
    "        if table_type == 'average':\n",
    "            formatted_row = f\"| {train_type} | {format_row(columns, row_data)} |\"\n",
    "        elif table_type == 'worst':\n",
    "            formatted_row = f\"| {train_type} | {format_row(columns, {col: val['1th_worst'] for col, val in row_data.items()})} |\"\n",
    "        rows.append(formatted_row)\n",
    "\n",
    "    if table_type == 'average':\n",
    "        title = f'# average loss of datasize {datasize}, equal_weight = {data[equal_weight]}\\n'\n",
    "    elif table_type == 'worst':\n",
    "        title = f'# biggest loss of datasize {datasize}\\n'\n",
    "\n",
    "    return title + header + format_header + \"\\n\".join(rows) + \"\\n\\n\"\n",
    "\n",
    "\n",
    "# Parse the input file\n",
    "parsed_data = parse_file('notes/output.txt')\n",
    "print(parsed_data)\n",
    "\n",
    "# Optionally write to files\n",
    "with open('notes/loss1.md', 'w') as f:\n",
    "    for datasize in [500,1000,1500]:\n",
    "        avg_loss_md = generate_markdown_table(parsed_data, datasize, 'average')\n",
    "        # Print or save to files\n",
    "        print(avg_loss_md)\n",
    "        f.write(avg_loss_md)\n",
    "\n",
    "    for datasize in [500,1000,1500]:\n",
    "        worst_loss_md = generate_markdown_table(parsed_data, datasize, 'worst')\n",
    "        print(worst_loss_md)\n",
    "        f.write(worst_loss_md)\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
