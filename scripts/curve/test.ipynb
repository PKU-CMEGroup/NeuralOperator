{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timeit import default_timer\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../../\")\n",
    "from utility.adam import Adam\n",
    "from utility.losses import LpLoss\n",
    "from utility.normalizer import UnitGaussianNormalizer\n",
    "from pcno.geo_utility import compute_edge_gradient_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use normalized raw measures\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../../data/curve/\"\n",
    "data = np.load(data_path+\"pcno_curve_data_3_3_grad.npz\")\n",
    "\n",
    "equal_weights = False\n",
    "nnodes, node_mask, nodes = data[\"nnodes\"], data[\"node_mask\"], data[\"nodes\"]\n",
    "node_weights = data[\"node_measures_raw\"]\n",
    "# print('use node_weight')\n",
    "node_weights = node_weights/np.amax(np.sum(node_weights, axis = 1))\n",
    "print('use normalized raw measures')\n",
    "directed_edges, edge_gradient_weights = data[\"directed_edges\"], data[\"edge_gradient_weights\"]\n",
    "features = data[\"features\"]\n",
    "node_measures = data[\"node_measures\"]\n",
    "node_measures_raw = data[\"node_measures_raw\"]\n",
    "indices = np.isfinite(node_measures_raw)\n",
    "node_rhos = np.copy(node_weights)\n",
    "node_rhos[indices] = node_rhos[indices]/node_measures[indices]\n",
    "\n",
    "\n",
    "nnodes = torch.from_numpy(nnodes)\n",
    "node_mask = torch.from_numpy(node_mask)\n",
    "nodes = torch.from_numpy(nodes.astype(np.float32))\n",
    "node_weights = torch.from_numpy(node_weights.astype(np.float32))\n",
    "node_rhos = torch.from_numpy(node_rhos.astype(np.float32))\n",
    "features = torch.from_numpy(features.astype(np.float32))\n",
    "directed_edges = torch.from_numpy(directed_edges.astype(np.int64))\n",
    "edge_gradient_weights = torch.from_numpy(edge_gradient_weights.astype(np.float32))\n",
    "\n",
    "nodes_input = nodes.clone()\n",
    "N = 1000\n",
    "n_train, n_test = 900, 100\n",
    "\n",
    "\n",
    "# x_train, x_test = torch.cat((features[:n_train, :, :1], nodes_input[:n_train, ...], node_rhos[:n_train, ...]), -1), torch.cat((features[-n_test:, :, :1],nodes_input[-n_test:, ...], node_rhos[-n_test:, ...]),-1)\n",
    "\n",
    "# aux_train       = (node_mask[0:n_train,...], nodes[0:n_train,...], node_weights[0:n_train,...], directed_edges[0:n_train,...], edge_gradient_weights[0:n_train,...])\n",
    "# aux_test        = (node_mask[-n_test:,...],  nodes[-n_test:,...],  node_weights[-n_test:,...],  directed_edges[-n_test:,...],  edge_gradient_weights[-n_test:,...])\n",
    "\n",
    "# y_train, y_test = features[:n_train, :, 1:],     features[-n_test:, :, 1:]\n",
    "x_all = torch.cat((features[:, :, :1], nodes_input, node_rhos), -1)\n",
    "y_all = features[:, :, 1:]\n",
    "aux_all = (node_mask, nodes, node_weights, directed_edges, edge_gradient_weights)\n",
    "\n",
    "normalization_x = False\n",
    "normalization_y = True\n",
    "normalization_dim_x = []\n",
    "normalization_dim_y = []\n",
    "non_normalized_dim_x = 4\n",
    "non_normalized_dim_y = 0\n",
    "\n",
    "config = {\"train\" : {\"normalization_x\": normalization_x,\"normalization_y\": normalization_y, \n",
    "                     \"normalization_dim_x\": normalization_dim_x, \"normalization_dim_y\": normalization_dim_y, \n",
    "                     \"non_normalized_dim_x\": non_normalized_dim_x, \"non_normalized_dim_y\": non_normalized_dim_y}\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from contextlib import redirect_stdout\n",
    "from pcno.pcno import compute_Fourier_modes, PCNO, PCNO_train\n",
    "from generate_curves_data import compute_unit_normals\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 17\n",
    "def test_normalizer(all_tuple, config, model_n_train):\n",
    "    x_all,y_all,node_mask,nodes,node_weights,directed_edges,edge_gradient_weights = all_tuple\n",
    "    x_all_copy = x_all.clone()\n",
    "    y_all_copy = y_all.clone()    \n",
    "\n",
    "    normalization_x, normalization_y = config[\"train\"][\"normalization_x\"], config[\"train\"][\"normalization_y\"]\n",
    "    normalization_dim_x, normalization_dim_y = config[\"train\"][\"normalization_dim_x\"], config[\"train\"][\"normalization_dim_y\"]\n",
    "    non_normalized_dim_x, non_normalized_dim_y = config[\"train\"][\"non_normalized_dim_x\"], config[\"train\"][\"non_normalized_dim_y\"]\n",
    "\n",
    "\n",
    "    x_train = x_all_copy[:model_n_train]\n",
    "    y_train = y_all_copy[:model_n_train]\n",
    "\n",
    "    if normalization_x:\n",
    "        x_normalizer = UnitGaussianNormalizer(x_train, non_normalized_dim = non_normalized_dim_x, normalization_dim=normalization_dim_x)\n",
    "        x_test = x_normalizer.encode(x_all_copy)\n",
    "    else:\n",
    "        x_normalizer = None\n",
    "        x_test =  x_all_copy\n",
    "    if normalization_y:\n",
    "        y_normalizer = UnitGaussianNormalizer(y_train, non_normalized_dim = non_normalized_dim_y, normalization_dim=normalization_dim_y)\n",
    "        y_test = y_normalizer.encode(y_all_copy)\n",
    "        y_normalizer.to(device)\n",
    "    else:\n",
    "        y_normalizer = None\n",
    "        y_test = y_all_copy\n",
    "    test_tuple = (x_test, y_test, node_mask,nodes,node_weights,directed_edges,edge_gradient_weights)\n",
    "\n",
    "    return test_tuple, y_normalizer\n",
    "\n",
    "def test_model(model,n_test,test_tuple, y_normalizer):\n",
    "\n",
    "    x_test,y_test,node_mask,nodes,node_weights,directed_edges,edge_gradient_weights = test_tuple\n",
    "\n",
    "    rel_l2 = []\n",
    "    index = []\n",
    "    myloss = LpLoss(d=1, p=2, size_average=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(N-n_test,N):\n",
    "            \n",
    "            x, y = x_test[i:i+1].to(device), y_test[i:i+1].to(device)\n",
    "            aux_batch = (\n",
    "            node_mask[i:i+1].to(device), nodes[i:i+1].to(device),\n",
    "            node_weights[i:i+1].to(device), directed_edges[i:i+1].to(device),\n",
    "            edge_gradient_weights[i:i+1].to(device)\n",
    "            )\n",
    "\n",
    "            out = model(x, aux_batch) #.reshape(batch_size_,  -1)\n",
    "            if y_normalizer:\n",
    "                out = y_normalizer.decode(out)\n",
    "                y = y_normalizer.decode(y)\n",
    "            batch_size_ = x.shape[0]\n",
    "            out = out * node_mask[i:i+1].to(device) #mask the padded value with 0,(1 for node, 0 for padding)\n",
    "            test_rel_l2 = myloss(out.view(batch_size_,-1), y.view(batch_size_,-1)).item()\n",
    "            # test_l2 = myloss.abs(out.view(batch_size_,-1), y.view(batch_size_,-1)).item()\n",
    "\n",
    "            rel_l2.append(test_rel_l2)\n",
    "            index.append(i)            \n",
    "            print(f'test index: {i}, test_rel_l2: {test_rel_l2}')\n",
    "    return  rel_l2, index\n",
    "\n",
    "def sorted_result( rel_l2, index):\n",
    "\n",
    "\n",
    "    sorted_l2 = sorted(enumerate(rel_l2), key=lambda x: x[1], reverse=True)\n",
    "    average_loss = sum(rel_l2)/len(rel_l2)\n",
    "    print('average_rel_l2_of all :  ',round(average_loss,5), flush = True)\n",
    "    print()\n",
    "    n = 3\n",
    "    index_3 = [index[sorted_l2[0][0]],index[sorted_l2[len(sorted_l2)//2][0]],index[sorted_l2[-1][0]]]\n",
    "    for j in range(n):\n",
    "        print(f'{j+1}th_worst_rel_l2_of all :  ',round(sorted_l2[j][1],5), ' index : ',index[sorted_l2[j][0]])\n",
    "        print('medium_rel_l2_of all : ',round(sorted_l2[len(sorted_l2)//2][1],5), ' index : ',index_3[1])\n",
    "    for j in range(n):\n",
    "        print(f'{j+1}th_best_rel_l2_of all :  ',round(sorted_l2[-j-1][1],5), ' index : ',index[sorted_l2[-j-1][0]],flush = True)\n",
    "    print()\n",
    "    return average_loss,index_3\n",
    "\n",
    "\n",
    "\n",
    "def myplot(index_plot,save_figure_path,\n",
    "           model,test_tuple,y_normalizer):\n",
    "\n",
    "    x_test,y_test,node_mask,nodes,node_weights,directed_edges,edge_gradient_weights = test_tuple\n",
    "    myloss = LpLoss(d=1, p=2, size_average=False)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        fig, axs = plt.subplots(len(index_plot), 4, figsize=(20, 4*len(index_plot)))\n",
    "\n",
    "        for j in range(len(index_plot)):\n",
    "            i = index_plot[j]\n",
    "            x, y = x_test[i:i+1].to(device), y_test[i:i+1].to(device)\n",
    "            aux_batch = (\n",
    "            node_mask[i:i+1].to(device), nodes[i:i+1].to(device),\n",
    "            node_weights[i:i+1].to(device), directed_edges[i:i+1].to(device),\n",
    "            edge_gradient_weights[i:i+1].to(device)\n",
    "            )\n",
    "            out = model(x, aux_batch)\n",
    "            if y_normalizer:\n",
    "                out = y_normalizer.decode(out)\n",
    "                y = y_normalizer.decode(y)\n",
    "            batch_size_ = x.shape[0]\n",
    "            out = out * node_mask[i:i+1].to(device)\n",
    "            test_rel_l2 = myloss(out.view(batch_size_,-1), y.view(batch_size_,-1)).item()\n",
    "\n",
    "            nodes_plot = nodes[i].detach().cpu()\n",
    "            normal_plot = compute_unit_normals(nodes_plot, None)\n",
    "            f_plot = x_test[i:i+1][:,:,0].reshape(-1).detach().cpu()\n",
    "            g_plot = y.reshape(-1).detach().cpu()\n",
    "            out_plot = out.reshape(-1).detach().cpu()\n",
    "            error_plot = out_plot - g_plot\n",
    "\n",
    "            # Consistent color scale for g and prediction\n",
    "            vmin_go = min(g_plot.min().item(), out_plot.min().item())\n",
    "            vmax_go = max(g_plot.max().item(), out_plot.max().item())\n",
    "            norm_go = mpl.colors.Normalize(vmin=vmin_go, vmax=vmax_go)\n",
    "\n",
    "            # Symmetric color scale around 0 for error\n",
    "            vmax_err = torch.max(torch.abs(error_plot)).item()\n",
    "            norm_err = mpl.colors.TwoSlopeNorm(vmin=-vmax_err, vcenter=0.0, vmax=vmax_err)\n",
    "\n",
    "            axs[j,0].plot(nodes_plot[:, 0], nodes_plot[:, 1], color='blue', alpha=0.5)\n",
    "            scatter_f = axs[j,0].scatter(nodes_plot[:, 0], nodes_plot[:, 1], c=f_plot, cmap='viridis', s=40)\n",
    "            axs[j,0].quiver(nodes_plot[:, 0], nodes_plot[:, 1], normal_plot[:, 0], normal_plot[:, 1], color='red', scale=10, width=0.001, alpha=0.7)\n",
    "            axs[j,0].set_title('Input f(x)')\n",
    "            axs[j,0].axis('equal')\n",
    "            fig.colorbar(scatter_f, ax=axs[j,0])\n",
    "\n",
    "            axs[j,1].plot(nodes_plot[:, 0], nodes_plot[:, 1], color='blue', alpha=0.5)\n",
    "            scatter_g = axs[j,1].scatter(nodes_plot[:, 0], nodes_plot[:, 1], c=g_plot, cmap='viridis', s=40, norm=norm_go)\n",
    "            axs[j,1].set_title('Ground Truth g(x)')\n",
    "            axs[j,1].axis('equal')\n",
    "            fig.colorbar(scatter_g, ax=axs[j,1])\n",
    "\n",
    "            axs[j,2].plot(nodes_plot[:, 0], nodes_plot[:, 1], color='blue', alpha=0.5)\n",
    "            scatter_out = axs[j,2].scatter(nodes_plot[:, 0], nodes_plot[:, 1], c=out_plot, cmap='viridis', s=40, norm=norm_go)\n",
    "            axs[j,2].set_title('Prediction g_pred(x)')\n",
    "            axs[j,2].axis('equal')\n",
    "            fig.colorbar(scatter_out, ax=axs[j,2])\n",
    "\n",
    "            axs[j,3].plot(nodes_plot[:, 0], nodes_plot[:, 1], color='blue', alpha=0.5)\n",
    "            scatter_error = axs[j,3].scatter(nodes_plot[:, 0], nodes_plot[:, 1], c=error_plot, cmap='coolwarm', s=40, norm=norm_err)\n",
    "            axs[j,3].set_title(f'Error, loss = {round(test_rel_l2,5)}')\n",
    "            axs[j,3].axis('equal')\n",
    "            fig.colorbar(scatter_error, ax=axs[j,3])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if not os.path.exists(save_figure_path):\n",
    "            os.makedirs(save_figure_path)\n",
    "        fig.savefig(save_figure_path + f'test_{index_plot}.png', format='png', bbox_inches='tight') \n",
    "        plt.close(fig)\n",
    "\n",
    "n_test = 1000\n",
    "device = 'cuda'\n",
    "\n",
    "with open('output.txt', 'a') as f:\n",
    "    with redirect_stdout(f):\n",
    "\n",
    "        model_path = 'E:/codes/mygithub2/scripts/curve/' + f'model/PCNO_curve_model_k8_L10.pth'\n",
    "        save_figure_path = 'E:/codes/mygithub2/scripts/curve/' + f'figures/'\n",
    "\n",
    "\n",
    "        k_max = 8\n",
    "        ndim = 2\n",
    "        L = 10\n",
    "\n",
    "        model_train_inv_L_scale = False\n",
    "        modes = compute_Fourier_modes(ndim, [k_max,k_max], [L,L])\n",
    "        modes = torch.tensor(modes, dtype=torch.float).to(device)\n",
    "        model = PCNO(ndim, modes, nmeasures=1,\n",
    "                    layers=[128,128,128,128,128],\n",
    "                    fc_dim=128,\n",
    "                    in_dim=x_all.shape[-1], out_dim=y_all.shape[-1],\n",
    "                    inv_L_scale_hyper = [model_train_inv_L_scale, 0.5, 2.0],\n",
    "                    act='gelu').to(device)\n",
    "\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint)\n",
    "\n",
    "        print(f'\\n\\nNew model : {model_path}', flush = True)\n",
    "        tuple_all = (x_all,y_all,node_mask,nodes,node_weights,directed_edges,edge_gradient_weights)\n",
    "        test_tuple, y_normalizer = test_normalizer(tuple_all, config, n_train)\n",
    "        # rel_l2, index = test_model(model, n_test, test_tuple, y_normalizer)\n",
    "        # average_loss_list,index_3 = sorted_result(rel_l2, index)\n",
    "\n",
    "        test_index = [925,952,999]\n",
    "        myplot(test_index,save_figure_path,\n",
    "                model,test_tuple, y_normalizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_exponential = np.array(rel_l2_exponential)\n",
    "# loss_linear = np.array(rel_l2_linear)\n",
    "# loss_uniform = np.array(rel_l2_uniform)\n",
    "# np.savez('test_result/test_losses.npz', loss_exponential=loss_exponential, loss_linear=loss_linear, loss_uniform = loss_uniform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
