{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GkNN for 1d Arbitrary Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from models import FNN1d, FNN_train, construct_model, compute_1dFourier_bases_arbitrary,compute_1dWeights\n",
    "torch.set_printoptions(precision=16)\n",
    "\n",
    "seed = 0\n",
    "\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "torch.set_printoptions(precision=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../mytest/data/burgers_data_R10.mat\"\n",
    "data = loadmat(data_path)\n",
    "data_in = data[\"a\"]\n",
    "data_out = data[\"u\"]\n",
    "\n",
    "L = 1\n",
    "subrate = 2**3\n",
    "gridsize = 2**13 // subrate\n",
    "x_data = torch.from_numpy(data[\"a\"][:, ::subrate]).unsqueeze(-1)\n",
    "y_data = torch.from_numpy(data[\"u\"][:, ::subrate])\n",
    "num_data = x_data.size(0)\n",
    "\n",
    "grid = torch.linspace(0, L, gridsize + 1)[:-1].unsqueeze(-1)\n",
    "x_data = torch.cat((x_data, grid.repeat([num_data, 1, 1])), dim=-1)\n",
    "\n",
    "print(\"number of data:\", num_data, \", grid size now:\", gridsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Arbitrary Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 2**8\n",
    "np.random.seed(seed)\n",
    "index_selected = np.random.choice(range(gridsize), s, replace=False)\n",
    "\n",
    "index_selected.sort()\n",
    "index_selected[0], index_selected[s - 1] = 0, gridsize - 1\n",
    "grid = torch.tensor(index_selected) / gridsize\n",
    "print(\"points selected:\", s)\n",
    "print(\"index selected:\\n\", index_selected)\n",
    "\n",
    "# selecting points, concatenating x and grid\n",
    "grid_expanded=grid.unsqueeze(0).unsqueeze(-1)\n",
    "x_selected = x_data[:, index_selected, :]\n",
    "y_selected = y_data[:, index_selected]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(grid, y_selected[0, :], marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 2**10\n",
    "ntest = 2**9\n",
    "\n",
    "x_train = x_selected[:ntrain, :, :].float()\n",
    "y_train = y_selected[:ntrain, :].float()\n",
    "x_test = x_selected[:-ntest, :, :].float()\n",
    "y_test = y_selected[:-ntest, :].float()\n",
    "print(x_train.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Basis\n",
    "by SVD\n",
    "$$USV^T=Y, and~ U^TU=I_{n\\times n}$$\n",
    "let \n",
    "$$W:=diag\\{\\Delta x_1,\\Delta x_2,\\cdots,\\Delta x_n\\},B:=\\sqrt{W}^{-1}U$$\n",
    "then\n",
    "$$B^TWB=U^T\\sqrt{W}^{-T}W\\sqrt{W}^{-1}U=U^TU=I_{n\\times n}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max = 33\n",
    "weights = compute_1dWeights(grid)\n",
    "\n",
    "bases_fourier, wbases_fourier = compute_1dFourier_bases_arbitrary(\n",
    "    s, k_max, grid, weights\n",
    ")\n",
    "\n",
    "pca_data = y_train.T\n",
    "U, S, VT = np.linalg.svd(pca_data)\n",
    "U = torch.from_numpy(U.astype(np.float32))\n",
    "bases_pca = U[:, :k_max] / torch.sqrt(weights.unsqueeze(1))\n",
    "wbases_pca = U[:, :k_max] * torch.sqrt(weights.unsqueeze(1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(1, 4):\n",
    "    plt.scatter(grid, bases_fourier[:, i], marker=\".\", label=\"base\" + str(i + 1))\n",
    "plt.legend()\n",
    "print(\n",
    "    f\"verify orthonormality in fourier bases:\\n Int(base3^2 dx)={sum(bases_fourier[:, 6] * wbases_fourier[:, 6]).item()},Int(base3*base5 dx)={sum(bases_fourier[:, 1] * wbases_fourier[:, 10]).item()}\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(grid, pca_data[:, 0], marker=\".\", label=\"u\")\n",
    "for i in range(3):\n",
    "    plt.scatter(grid, bases_pca[:, i], marker=\".\", label=\"base\" + str(i + 1))\n",
    "plt.legend()\n",
    "print(\n",
    "    f\"verify orthonormality in pca bases:\\n Int(base3^2 dx)={sum(bases_pca[:, 3] * wbases_pca[:, 3]).item()},Int(base3*base5 dx)={sum(bases_pca[:, 3] * wbases_pca[:, 5]).item()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GkNN(\n",
      "  (fc0): Linear(in_features=2, out_features=64, bias=True)\n",
      "  (sp_layers): ModuleList(\n",
      "    (0-2): 3 x GalerkinConv()\n",
      "  )\n",
      "  (ws): ModuleList(\n",
      "    (0-2): 3 x Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_type = \"GalerkinNO\"\n",
    "galerkin_config_std1 = {\n",
    "    \"type\": \"GalerkinConv\",\n",
    "    \"num_modes\": k_max,\n",
    "    \"bases\": bases_fourier,\n",
    "    \"wbases\": wbases_fourier,\n",
    "}\n",
    "galerkin_config_std2 = {\n",
    "    \"type\": \"GalerkinConv\",\n",
    "    \"num_modes\": k_max,\n",
    "    \"bases\": bases_pca,\n",
    "    \"wbases\": wbases_pca,\n",
    "}\n",
    "attention_config_std = {\n",
    "    \"type\": \"Attention\",\n",
    "    \"num_heads\": 1,\n",
    "    \"attention_type\": \"galerkin\",\n",
    "}\n",
    "layer_configs = [galerkin_config_std1, galerkin_config_std1, galerkin_config_std2]\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"model\": model_type,\n",
    "        \"dim\": 1,\n",
    "        \"fc_dim\": 128,\n",
    "        \"layers\": [64] * 4,\n",
    "        \"in_dim\": 2,\n",
    "        \"out_dim\": 1,\n",
    "        \"act\": \"gelu\",\n",
    "        \"pad_ratio\": -1,\n",
    "        \"layer_configs\": layer_configs,\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"base_lr\": 0.001,\n",
    "        \"weight_decay\": 1.0e-4,\n",
    "        \"epochs\": 200,\n",
    "        \"scheduler\": \"MultiStepLR\",\n",
    "        \"milestones\": [50, 100, 150],\n",
    "        \"scheduler_gamma\": 0.5,\n",
    "        \"batch_size\": 8,\n",
    "        \"normalization_x\": True,\n",
    "        \"normalization_y\": True,\n",
    "        \"normalization_dim\": [],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "model = construct_model(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training  GalerkinNO\n",
      "Epoch :  0  Rel. Train L2 Loss :  0.32311135379131883  Rel. Test L2 Loss :  0.21017331435965994  Test L2 Loss :  0.09608563875857119\n",
      "Epoch :  1  Rel. Train L2 Loss :  0.14067617885302752  Rel. Test L2 Loss :  0.10468750939859699  Test L2 Loss :  0.04599779465934262\n",
      "Epoch :  2  Rel. Train L2 Loss :  0.07661074574571103  Rel. Test L2 Loss :  0.06700280324245493  Test L2 Loss :  0.030024038424016908\n",
      "Epoch :  3  Rel. Train L2 Loss :  0.06340742399333976  Rel. Test L2 Loss :  0.062405989485948034  Test L2 Loss :  0.02886938923135555\n",
      "Epoch :  4  Rel. Train L2 Loss :  0.05277043324895203  Rel. Test L2 Loss :  0.05424540881843617  Test L2 Loss :  0.025208435457898304\n",
      "Epoch :  5  Rel. Train L2 Loss :  0.04785550673841499  Rel. Test L2 Loss :  0.04712289554299787  Test L2 Loss :  0.021518290086532943\n",
      "Epoch :  6  Rel. Train L2 Loss :  0.041403880168218166  Rel. Test L2 Loss :  0.05020312602088476  Test L2 Loss :  0.022696815722156316\n",
      "Epoch :  7  Rel. Train L2 Loss :  0.04257374562439509  Rel. Test L2 Loss :  0.050301195859598614  Test L2 Loss :  0.022797151905251667\n",
      "Epoch :  8  Rel. Train L2 Loss :  0.04146524489624426  Rel. Test L2 Loss :  0.03714961011428386  Test L2 Loss :  0.017063572071492672\n",
      "Epoch :  9  Rel. Train L2 Loss :  0.035431009528110735  Rel. Test L2 Loss :  0.03341172490036115  Test L2 Loss :  0.015269736972792694\n",
      "Epoch :  10  Rel. Train L2 Loss :  0.035354462190298364  Rel. Test L2 Loss :  0.04376235566451214  Test L2 Loss :  0.020167557241317507\n",
      "Epoch :  11  Rel. Train L2 Loss :  0.03583357218303718  Rel. Test L2 Loss :  0.03272346816568946  Test L2 Loss :  0.015585776660979414\n",
      "Epoch :  12  Rel. Train L2 Loss :  0.03125542696216144  Rel. Test L2 Loss :  0.025509696793354426  Test L2 Loss :  0.011913010443095118\n",
      "Epoch :  13  Rel. Train L2 Loss :  0.03432505911041517  Rel. Test L2 Loss :  0.029494182497728616  Test L2 Loss :  0.013044693252595607\n",
      "Epoch :  14  Rel. Train L2 Loss :  0.03186202418874018  Rel. Test L2 Loss :  0.030480472070242588  Test L2 Loss :  0.013810906357927403\n",
      "Epoch :  15  Rel. Train L2 Loss :  0.02967321898904629  Rel. Test L2 Loss :  0.029663445689948276  Test L2 Loss :  0.013456891744378177\n",
      "Epoch :  16  Rel. Train L2 Loss :  0.029354016951401718  Rel. Test L2 Loss :  0.02812768064904958  Test L2 Loss :  0.013136484257605238\n",
      "Epoch :  17  Rel. Train L2 Loss :  0.030994773071142845  Rel. Test L2 Loss :  0.025930758497755352  Test L2 Loss :  0.012345406740981465\n",
      "Epoch :  18  Rel. Train L2 Loss :  0.02844831845141016  Rel. Test L2 Loss :  0.027050192215635132  Test L2 Loss :  0.01248547650417701\n",
      "Epoch :  19  Rel. Train L2 Loss :  0.02751278215146158  Rel. Test L2 Loss :  0.03991992381634191  Test L2 Loss :  0.015687313740878988\n",
      "Epoch :  20  Rel. Train L2 Loss :  0.028734625520883128  Rel. Test L2 Loss :  0.04536873117710153  Test L2 Loss :  0.019695410184795037\n",
      "Epoch :  21  Rel. Train L2 Loss :  0.030739353285753168  Rel. Test L2 Loss :  0.027825705823488533  Test L2 Loss :  0.012385024211350052\n",
      "Epoch :  22  Rel. Train L2 Loss :  0.02755201794934692  Rel. Test L2 Loss :  0.025931064010364935  Test L2 Loss :  0.01164829706006761\n",
      "Epoch :  23  Rel. Train L2 Loss :  0.023616831356775947  Rel. Test L2 Loss :  0.024540276509166386  Test L2 Loss :  0.010729188186815009\n",
      "Epoch :  24  Rel. Train L2 Loss :  0.027285091069643386  Rel. Test L2 Loss :  0.026906569983111694  Test L2 Loss :  0.011396696989928993\n",
      "Epoch :  25  Rel. Train L2 Loss :  0.02635832771193236  Rel. Test L2 Loss :  0.02282922559728225  Test L2 Loss :  0.010405544550546134\n",
      "Epoch :  26  Rel. Train L2 Loss :  0.026580159028526396  Rel. Test L2 Loss :  0.023157968651503325  Test L2 Loss :  0.010622138256925004\n",
      "Epoch :  27  Rel. Train L2 Loss :  0.02624018541246187  Rel. Test L2 Loss :  0.025244032983512927  Test L2 Loss :  0.011200694355162947\n",
      "Epoch :  28  Rel. Train L2 Loss :  0.026291350310202688  Rel. Test L2 Loss :  0.023584241843006264  Test L2 Loss :  0.010845438157654522\n",
      "Epoch :  29  Rel. Train L2 Loss :  0.025506102145300247  Rel. Test L2 Loss :  0.021656291278001543  Test L2 Loss :  0.009556119000383964\n",
      "Epoch :  30  Rel. Train L2 Loss :  0.027139619007357396  Rel. Test L2 Loss :  0.026709559955634177  Test L2 Loss :  0.012329055381997023\n",
      "Epoch :  31  Rel. Train L2 Loss :  0.025714263349073008  Rel. Test L2 Loss :  0.0263376106255843  Test L2 Loss :  0.011352162815455813\n",
      "Epoch :  32  Rel. Train L2 Loss :  0.025662424202891998  Rel. Test L2 Loss :  0.02511715415554742  Test L2 Loss :  0.01093584507422444\n",
      "Epoch :  33  Rel. Train L2 Loss :  0.023141174657212105  Rel. Test L2 Loss :  0.026739721855847165  Test L2 Loss :  0.01158728154405253\n",
      "Epoch :  34  Rel. Train L2 Loss :  0.024857916723703966  Rel. Test L2 Loss :  0.02933211499475874  Test L2 Loss :  0.013451610502670519\n",
      "Epoch :  35  Rel. Train L2 Loss :  0.025471222936175764  Rel. Test L2 Loss :  0.035482065189474575  Test L2 Loss :  0.015843670664859626\n",
      "Epoch :  36  Rel. Train L2 Loss :  0.02726465780142462  Rel. Test L2 Loss :  0.04185977475329613  Test L2 Loss :  0.01945759957986108\n",
      "Epoch :  37  Rel. Train L2 Loss :  0.025381976229255088  Rel. Test L2 Loss :  0.023211560318789754  Test L2 Loss :  0.010556192840643538\n",
      "Epoch :  38  Rel. Train L2 Loss :  0.022825244290288538  Rel. Test L2 Loss :  0.026995680964319035  Test L2 Loss :  0.012815513948832328\n",
      "Epoch :  39  Rel. Train L2 Loss :  0.026391494277049787  Rel. Test L2 Loss :  0.024821359431371093  Test L2 Loss :  0.011063803809520323\n",
      "Epoch :  40  Rel. Train L2 Loss :  0.02454949870298151  Rel. Test L2 Loss :  0.022607381329483662  Test L2 Loss :  0.009922534006667169\n",
      "Epoch :  41  Rel. Train L2 Loss :  0.02484542000456713  Rel. Test L2 Loss :  0.028244566531308617  Test L2 Loss :  0.01225036065443419\n",
      "Epoch :  42  Rel. Train L2 Loss :  0.022922299249330536  Rel. Test L2 Loss :  0.01859987122588791  Test L2 Loss :  0.008532422189697778\n",
      "Epoch :  43  Rel. Train L2 Loss :  0.020850347442319617  Rel. Test L2 Loss :  0.023893197008874267  Test L2 Loss :  0.010774337186982544\n",
      "Epoch :  44  Rel. Train L2 Loss :  0.02321070850302931  Rel. Test L2 Loss :  0.01917132211868496  Test L2 Loss :  0.008394754865245583\n",
      "Epoch :  45  Rel. Train L2 Loss :  0.020028851795359515  Rel. Test L2 Loss :  0.023058133199810982  Test L2 Loss :  0.01019218013000985\n",
      "Epoch :  46  Rel. Train L2 Loss :  0.021732277265982702  Rel. Test L2 Loss :  0.033042992445795484  Test L2 Loss :  0.014895782936946489\n",
      "Epoch :  47  Rel. Train L2 Loss :  0.026741115492768586  Rel. Test L2 Loss :  0.026058251707581803  Test L2 Loss :  0.011252416489393605\n",
      "Epoch :  48  Rel. Train L2 Loss :  0.020843565915129147  Rel. Test L2 Loss :  0.029323720780666918  Test L2 Loss :  0.013133381944498979\n",
      "Epoch :  49  Rel. Train L2 Loss :  0.023222888266900554  Rel. Test L2 Loss :  0.028310328809311613  Test L2 Loss :  0.011878163134194134\n",
      "Epoch :  50  Rel. Train L2 Loss :  0.013645973478560336  Rel. Test L2 Loss :  0.012682995885067308  Test L2 Loss :  0.00585398168550455\n",
      "Epoch :  51  Rel. Train L2 Loss :  0.012941408065671567  Rel. Test L2 Loss :  0.013524350273655728  Test L2 Loss :  0.006050832964926182\n",
      "Epoch :  52  Rel. Train L2 Loss :  0.013066912353679072  Rel. Test L2 Loss :  0.01675994394463487  Test L2 Loss :  0.007536999297978279\n",
      "Epoch :  53  Rel. Train L2 Loss :  0.013009420064918231  Rel. Test L2 Loss :  0.012233321060193703  Test L2 Loss :  0.005543906522992377\n",
      "Epoch :  54  Rel. Train L2 Loss :  0.014018590743944515  Rel. Test L2 Loss :  0.014842748127800101  Test L2 Loss :  0.006623701568363079\n",
      "Epoch :  55  Rel. Train L2 Loss :  0.012321306036028545  Rel. Test L2 Loss :  0.012097406962614818  Test L2 Loss :  0.005419424458523281\n",
      "Epoch :  56  Rel. Train L2 Loss :  0.013320628662768286  Rel. Test L2 Loss :  0.01768119960615877  Test L2 Loss :  0.007805650634206056\n",
      "Epoch :  57  Rel. Train L2 Loss :  0.012678060680627823  Rel. Test L2 Loss :  0.014200570420750106  Test L2 Loss :  0.006262907386068643\n",
      "Epoch :  58  Rel. Train L2 Loss :  0.012965521898877341  Rel. Test L2 Loss :  0.012739491493751606  Test L2 Loss :  0.005796922043373343\n",
      "Epoch :  59  Rel. Train L2 Loss :  0.012027120603306685  Rel. Test L2 Loss :  0.012418580435526868  Test L2 Loss :  0.005642063292422487\n",
      "Epoch :  60  Rel. Train L2 Loss :  0.012381235654174816  Rel. Test L2 Loss :  0.011792809518131738  Test L2 Loss :  0.005442940416590621\n",
      "Epoch :  61  Rel. Train L2 Loss :  0.012332425700151362  Rel. Test L2 Loss :  0.013023331688600592  Test L2 Loss :  0.005847475201638493\n",
      "Epoch :  62  Rel. Train L2 Loss :  0.0128830632311292  Rel. Test L2 Loss :  0.013521026291224795  Test L2 Loss :  0.006120185918310502\n",
      "Epoch :  63  Rel. Train L2 Loss :  0.012195138297101948  Rel. Test L2 Loss :  0.013998057116017057  Test L2 Loss :  0.006583581154700369\n",
      "Epoch :  64  Rel. Train L2 Loss :  0.0142044327876647  Rel. Test L2 Loss :  0.01533280428945242  Test L2 Loss :  0.006847106781303107\n",
      "Epoch :  65  Rel. Train L2 Loss :  0.014282676849688869  Rel. Test L2 Loss :  0.021232973066313814  Test L2 Loss :  0.009239227996052554\n",
      "Epoch :  66  Rel. Train L2 Loss :  0.012764894046995323  Rel. Test L2 Loss :  0.01354625932193206  Test L2 Loss :  0.0063336876179770725\n",
      "Epoch :  67  Rel. Train L2 Loss :  0.012859639769885689  Rel. Test L2 Loss :  0.011337623916915618  Test L2 Loss :  0.005278978221516202\n",
      "Epoch :  68  Rel. Train L2 Loss :  0.013605119122075848  Rel. Test L2 Loss :  0.012752934640351063  Test L2 Loss :  0.0058165763948636595\n",
      "Epoch :  69  Rel. Train L2 Loss :  0.012419359278283082  Rel. Test L2 Loss :  0.014726606910699047  Test L2 Loss :  0.00658349429674369\n",
      "Epoch :  70  Rel. Train L2 Loss :  0.013142918687663041  Rel. Test L2 Loss :  0.014865232728576908  Test L2 Loss :  0.0067221480239822995\n",
      "Epoch :  71  Rel. Train L2 Loss :  0.01238109377300134  Rel. Test L2 Loss :  0.012034210230922326  Test L2 Loss :  0.005339409806765616\n",
      "Epoch :  72  Rel. Train L2 Loss :  0.012506698920333292  Rel. Test L2 Loss :  0.011645821034714269  Test L2 Loss :  0.005442640569526702\n",
      "Epoch :  73  Rel. Train L2 Loss :  0.011311247948469827  Rel. Test L2 Loss :  0.012127007357776165  Test L2 Loss :  0.005591477878624573\n",
      "Epoch :  74  Rel. Train L2 Loss :  0.012185200881503988  Rel. Test L2 Loss :  0.014751163699353734  Test L2 Loss :  0.006601773107831832\n",
      "Epoch :  75  Rel. Train L2 Loss :  0.013792580590234138  Rel. Test L2 Loss :  0.013883602567754375  Test L2 Loss :  0.005914842914838421\n",
      "Epoch :  76  Rel. Train L2 Loss :  0.012647067596844863  Rel. Test L2 Loss :  0.01210386878422772  Test L2 Loss :  0.005661804059248728\n",
      "Epoch :  77  Rel. Train L2 Loss :  0.012761462458001915  Rel. Test L2 Loss :  0.012468504690332338  Test L2 Loss :  0.0057072305656523286\n",
      "Epoch :  78  Rel. Train L2 Loss :  0.01258640454034321  Rel. Test L2 Loss :  0.011956919590981366  Test L2 Loss :  0.005365079677479419\n",
      "Epoch :  79  Rel. Train L2 Loss :  0.012260051880730316  Rel. Test L2 Loss :  0.010457424022509562  Test L2 Loss :  0.0048344329588871915\n",
      "Epoch :  80  Rel. Train L2 Loss :  0.011870831840496976  Rel. Test L2 Loss :  0.011783036514922665  Test L2 Loss :  0.005363586666741564\n",
      "Epoch :  81  Rel. Train L2 Loss :  0.012985075038159266  Rel. Test L2 Loss :  0.010967177566878187  Test L2 Loss :  0.005045087781278805\n",
      "Epoch :  82  Rel. Train L2 Loss :  0.011666012345813215  Rel. Test L2 Loss :  0.012258961311696718  Test L2 Loss :  0.00571657042504133\n",
      "Epoch :  83  Rel. Train L2 Loss :  0.011621690522588324  Rel. Test L2 Loss :  0.01262386264958574  Test L2 Loss :  0.005502303913090145\n",
      "Epoch :  84  Rel. Train L2 Loss :  0.011967236183409113  Rel. Test L2 Loss :  0.011524392369513711  Test L2 Loss :  0.005103686045913491\n",
      "Epoch :  85  Rel. Train L2 Loss :  0.012533681190689094  Rel. Test L2 Loss :  0.011568721262544083  Test L2 Loss :  0.005413142851466546\n",
      "Epoch :  86  Rel. Train L2 Loss :  0.012151445320341736  Rel. Test L2 Loss :  0.01260849026342233  Test L2 Loss :  0.005844750047496443\n",
      "Epoch :  87  Rel. Train L2 Loss :  0.011603698825638276  Rel. Test L2 Loss :  0.014612803128936017  Test L2 Loss :  0.006694698156934464\n",
      "Epoch :  88  Rel. Train L2 Loss :  0.012361790402792394  Rel. Test L2 Loss :  0.010571828415171089  Test L2 Loss :  0.004774593101804688\n",
      "Epoch :  89  Rel. Train L2 Loss :  0.012462113038054667  Rel. Test L2 Loss :  0.010792412251854936  Test L2 Loss :  0.0049957759692915715\n",
      "Epoch :  90  Rel. Train L2 Loss :  0.011639930035016732  Rel. Test L2 Loss :  0.010324039302455882  Test L2 Loss :  0.004766816306073451\n",
      "Epoch :  91  Rel. Train L2 Loss :  0.01283170105307363  Rel. Test L2 Loss :  0.011710070296733951  Test L2 Loss :  0.005429096267713855\n",
      "Epoch :  92  Rel. Train L2 Loss :  0.012221606521052308  Rel. Test L2 Loss :  0.01625294652573454  Test L2 Loss :  0.0072688943619141355\n",
      "Epoch :  93  Rel. Train L2 Loss :  0.013255774152639788  Rel. Test L2 Loss :  0.012391679013186755  Test L2 Loss :  0.00568475923622221\n",
      "Epoch :  94  Rel. Train L2 Loss :  0.012194679933600128  Rel. Test L2 Loss :  0.013074216069071554  Test L2 Loss :  0.005797112574024747\n",
      "Epoch :  95  Rel. Train L2 Loss :  0.010904885748459492  Rel. Test L2 Loss :  0.011172480328241363  Test L2 Loss :  0.0050831544006844824\n",
      "Epoch :  96  Rel. Train L2 Loss :  0.011531941781868227  Rel. Test L2 Loss :  0.010523969554924406  Test L2 Loss :  0.004614317817564976\n",
      "Epoch :  97  Rel. Train L2 Loss :  0.013001163861190435  Rel. Test L2 Loss :  0.014917766961540716  Test L2 Loss :  0.006454097837073884\n",
      "Epoch :  98  Rel. Train L2 Loss :  0.01224354116129689  Rel. Test L2 Loss :  0.013266601522142688  Test L2 Loss :  0.006317974167662517\n",
      "Epoch :  99  Rel. Train L2 Loss :  0.011064219346735626  Rel. Test L2 Loss :  0.01534958740133637  Test L2 Loss :  0.007119998453466299\n",
      "Epoch :  100  Rel. Train L2 Loss :  0.0080466527651879  Rel. Test L2 Loss :  0.0074211516305998275  Test L2 Loss :  0.003392138105482445\n",
      "Epoch :  101  Rel. Train L2 Loss :  0.007161429759435123  Rel. Test L2 Loss :  0.006991707537963521  Test L2 Loss :  0.0032033358450765568\n",
      "Epoch :  102  Rel. Train L2 Loss :  0.0071227079824893735  Rel. Test L2 Loss :  0.008475844901113305  Test L2 Loss :  0.003930753039336802\n",
      "Epoch :  103  Rel. Train L2 Loss :  0.007121833161363611  Rel. Test L2 Loss :  0.008040053178168213  Test L2 Loss :  0.0038144244917930337\n",
      "Epoch :  104  Rel. Train L2 Loss :  0.006887415191158652  Rel. Test L2 Loss :  0.007186532508058008  Test L2 Loss :  0.0032754911820423636\n",
      "Epoch :  105  Rel. Train L2 Loss :  0.006977300745347748  Rel. Test L2 Loss :  0.0079287676999229  Test L2 Loss :  0.003616865611547837\n",
      "Epoch :  106  Rel. Train L2 Loss :  0.007199552030215273  Rel. Test L2 Loss :  0.007880105389631353  Test L2 Loss :  0.0036558601010862426\n",
      "Epoch :  107  Rel. Train L2 Loss :  0.007264005860633915  Rel. Test L2 Loss :  0.0077557600404058276  Test L2 Loss :  0.003566812879701805\n",
      "Epoch :  108  Rel. Train L2 Loss :  0.007772759061481338  Rel. Test L2 Loss :  0.007847226848146724  Test L2 Loss :  0.0034919244765963717\n",
      "Epoch :  109  Rel. Train L2 Loss :  0.007533517407864565  Rel. Test L2 Loss :  0.0075150093956229585  Test L2 Loss :  0.003528663835216624\n",
      "Epoch :  110  Rel. Train L2 Loss :  0.007770320113195339  Rel. Test L2 Loss :  0.007263542228126123  Test L2 Loss :  0.003382228603489542\n",
      "Epoch :  111  Rel. Train L2 Loss :  0.007090358089044457  Rel. Test L2 Loss :  0.007657516638573725  Test L2 Loss :  0.003508580489627396\n",
      "Epoch :  112  Rel. Train L2 Loss :  0.007010333425569115  Rel. Test L2 Loss :  0.008670924895947488  Test L2 Loss :  0.003996381306933472\n",
      "Epoch :  113  Rel. Train L2 Loss :  0.007184130012319656  Rel. Test L2 Loss :  0.00689478064547681  Test L2 Loss :  0.0031392120569459316\n",
      "Epoch :  114  Rel. Train L2 Loss :  0.0072761830197123345  Rel. Test L2 Loss :  0.007575804328856369  Test L2 Loss :  0.003438425937323094\n",
      "Epoch :  115  Rel. Train L2 Loss :  0.006748123949364526  Rel. Test L2 Loss :  0.0071246303972050855  Test L2 Loss :  0.003251332552584548\n",
      "Epoch :  116  Rel. Train L2 Loss :  0.007710678386501968  Rel. Test L2 Loss :  0.007581112474629966  Test L2 Loss :  0.003423932990699541\n",
      "Epoch :  117  Rel. Train L2 Loss :  0.0072272289799002465  Rel. Test L2 Loss :  0.008363887556091262  Test L2 Loss :  0.003736235545754122\n",
      "Epoch :  118  Rel. Train L2 Loss :  0.007253552215843229  Rel. Test L2 Loss :  0.00695469583782445  Test L2 Loss :  0.00328255307310125\n",
      "Epoch :  119  Rel. Train L2 Loss :  0.007190773903857917  Rel. Test L2 Loss :  0.007303744130088792  Test L2 Loss :  0.0034339497736558164\n",
      "Epoch :  120  Rel. Train L2 Loss :  0.007326352915697498  Rel. Test L2 Loss :  0.008041387647002315  Test L2 Loss :  0.0036683813583901306\n",
      "Epoch :  121  Rel. Train L2 Loss :  0.007380947670753812  Rel. Test L2 Loss :  0.00848340313435377  Test L2 Loss :  0.0037738251048722304\n",
      "Epoch :  122  Rel. Train L2 Loss :  0.006873767360957572  Rel. Test L2 Loss :  0.007643646567885298  Test L2 Loss :  0.003458071261169001\n",
      "Epoch :  123  Rel. Train L2 Loss :  0.006800762996135745  Rel. Test L2 Loss :  0.007157594513652536  Test L2 Loss :  0.003304404366038701\n",
      "Epoch :  124  Rel. Train L2 Loss :  0.00723436526095611  Rel. Test L2 Loss :  0.00721750728553161  Test L2 Loss :  0.00331775311800205\n",
      "Epoch :  125  Rel. Train L2 Loss :  0.006559236102475552  Rel. Test L2 Loss :  0.007654879613255616  Test L2 Loss :  0.0035577531956126527\n",
      "Epoch :  126  Rel. Train L2 Loss :  0.006817152076109778  Rel. Test L2 Loss :  0.007103004315771007  Test L2 Loss :  0.00323778224325603\n",
      "Epoch :  127  Rel. Train L2 Loss :  0.006797123121941695  Rel. Test L2 Loss :  0.006605609856099666  Test L2 Loss :  0.003063333629446182\n",
      "Epoch :  128  Rel. Train L2 Loss :  0.006832402810687199  Rel. Test L2 Loss :  0.008125398317739988  Test L2 Loss :  0.003499588047513195\n",
      "Epoch :  129  Rel. Train L2 Loss :  0.006997901669819839  Rel. Test L2 Loss :  0.0068734601979182726  Test L2 Loss :  0.003146459689863453\n",
      "Epoch :  130  Rel. Train L2 Loss :  0.006935793295269832  Rel. Test L2 Loss :  0.007500143505846306  Test L2 Loss :  0.0035127050468872767\n",
      "Epoch :  131  Rel. Train L2 Loss :  0.006681822946120519  Rel. Test L2 Loss :  0.00676065625884803  Test L2 Loss :  0.003157827439281391\n",
      "Epoch :  132  Rel. Train L2 Loss :  0.006665178090770496  Rel. Test L2 Loss :  0.007513612300196352  Test L2 Loss :  0.003451185052957347\n",
      "Epoch :  133  Rel. Train L2 Loss :  0.006913209097547224  Rel. Test L2 Loss :  0.007342545621213503  Test L2 Loss :  0.003452647345208485\n",
      "Epoch :  134  Rel. Train L2 Loss :  0.007537095996667631  Rel. Test L2 Loss :  0.007664717561662353  Test L2 Loss :  0.0035559711741370847\n",
      "Epoch :  135  Rel. Train L2 Loss :  0.006867014901217772  Rel. Test L2 Loss :  0.00739935119539344  Test L2 Loss :  0.0033315160872007255\n",
      "Epoch :  136  Rel. Train L2 Loss :  0.006786908943468006  Rel. Test L2 Loss :  0.006661284479681247  Test L2 Loss :  0.003133348851406481\n",
      "Epoch :  137  Rel. Train L2 Loss :  0.0072745759098324925  Rel. Test L2 Loss :  0.011284780188968094  Test L2 Loss :  0.004845736180868698\n",
      "Epoch :  138  Rel. Train L2 Loss :  0.00753000282929861  Rel. Test L2 Loss :  0.0085080887292861  Test L2 Loss :  0.0037654179638290466\n",
      "Epoch :  139  Rel. Train L2 Loss :  0.006790515370084904  Rel. Test L2 Loss :  0.007983697214513086  Test L2 Loss :  0.003494544933346333\n",
      "Epoch :  140  Rel. Train L2 Loss :  0.00705095711236936  Rel. Test L2 Loss :  0.0067026204099723445  Test L2 Loss :  0.003073668554861797\n",
      "Epoch :  141  Rel. Train L2 Loss :  0.007040718563075643  Rel. Test L2 Loss :  0.007411440545790053  Test L2 Loss :  0.0033753286273470926\n",
      "Epoch :  142  Rel. Train L2 Loss :  0.007118276320397854  Rel. Test L2 Loss :  0.0074796952079244266  Test L2 Loss :  0.003461019094781174\n",
      "Epoch :  143  Rel. Train L2 Loss :  0.008087845733825816  Rel. Test L2 Loss :  0.0071750893524343455  Test L2 Loss :  0.0033422706886388673\n",
      "Epoch :  144  Rel. Train L2 Loss :  0.006279437144257827  Rel. Test L2 Loss :  0.007483861913594107  Test L2 Loss :  0.0033369855427736184\n",
      "Epoch :  145  Rel. Train L2 Loss :  0.006823224164691055  Rel. Test L2 Loss :  0.008155421409658933  Test L2 Loss :  0.003704632571801388\n",
      "Epoch :  146  Rel. Train L2 Loss :  0.006707374071993399  Rel. Test L2 Loss :  0.006453603413926127  Test L2 Loss :  0.0030127095142233884\n",
      "Epoch :  147  Rel. Train L2 Loss :  0.005996240564854816  Rel. Test L2 Loss :  0.0061400213138161535  Test L2 Loss :  0.0028584756661681845\n",
      "Epoch :  148  Rel. Train L2 Loss :  0.006321447555819759  Rel. Test L2 Loss :  0.006767145586006033  Test L2 Loss :  0.003180781821962834\n",
      "Epoch :  149  Rel. Train L2 Loss :  0.006782493695936864  Rel. Test L2 Loss :  0.007453503800206818  Test L2 Loss :  0.003499768292385852\n",
      "Epoch :  150  Rel. Train L2 Loss :  0.00499070400474011  Rel. Test L2 Loss :  0.005550016855219535  Test L2 Loss :  0.0025105803900563237\n",
      "Epoch :  151  Rel. Train L2 Loss :  0.0044666624435194535  Rel. Test L2 Loss :  0.005051994657454391  Test L2 Loss :  0.0023565871894485704\n",
      "Epoch :  152  Rel. Train L2 Loss :  0.004441643835889408  Rel. Test L2 Loss :  0.00488457508739278  Test L2 Loss :  0.002322863603088384\n",
      "Epoch :  153  Rel. Train L2 Loss :  0.004426543087902246  Rel. Test L2 Loss :  0.004857935908754977  Test L2 Loss :  0.0022853589537893035\n",
      "Epoch :  154  Rel. Train L2 Loss :  0.004469147515919758  Rel. Test L2 Loss :  0.005658747691389483  Test L2 Loss :  0.002591963701585579\n",
      "Epoch :  155  Rel. Train L2 Loss :  0.004568928774460801  Rel. Test L2 Loss :  0.005286030253046192  Test L2 Loss :  0.0024692732610371118\n",
      "Epoch :  156  Rel. Train L2 Loss :  0.004515399295996758  Rel. Test L2 Loss :  0.005823947183671407  Test L2 Loss :  0.002674851285216088\n",
      "Epoch :  157  Rel. Train L2 Loss :  0.004539367322649923  Rel. Test L2 Loss :  0.004850857124741499  Test L2 Loss :  0.0022753666210822607\n",
      "Epoch :  158  Rel. Train L2 Loss :  0.004683020675656735  Rel. Test L2 Loss :  0.005136882862037358  Test L2 Loss :  0.0024772035067144316\n",
      "Epoch :  159  Rel. Train L2 Loss :  0.004448603709533927  Rel. Test L2 Loss :  0.0054091503491993835  Test L2 Loss :  0.0025179577854335853\n",
      "Epoch :  160  Rel. Train L2 Loss :  0.0045465512812370434  Rel. Test L2 Loss :  0.004669525532032519  Test L2 Loss :  0.0022167353254189948\n",
      "Epoch :  161  Rel. Train L2 Loss :  0.004560964982374571  Rel. Test L2 Loss :  0.004980380593527419  Test L2 Loss :  0.0023985738092354345\n",
      "Epoch :  162  Rel. Train L2 Loss :  0.004687117301727994  Rel. Test L2 Loss :  0.006466578255640343  Test L2 Loss :  0.0030380630487343296\n",
      "Epoch :  163  Rel. Train L2 Loss :  0.004858328311456717  Rel. Test L2 Loss :  0.005443224513631624  Test L2 Loss :  0.0025054745383386035\n",
      "Epoch :  164  Rel. Train L2 Loss :  0.00468524149255245  Rel. Test L2 Loss :  0.004813491251600984  Test L2 Loss :  0.0022766379497625167\n",
      "Epoch :  165  Rel. Train L2 Loss :  0.004776067842612974  Rel. Test L2 Loss :  0.007915360845800024  Test L2 Loss :  0.0037720832284927988\n",
      "Epoch :  166  Rel. Train L2 Loss :  0.005138828422786901  Rel. Test L2 Loss :  0.005244031173788244  Test L2 Loss :  0.002504308635252528\n",
      "Epoch :  167  Rel. Train L2 Loss :  0.004619992592779454  Rel. Test L2 Loss :  0.0050518352109065745  Test L2 Loss :  0.0023966332319105277\n",
      "Epoch :  168  Rel. Train L2 Loss :  0.004491212303037173  Rel. Test L2 Loss :  0.005340884288064747  Test L2 Loss :  0.0025003033406392206\n",
      "Epoch :  169  Rel. Train L2 Loss :  0.004512069683187292  Rel. Test L2 Loss :  0.005031560905384443  Test L2 Loss :  0.002339542328021101\n",
      "Epoch :  170  Rel. Train L2 Loss :  0.004304727581256884  Rel. Test L2 Loss :  0.005415822412032867  Test L2 Loss :  0.0025714100290012234\n",
      "Epoch :  171  Rel. Train L2 Loss :  0.004965063841154915  Rel. Test L2 Loss :  0.004979945618591349  Test L2 Loss :  0.002313172059075441\n",
      "Epoch :  172  Rel. Train L2 Loss :  0.004496815028687706  Rel. Test L2 Loss :  0.005066976314992644  Test L2 Loss :  0.002429161070419165\n",
      "Epoch :  173  Rel. Train L2 Loss :  0.004398926424983074  Rel. Test L2 Loss :  0.004801828832569299  Test L2 Loss :  0.002307341563209775\n",
      "Epoch :  174  Rel. Train L2 Loss :  0.004357625259217457  Rel. Test L2 Loss :  0.005257722616079263  Test L2 Loss :  0.0024660434816420698\n",
      "Epoch :  175  Rel. Train L2 Loss :  0.004614986211890937  Rel. Test L2 Loss :  0.005342474692345907  Test L2 Loss :  0.0024434341624631393\n",
      "Epoch :  176  Rel. Train L2 Loss :  0.004711990666692145  Rel. Test L2 Loss :  0.004775505762760683  Test L2 Loss :  0.002237167803590031\n",
      "Epoch :  177  Rel. Train L2 Loss :  0.004308382422095747  Rel. Test L2 Loss :  0.004678703069657786  Test L2 Loss :  0.0022015210391449123\n",
      "Epoch :  178  Rel. Train L2 Loss :  0.004576145778628415  Rel. Test L2 Loss :  0.005110853106695383  Test L2 Loss :  0.002366238764807349\n",
      "Epoch :  179  Rel. Train L2 Loss :  0.004503085618125624  Rel. Test L2 Loss :  0.004678306065519185  Test L2 Loss :  0.0022159315327977915\n",
      "Epoch :  180  Rel. Train L2 Loss :  0.004376611808766029  Rel. Test L2 Loss :  0.004981989333221766  Test L2 Loss :  0.002309888276310327\n",
      "Epoch :  181  Rel. Train L2 Loss :  0.004730287337224581  Rel. Test L2 Loss :  0.005654658363103711  Test L2 Loss :  0.002650102904832844\n",
      "Epoch :  182  Rel. Train L2 Loss :  0.00435886744344316  Rel. Test L2 Loss :  0.00457467911716473  Test L2 Loss :  0.0021737187950445027\n",
      "Epoch :  183  Rel. Train L2 Loss :  0.00454155388615618  Rel. Test L2 Loss :  0.004836236743964643  Test L2 Loss :  0.002273921761419236\n",
      "Epoch :  184  Rel. Train L2 Loss :  0.004316937582188984  Rel. Test L2 Loss :  0.005648067702471356  Test L2 Loss :  0.002538390169017172\n",
      "Epoch :  185  Rel. Train L2 Loss :  0.004505625936872093  Rel. Test L2 Loss :  0.004664399868488545  Test L2 Loss :  0.0022016892883887826\n",
      "Epoch :  186  Rel. Train L2 Loss :  0.004078840142028639  Rel. Test L2 Loss :  0.0047122609527529375  Test L2 Loss :  0.002254663812588357\n",
      "Epoch :  187  Rel. Train L2 Loss :  0.004498713478824357  Rel. Test L2 Loss :  0.00570322954445146  Test L2 Loss :  0.002718981742267109\n",
      "Epoch :  188  Rel. Train L2 Loss :  0.004341266852861736  Rel. Test L2 Loss :  0.005660009929367031  Test L2 Loss :  0.0027480931178918886\n",
      "Epoch :  189  Rel. Train L2 Loss :  0.0047464909384871135  Rel. Test L2 Loss :  0.006493440053115289  Test L2 Loss :  0.0029403312158441017\n",
      "Epoch :  190  Rel. Train L2 Loss :  0.00458686865385971  Rel. Test L2 Loss :  0.00510178445862645  Test L2 Loss :  0.002362147959502181\n",
      "Epoch :  191  Rel. Train L2 Loss :  0.004373969544758438  Rel. Test L2 Loss :  0.005768999789628045  Test L2 Loss :  0.0026940141069644596\n",
      "Epoch :  192  Rel. Train L2 Loss :  0.004274815652024699  Rel. Test L2 Loss :  0.004835569298544821  Test L2 Loss :  0.002255015683961877\n",
      "Epoch :  193  Rel. Train L2 Loss :  0.004364993748822599  Rel. Test L2 Loss :  0.004663647146420165  Test L2 Loss :  0.002200840111981961\n",
      "Epoch :  194  Rel. Train L2 Loss :  0.004289934804546647  Rel. Test L2 Loss :  0.005084698307958509  Test L2 Loss :  0.00239301957662974\n",
      "Epoch :  195  Rel. Train L2 Loss :  0.00470679754835146  Rel. Test L2 Loss :  0.004948014845770861  Test L2 Loss :  0.0023374935972242383\n",
      "Epoch :  196  Rel. Train L2 Loss :  0.004278957992937649  Rel. Test L2 Loss :  0.004641072326194262  Test L2 Loss :  0.0022064189694598704\n",
      "Epoch :  197  Rel. Train L2 Loss :  0.004282936879462795  Rel. Test L2 Loss :  0.00623778529795042  Test L2 Loss :  0.002915390143850042\n",
      "Epoch :  198  Rel. Train L2 Loss :  0.00445260858941765  Rel. Test L2 Loss :  0.004554149584388749  Test L2 Loss :  0.002163040428664923\n",
      "Epoch :  199  Rel. Train L2 Loss :  0.004163610674368101  Rel. Test L2 Loss :  0.004895757547880446  Test L2 Loss :  0.0023159659964827974\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training \", config[\"model\"][\"model\"])\n",
    "train_rel_l2_losses, test_rel_l2_losses, test_l2_losses = FNN_train(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    config,\n",
    "    model,\n",
    "    save_model_name=\"../models/save/test.pth\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
