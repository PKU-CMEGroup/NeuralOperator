{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D FNO Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "import random\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer\n",
    "from scipy.io import loadmat\n",
    "\n",
    "sys.path.append('../')\n",
    "from models import FNN1d, FNN_train, construct_model, compute_1dFourier_bases\n",
    "torch.set_printoptions(precision=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../mytest/data/burgers_data_R10.mat\"\n",
    "data = loadmat(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = data[\"a\"]\n",
    "data_out = data[\"u\"]\n",
    "print(\"data size = \", data_in.shape[0], \" mesh elements = \", data_in.shape[1])\n",
    "L = 1\n",
    "Ne_ref = data_in.shape[1]\n",
    "grid = np.linspace(0, L, Ne_ref + 1)[:-1]\n",
    "plt.plot(grid, data_in[0, :], label=\"input\")\n",
    "plt.plot(grid, data_out[0, :], label=\"output\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Fourier Transform\n",
    "\n",
    "We compute the Fourier transform for the last dimension of x. We first define the mode set (assume $n_x$ is an even number).\n",
    "\n",
    "$$K = \\{k_x | k_x = 0,1,...,n_x/2-1, -n_x/2, -n_x/2+1, ... -1 \\} $$\n",
    "\n",
    "Then the Fourier transform and inverse Fourier transform give the relation between \n",
    "$\\{\\hat{f}[k_x]: k_x \\in K \\}$ and $\\{f[j_x] : 0 \\leq j_x \\leq n_x\\}$\n",
    "\n",
    "\\begin{align*}\n",
    "   f(x) &= \\frac{1}{n_x} \\sum_{k_x \\in K}  \\hat{f}[k_x]  e^{i k_x \\frac{2\\pi x}{L_x}} \n",
    "   \\\\ \n",
    "   f[j_x] &= \\frac{1}{n_x} \\sum_{k_x \\in K}  \\hat{f}[k_x]  e^{i k_x \\frac{2\\pi j_x \\Delta x}{L_x}} \n",
    "   \\\\ \n",
    "   \\hat{f}[k_x] &= \\frac{n_x}{L_x} \\int f(x)  e^{-i k_x \\frac{2\\pi x}{L_x}} dx \\\\\n",
    "                &\\approx \\frac{n_x}{L_x}  \\sum_{j_x = 0}^{n_x - 1} f[j_x]  e^{-i k_x \\frac{2\\pi j_x \\Delta x }{L_x}} \\Delta x \\qquad \\textrm{when f has certain form, this is accurate.}\\\\\n",
    "                &= \\sum_{j_x = 0}^{n_x - 1} f[j_x]  e^{-i k_x \\frac{2\\pi  j_x}{n_x}}\n",
    "\\end{align*}\n",
    "\n",
    "When $f(x)$ is real, we have $\\hat{f}[k_x] = conj(\\hat{f}[-k_x])$, and hence `rfftn` only need to save the subset of $K$ \n",
    "\n",
    "$$K^r = \\{k_x | k_x = 0,1,...,n_x/2-1, -n_x/2\\} $$\n",
    "\n",
    "For $k_x = 0$ and $-nx/2$, the correponding $\\hat{f}[k_x]$ are real numbers. \n",
    "\n",
    "When we truncate to first $k_f$ modes, we have \n",
    "\\begin{align*}\n",
    "f(x) = \\frac{1}{n_x} \\Bigl(\\hat{f}[0]  \\phi_0(x)  +  \\sum_{k=1}^{k_f-1}  (\\hat{f}[k]  \\phi_k(x) + \\hat{f}[-k]  \\phi_{-k}(x)  )\\Bigr)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a test, be careful, the torch will generally use float32.\n",
    "# To get accurate error estimate, use float64\n",
    "\n",
    "downsample_ratio = 32\n",
    "n_train = 2**10\n",
    "x_train = torch.from_numpy(\n",
    "    np.stack(\n",
    "        (\n",
    "            data_in[0:n_train, 0::downsample_ratio],\n",
    "            np.tile(grid[0::downsample_ratio], (n_train, 1)),\n",
    "        ),\n",
    "        axis=-1,\n",
    "    )\n",
    ")\n",
    "batchsize = 16\n",
    "x = x_train[0:batchsize]\n",
    "x = x.permute(0, 2, 1)\n",
    "x_ft = torch.fft.rfftn(x, dim=[2])\n",
    "print(\" The shape of x is \", x.shape, \" the shape of x_ft is \", x_ft.shape)\n",
    "\n",
    "n_b, n_c, n_x = x.shape\n",
    "assert n_x % 2 == 0\n",
    "n_k = n_x // 2\n",
    "\n",
    "x = x.to(torch.complex128)\n",
    "\n",
    "\n",
    "# Implementation 1\n",
    "Kr = list(range(0, n_k)) + [-n_k]\n",
    "x_ft2 = torch.zeros(n_b, n_c, n_k + 1, dtype=torch.complex128)\n",
    "for j_k in range(n_k + 1):\n",
    "    basis = torch.exp(\n",
    "        (\n",
    "            -Kr[j_k]\n",
    "            * 2\n",
    "            * np.pi\n",
    "            / n_x\n",
    "            * 1.0j\n",
    "            * torch.linspace(0, n_x - 1, n_x, dtype=torch.float64)\n",
    "        )\n",
    "    )\n",
    "    x_ft2[:, :, j_k] += torch.einsum(\"bcx,x->bc\", x, basis)\n",
    "print(\"Error between x_ft and x_ft2 is \", torch.norm(x_ft - x_ft2))\n",
    "\n",
    "# Implementation 2\n",
    "Kr = torch.tensor(list(range(0, n_k)) + [-n_k], dtype=torch.float64)\n",
    "bases = torch.exp(\n",
    "    torch.outer(\n",
    "        torch.linspace(0, n_x - 1, n_x, dtype=torch.float64),\n",
    "        -2 * np.pi / n_x * 1.0j * Kr,\n",
    "    )\n",
    ")\n",
    "x_ft2 = torch.einsum(\"bcx,xk->bck\", x, bases)\n",
    "print(\"Error between x_ft and x_ft2 is \", torch.norm(x_ft - x_ft2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ift = torch.fft.irfftn(x_ft, dim=[2])\n",
    "print(\"Error between x and x_ift is \", torch.norm(x - x_ift))\n",
    "\n",
    "modes = 6\n",
    "#  Truncate to the first m modes\n",
    "x_ft_trunc = torch.zeros(n_b, n_c, n_k + 1, dtype=torch.complex128)\n",
    "x_ft_trunc[:, :, :modes] = x_ft[:, :, :modes]\n",
    "\n",
    "x_ift_trunc = torch.fft.irfftn(x_ft_trunc, dim=[2])\n",
    "\n",
    "# Implementation 1\n",
    "Kr = torch.tensor(list(range(0, n_k)) + [-n_k], dtype=torch.float64)\n",
    "bases = torch.exp(\n",
    "    torch.outer(\n",
    "        torch.linspace(0, n_x - 1, n_x, dtype=torch.float64),\n",
    "        2 * np.pi / n_x * 1.0j * Kr,\n",
    "    )\n",
    ")\n",
    "bases[:, 1:-1] *= 2.0\n",
    "x_ift2 = torch.real(torch.einsum(\"bck,xk->bcx\", x_ft_trunc, bases)) / n_x\n",
    "print(\"Error between x and x_ift2 is \", torch.norm(x_ift_trunc - x_ift2))\n",
    "\n",
    "# Implementation 2\n",
    "Kr = torch.tensor(list(range(0, modes)), dtype=torch.float64)\n",
    "bases = torch.exp(\n",
    "    torch.outer(\n",
    "        torch.linspace(0, n_x - 1, n_x, dtype=torch.float64),\n",
    "        2 * np.pi / n_x * 1.0j * Kr,\n",
    "    )\n",
    ")\n",
    "bases[:, 1:] *= 2.0\n",
    "x_ift2 = torch.real(torch.einsum(\"bck,xk->bcx\", x_ft_trunc[:, :, :modes], bases)) / n_x\n",
    "print(\"Error between x and x_ift2 is \", torch.norm(x_ift_trunc - x_ift2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Transform\n",
    "\n",
    "We compute the spectral transform for the last dimension of x. \n",
    "We first define $n_k$ orthonal bases $\\{ \\phi_k \\}_{k = 0}^{n_k-1}$\n",
    "\n",
    "\n",
    "Then the spectral transform and inverse spectral transform give the relation between \n",
    "$\\{\\hat{f}[k]: k \\in K \\}$ and $\\{f(x_j) : 0 \\leq j \\leq n_x\\}$\n",
    "\n",
    "\\begin{align*}\n",
    "   f(x) &= \\sum_{k=0}^{n_k-1}  \\hat{f}[k]  \\phi_k(x) \n",
    "   \\\\ \n",
    "   f(x_j) &= \\sum_{k_x=0}^{n_k-1}  \\hat{f}[k]  \\phi_k(x_j)\n",
    "   \\\\ \n",
    "   \\hat{f}[k] &= \\int f(x)  \\phi_k(x) dx\\\\\n",
    "                &= \\sum_{j = 0}^{n_x}  f(x_j)  \\phi_k(x_j) \\Delta x_j\n",
    "\\end{align*}\n",
    "Here $\\phi_k(x)$ are orthogonal bases with $$\\int \\phi_i(x) \\overline{\\phi_k(x)} dx = \\delta_{ik},$$ and at the discrete level\n",
    "$$\\sum_{j = 0}^{n_x} \\phi_i(x_j) \\overline{\\phi_j(x_j)} \\Delta x_j = \\delta_{ik}$$\n",
    "\n",
    "\n",
    "\n",
    "We set bases \n",
    "\\begin{equation}\n",
    "B = \n",
    "\\begin{bmatrix}\n",
    "\\phi_0(x_j)  & \\phi_1(x_j)  &  \\cdots &  \\phi_{n_k-1}(x_j)  \n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "and weighted bases \n",
    "\\begin{equation}\n",
    "B = \n",
    "\\begin{bmatrix}\n",
    "\\phi_0(x_j) \\Delta x_j  & \\phi_1(x_j) \\Delta x_j &  \\cdots &  \\phi_{n_k-1}(x_j) \\Delta x_j\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "To recover 1D Fourier Transform, we set \n",
    "\n",
    "$$\\phi_k(x) = \\frac{1}{\\sqrt{L_x}},\\, \\frac{\\sqrt{2}}{\\sqrt{L_x}}\\cos(\\frac{2\\pi  x}{L_x}), \\frac{\\sqrt{2}}{\\sqrt{L_x}}\\sin(\\frac{2\\pi  x}{L_x}), \\cdots \\frac{\\sqrt{2}}{\\sqrt{L_x}}\\cos(\\frac{2k\\pi x}{L_x}), \\frac{\\sqrt{2}}{\\sqrt{L_x}}\\sin(\\frac{2k\\pi x}{L_x}),\\cdots $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "downsample_ratio = 16\n",
    "\n",
    "L = 1.0\n",
    "Ne_ref = data_in.shape[1]\n",
    "grid = np.linspace(0, L, Ne_ref + 1)[:-1]\n",
    "\n",
    "n_train = n_test = 2**10\n",
    "x_train = torch.from_numpy(\n",
    "    np.stack(\n",
    "        (\n",
    "            data_in[0:n_train, 0::downsample_ratio],\n",
    "            np.tile(grid[0::downsample_ratio], (n_train, 1)),\n",
    "        ),\n",
    "        axis=-1,\n",
    "    ).astype(np.float32)\n",
    ")\n",
    "y_train = torch.from_numpy(\n",
    "    data_out[0:n_train, 0::downsample_ratio, np.newaxis].astype(np.float32)\n",
    ")\n",
    "# x_train, y_train are [n_data, n_x, n_channel] arrays\n",
    "x_test = torch.from_numpy(\n",
    "    np.stack(\n",
    "        (\n",
    "            data_in[-n_test:, 0::downsample_ratio],\n",
    "            np.tile(grid[0::downsample_ratio], (n_train, 1)),\n",
    "        ),\n",
    "        axis=-1,\n",
    "    ).astype(np.float32)\n",
    ")\n",
    "y_test = torch.from_numpy(\n",
    "    data_out[-n_test:, 0::downsample_ratio, np.newaxis].astype(np.float32)\n",
    ")\n",
    "# x_test, y_test are [n_data, n_x, n_channel] arrays\n",
    "\n",
    "\n",
    "n_fno_layers = 3\n",
    "k_max = 33  # 16\n",
    "d_f = 128\n",
    "# fourier k_max\n",
    "modes = [k_max] * n_fno_layers\n",
    "# channel d_f\n",
    "layers = [d_f] * (n_fno_layers + 1)\n",
    "fc_dim = d_f\n",
    "in_dim = 2\n",
    "out_dim = 1\n",
    "act = \"gelu\"\n",
    "\n",
    "epochs = 1000\n",
    "base_lr = 0.001\n",
    "\n",
    "\n",
    "milestones = [200, 300, 400, 500, 800, 900]\n",
    "scheduler_gamma = 0.5\n",
    "\n",
    "scheduler = \"MultiStepLR\"\n",
    "weight_decay = 1.0e-4\n",
    "# batch_size=32\n",
    "dim = 1\n",
    "\n",
    "# scheduler = \"CosineAnnealingLR\"\n",
    "# weight_decay = 1.0e-4\n",
    "batch_size = 64\n",
    "\n",
    "normalization_x = True\n",
    "normalization_y = True\n",
    "normalization_dim = []\n",
    "pad_ratio = 0.05\n",
    "\n",
    "\n",
    "basis_type = \"Galerkin_bases\"\n",
    "\n",
    "if basis_type == \"Fast_Fourier_Transform\":\n",
    "\n",
    "    k_max = k_max // 2  # 16\n",
    "    modes = [k_max] * n_fno_layers\n",
    "\n",
    "    bases = None\n",
    "    wbases = None\n",
    "    model_type = \"FNO\"\n",
    "\n",
    "elif basis_type == \"Fourier_bases\":\n",
    "\n",
    "    Ne = Ne_ref // downsample_ratio\n",
    "    grid, fbases, weights = compute_1dFourier_bases(Ne, k_max, L)\n",
    "    wfbases = fbases * np.tile(weights, (k_max, 1)).T\n",
    "    bases = [torch.from_numpy(fbases.astype(np.float32))]\n",
    "    wbases = [torch.from_numpy(wfbases.astype(np.float32))]\n",
    "    model_type = \"GalerkinNO\"\n",
    "\n",
    "elif basis_type == \"Galerkin_bases\":\n",
    "\n",
    "    Ne = Ne_ref // downsample_ratio\n",
    "\n",
    "    pca_data = data_out[0:n_train, 0::downsample_ratio]\n",
    "    pca_include_input = False\n",
    "    pca_include_grid = False\n",
    "    if pca_include_input:\n",
    "        pca_data = np.vstack((pca_data, data_in[0:n_train, 0::downsample_ratio]))\n",
    "    if pca_include_grid:\n",
    "        n_grid = 1\n",
    "        pca_data = np.vstack(\n",
    "            (pca_data, np.tile(grid[0::downsample_ratio], (n_grid, 1)))\n",
    "        )\n",
    "\n",
    "    U, S, VT = np.linalg.svd(pca_data.T)\n",
    "    # the integration of the basis is 1.\n",
    "    fbases = U[:, 0:k_max] / np.sqrt(L / Ne)\n",
    "    wfbases = L / Ne * fbases\n",
    "    bases = [torch.from_numpy(fbases.astype(np.float32))]\n",
    "    wbases = [torch.from_numpy(wfbases.astype(np.float32))]\n",
    "    model_type = \"GalerkinNO\"\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Bases construction error\")\n",
    "\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"model\": model_type,\n",
    "        \"dim\": dim,\n",
    "        \"modes\": modes,\n",
    "        \"fc_dim\": fc_dim,\n",
    "        \"layers\": layers,\n",
    "        \"in_dim\": in_dim,\n",
    "        \"out_dim\": out_dim,\n",
    "        \"act\": act,\n",
    "        \"pad_ratio\": pad_ratio,\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"base_lr\": base_lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"epochs\": epochs,\n",
    "        \"scheduler\": scheduler,\n",
    "        \"milestones\": milestones,\n",
    "        \"scheduler_gamma\": scheduler_gamma,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"normalization_x\": normalization_x,\n",
    "        \"normalization_y\": normalization_y,\n",
    "        \"normalization_dim\": normalization_dim,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "model = construct_model(config, bases, wbases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Start training \", config[\"model\"][\"model\"])\n",
    "train_rel_l2_losses, test_rel_l2_losses, test_l2_losses, cost = FNN_train(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    config,\n",
    "    model,\n",
    "    save_model_name=\"../models/save/test.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start training  GalerkinNO\n",
    "Epoch :  0  Rel. Train L2 Loss :  0.5530278962105513  Rel. Test L2 Loss :  0.3639783691614866  Test L2 Loss :  0.16688358411192894\n",
    "Epoch :  10  Rel. Train L2 Loss :  0.05540479300543666  Rel. Test L2 Loss :  0.06010106788016856  Test L2 Loss :  0.027525703771971166\n",
    "Epoch :  20  Rel. Train L2 Loss :  0.02729798946529627  Rel. Test L2 Loss :  0.030181090580299497  Test L2 Loss :  0.014549962885212153\n",
    "Epoch :  30  Rel. Train L2 Loss :  0.023524480871856213  Rel. Test L2 Loss :  0.02733145747333765  Test L2 Loss :  0.012954483041539788\n",
    "Epoch :  40  Rel. Train L2 Loss :  0.03390151506755501  Rel. Test L2 Loss :  0.032641293248161674  Test L2 Loss :  0.014019099180586636\n",
    "Epoch :  50  Rel. Train L2 Loss :  0.028370561194606125  Rel. Test L2 Loss :  0.02992247836664319  Test L2 Loss :  0.013746349257417023\n",
    "Epoch :  60  Rel. Train L2 Loss :  0.016377027553971857  Rel. Test L2 Loss :  0.02047357289120555  Test L2 Loss :  0.009557288809446618\n",
    "Epoch :  70  Rel. Train L2 Loss :  0.016247911378741264  Rel. Test L2 Loss :  0.0221169067081064  Test L2 Loss :  0.009881779027637094\n",
    "Epoch :  80  Rel. Train L2 Loss :  0.018533223308622837  Rel. Test L2 Loss :  0.019773047999478877  Test L2 Loss :  0.009210543415974826\n",
    "Epoch :  90  Rel. Train L2 Loss :  0.014880945847835392  Rel. Test L2 Loss :  0.0222996415104717  Test L2 Loss :  0.01034952379995957\n",
    "Epoch :  100  Rel. Train L2 Loss :  0.023593697929754853  Rel. Test L2 Loss :  0.026125550735741854  Test L2 Loss :  0.012204635073430836\n",
    "Epoch :  110  Rel. Train L2 Loss :  0.0163455773727037  Rel. Test L2 Loss :  0.019539905828423798  Test L2 Loss :  0.009263549611205235\n",
    "Epoch :  120  Rel. Train L2 Loss :  0.012967587856110185  Rel. Test L2 Loss :  0.020988903241232038  Test L2 Loss :  0.009709200297947973\n",
    "Epoch :  130  Rel. Train L2 Loss :  0.014892612001858652  Rel. Test L2 Loss :  0.021787052624858916  Test L2 Loss :  0.009926729835569859\n",
    "Epoch :  140  Rel. Train L2 Loss :  0.022582464618608356  Rel. Test L2 Loss :  0.03254837263375521  Test L2 Loss :  0.014737804944161326\n",
    "Epoch :  150  Rel. Train L2 Loss :  0.017551998258568347  Rel. Test L2 Loss :  0.01922531088348478  Test L2 Loss :  0.008974029566161335\n",
    "Epoch :  160  Rel. Train L2 Loss :  0.01702975877560675  Rel. Test L2 Loss :  0.01916457514744252  Test L2 Loss :  0.00896387847024016\n",
    "Epoch :  170  Rel. Train L2 Loss :  0.01095853850711137  Rel. Test L2 Loss :  0.011898183031007648  Test L2 Loss :  0.005927028658334166\n",
    "Epoch :  180  Rel. Train L2 Loss :  0.01998165505938232  Rel. Test L2 Loss :  0.026575087918899953  Test L2 Loss :  0.012266059755347669\n",
    "Epoch :  190  Rel. Train L2 Loss :  0.010873873543459922  Rel. Test L2 Loss :  0.014671170676592737  Test L2 Loss :  0.007119098474504426\n",
    "Epoch :  200  Rel. Train L2 Loss :  0.01028971589403227  Rel. Test L2 Loss :  0.013454765372443944  Test L2 Loss :  0.006566085590748116\n",
    "Epoch :  210  Rel. Train L2 Loss :  0.006467006693128496  Rel. Test L2 Loss :  0.011402946896851063  Test L2 Loss :  0.0056951247388496995\n",
    "Epoch :  220  Rel. Train L2 Loss :  0.007936486625112593  Rel. Test L2 Loss :  0.010351956530939788  Test L2 Loss :  0.005171463039005175\n",
    "Epoch :  230  Rel. Train L2 Loss :  0.005818177392939106  Rel. Test L2 Loss :  0.009853201336227357  Test L2 Loss :  0.005036661037593149\n",
    "Epoch :  240  Rel. Train L2 Loss :  0.011104872624855489  Rel. Test L2 Loss :  0.01421331736491993  Test L2 Loss :  0.006690551701467484\n",
    "Epoch :  250  Rel. Train L2 Loss :  0.007700835034484044  Rel. Test L2 Loss :  0.010917078296188265  Test L2 Loss :  0.005546584783587605\n",
    "Epoch :  260  Rel. Train L2 Loss :  0.0068678474926855415  Rel. Test L2 Loss :  0.011556127166841179  Test L2 Loss :  0.005775771656772122\n",
    "Epoch :  270  Rel. Train L2 Loss :  0.006902240013005212  Rel. Test L2 Loss :  0.011474454251583666  Test L2 Loss :  0.005823984072776511\n",
    "Epoch :  280  Rel. Train L2 Loss :  0.007895537943113595  Rel. Test L2 Loss :  0.011459440807811916  Test L2 Loss :  0.005707353935576975\n",
    "Epoch :  290  Rel. Train L2 Loss :  0.007066981052048504  Rel. Test L2 Loss :  0.012559539813082665  Test L2 Loss :  0.0060487362497951835\n",
    "Epoch :  300  Rel. Train L2 Loss :  0.007625468191690743  Rel. Test L2 Loss :  0.009424212155863643  Test L2 Loss :  0.004864282789640129\n",
    "Epoch :  310  Rel. Train L2 Loss :  0.0034248315932927653  Rel. Test L2 Loss :  0.008168773783836514  Test L2 Loss :  0.004320120162446983\n",
    "Epoch :  320  Rel. Train L2 Loss :  0.0033389161253580824  Rel. Test L2 Loss :  0.008337368431966752  Test L2 Loss :  0.0043621797958621755\n",
    "Epoch :  330  Rel. Train L2 Loss :  0.0035498587385518476  Rel. Test L2 Loss :  0.008126365311909467  Test L2 Loss :  0.004305309703340754\n",
    "Epoch :  340  Rel. Train L2 Loss :  0.0035115051723551005  Rel. Test L2 Loss :  0.008424282394116744  Test L2 Loss :  0.004376467477413826\n",
    "Epoch :  350  Rel. Train L2 Loss :  0.0035720229643629864  Rel. Test L2 Loss :  0.008311846031574532  Test L2 Loss :  0.004340929983300157\n",
    "Epoch :  360  Rel. Train L2 Loss :  0.0037024067278252915  Rel. Test L2 Loss :  0.008369034912902862  Test L2 Loss :  0.004366784778540023\n",
    "Epoch :  370  Rel. Train L2 Loss :  0.003762367312447168  Rel. Test L2 Loss :  0.008014965802431107  Test L2 Loss :  0.00422419115784578\n",
    "Epoch :  380  Rel. Train L2 Loss :  0.005504093074705452  Rel. Test L2 Loss :  0.011670866864733398  Test L2 Loss :  0.0056989417935255915\n",
    "Epoch :  390  Rel. Train L2 Loss :  0.0038136375951580703  Rel. Test L2 Loss :  0.008301144553115591  Test L2 Loss :  0.004283903515897691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fourier bases\n",
    "\n",
    "Epoch :  0  Rel. Train L2 Loss :  0.5343242827802896  Rel. Test L2 Loss :  0.35652035661041737  Test L2 Loss :  0.16434418968856335\n",
    "Epoch :  10  Rel. Train L2 Loss :  0.05252824304625392  Rel. Test L2 Loss :  0.08707353239879012  Test L2 Loss :  0.038476263638585806\n",
    "Epoch :  20  Rel. Train L2 Loss :  0.03379512997344136  Rel. Test L2 Loss :  0.034905026433989406  Test L2 Loss :  0.016287033446133137\n",
    "Epoch :  30  Rel. Train L2 Loss :  0.03300807299092412  Rel. Test L2 Loss :  0.035803209990262985  Test L2 Loss :  0.0160839143791236\n",
    "Epoch :  40  Rel. Train L2 Loss :  0.02496281557250768  Rel. Test L2 Loss :  0.031191959278658032  Test L2 Loss :  0.01455920428270474\n",
    "Epoch :  50  Rel. Train L2 Loss :  0.019304369459860027  Rel. Test L2 Loss :  0.025718713994137943  Test L2 Loss :  0.01162591646425426\n",
    "Epoch :  60  Rel. Train L2 Loss :  0.019517153152264655  Rel. Test L2 Loss :  0.022306667640805244  Test L2 Loss :  0.010376633668784052\n",
    "Epoch :  70  Rel. Train L2 Loss :  0.02815706399269402  Rel. Test L2 Loss :  0.024218744249083102  Test L2 Loss :  0.011378672847058624\n",
    "Epoch :  80  Rel. Train L2 Loss :  0.01887817436363548  Rel. Test L2 Loss :  0.02192652691155672  Test L2 Loss :  0.01027321710716933\n",
    "Epoch :  90  Rel. Train L2 Loss :  0.01543609204236418  Rel. Test L2 Loss :  0.027948095579631627  Test L2 Loss :  0.012622756243217736\n",
    "Epoch :  100  Rel. Train L2 Loss :  0.018864598590880632  Rel. Test L2 Loss :  0.022070111241191626  Test L2 Loss :  0.010092632728628814\n",
    "Epoch :  110  Rel. Train L2 Loss :  0.01430836075451225  Rel. Test L2 Loss :  0.022063812939450145  Test L2 Loss :  0.010145931446459144\n",
    "Epoch :  120  Rel. Train L2 Loss :  0.025544452480971813  Rel. Test L2 Loss :  0.027506385231390595  Test L2 Loss :  0.012613549712114036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNO\n",
    "\n",
    "\n",
    "Epoch :  0  Rel. Train L2 Loss :  0.46048319712281227  Rel. Test L2 Loss :  0.21462657395750284  Test L2 Loss :  0.10189483733847737\n",
    "Epoch :  10  Rel. Train L2 Loss :  0.027924850466661155  Rel. Test L2 Loss :  0.07714566262438893  Test L2 Loss :  0.034094139584340155\n",
    "Epoch :  20  Rel. Train L2 Loss :  0.018535724841058254  Rel. Test L2 Loss :  0.015555003948975354  Test L2 Loss :  0.007413918210659176\n",
    "Epoch :  30  Rel. Train L2 Loss :  0.033206918742507696  Rel. Test L2 Loss :  0.03395757800899446  Test L2 Loss :  0.013370171596761793\n",
    "Epoch :  40  Rel. Train L2 Loss :  0.027009477373212576  Rel. Test L2 Loss :  0.034347848035395145  Test L2 Loss :  0.01633133686846122\n",
    "Epoch :  50  Rel. Train L2 Loss :  0.027066212380304933  Rel. Test L2 Loss :  0.028407053323462605  Test L2 Loss :  0.012727975437883288\n",
    "Epoch :  60  Rel. Train L2 Loss :  0.024569524568505585  Rel. Test L2 Loss :  0.01891404762864113  Test L2 Loss :  0.00866693290299736\n",
    "Epoch :  70  Rel. Train L2 Loss :  0.023619975079782307  Rel. Test L2 Loss :  0.02239359519444406  Test L2 Loss :  0.009641381271649152\n",
    "Epoch :  80  Rel. Train L2 Loss :  0.012316441512666643  Rel. Test L2 Loss :  0.011593313072808087  Test L2 Loss :  0.0054851210152264684\n",
    "Epoch :  90  Rel. Train L2 Loss :  0.022688035969622433  Rel. Test L2 Loss :  0.01963267824612558  Test L2 Loss :  0.008665522997034714\n",
    "Epoch :  100  Rel. Train L2 Loss :  0.014114718767814338  Rel. Test L2 Loss :  0.015954271773807704  Test L2 Loss :  0.006967136519961059"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
