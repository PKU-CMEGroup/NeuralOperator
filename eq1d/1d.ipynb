{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "sys.path.append('../')\n",
    "from models import FNN1d, FNN_train, construct_model, compute_1dFourier_bases\n",
    "torch.set_printoptions(precision=16)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size =  2048  mesh elements =  8192\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../mytest/data/burgers_data_R10.mat\"\n",
    "data = loadmat(data_path)\n",
    "data_in = data[\"a\"]\n",
    "data_out = data[\"u\"]\n",
    "print(\"data size = \", data_in.shape[0], \" mesh elements = \", data_in.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "L = 1.0\n",
    "Ne_ref = data_in.shape[1]\n",
    "grid = np.linspace(0, L, Ne_ref + 1)[:-1]\n",
    "\n",
    "downsample_ratio = 2**5\n",
    "n_train = n_test = 2**10\n",
    "\n",
    "x_train = torch.from_numpy(\n",
    "    np.stack(\n",
    "        (\n",
    "            data_in[0:n_train, 0::downsample_ratio],\n",
    "            np.tile(grid[0::downsample_ratio], (n_train, 1)),\n",
    "        ),\n",
    "        axis=-1,\n",
    "    ).astype(np.float32)\n",
    ")\n",
    "y_train = torch.from_numpy(\n",
    "    data_out[0:n_train, 0::downsample_ratio, np.newaxis].astype(np.float32)\n",
    ")\n",
    "x_test = torch.from_numpy(\n",
    "    np.stack(\n",
    "        (\n",
    "            data_in[-n_test:, 0::downsample_ratio],\n",
    "            np.tile(grid[0::downsample_ratio], (n_train, 1)),\n",
    "        ),\n",
    "        axis=-1,\n",
    "    ).astype(np.float32)\n",
    ")\n",
    "y_test = torch.from_numpy(\n",
    "    data_out[-n_test:, 0::downsample_ratio, np.newaxis].astype(np.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fno_layers = 3\n",
    "k_max = 33  # 16\n",
    "d_f = 128\n",
    "modes = [k_max] * n_fno_layers\n",
    "layers = [d_f] * (n_fno_layers + 1)\n",
    "fc_dim = d_f\n",
    "in_dim = 2\n",
    "out_dim = 1\n",
    "act = \"gelu\"\n",
    "\n",
    "epochs = 1000\n",
    "base_lr = 0.001\n",
    "\n",
    "\n",
    "milestones = [200, 300, 400, 500, 800, 900]\n",
    "scheduler_gamma = 0.5\n",
    "\n",
    "scheduler = \"MultiStepLR\"\n",
    "weight_decay = 1.0e-4\n",
    "dim = 1\n",
    "\n",
    "# weight_decay = 1.0e-4\n",
    "batch_size = 8\n",
    "\n",
    "normalization_x = True\n",
    "normalization_y = True\n",
    "normalization_dim = []\n",
    "pad_ratio = 0.05\n",
    "\n",
    "\n",
    "basis_type = \"Galerkin_bases\"\n",
    "\n",
    "if basis_type == \"Fast_Fourier_Transform\":\n",
    "    k_max = k_max // 2  # 16\n",
    "    modes = [k_max] * n_fno_layers\n",
    "\n",
    "    bases = None\n",
    "    wbases = None\n",
    "    model_type = \"FNO\"\n",
    "\n",
    "elif basis_type == \"Fourier_bases\":\n",
    "    Ne = Ne_ref // downsample_ratio\n",
    "    grid, fbases, weights = compute_1dFourier_bases(Ne, k_max, L)\n",
    "    wfbases = fbases * np.tile(weights, (k_max, 1)).T\n",
    "    bases = [torch.from_numpy(fbases.astype(np.float32))]\n",
    "    wbases = [torch.from_numpy(wfbases.astype(np.float32))]\n",
    "    model_type = \"GalerkinNO\"\n",
    "\n",
    "elif basis_type == \"Galerkin_bases\":\n",
    "    Ne = Ne_ref // downsample_ratio\n",
    "\n",
    "    pca_data = data_out[0:n_train, 0::downsample_ratio]\n",
    "    pca_include_input = False\n",
    "    pca_include_grid = False\n",
    "    if pca_include_input:\n",
    "        pca_data = np.vstack((pca_data, data_in[0:n_train, 0::downsample_ratio]))\n",
    "    if pca_include_grid:\n",
    "        n_grid = 1\n",
    "        pca_data = np.vstack(\n",
    "            (pca_data, np.tile(grid[0::downsample_ratio], (n_grid, 1)))\n",
    "        )\n",
    "\n",
    "    U, S, VT = np.linalg.svd(pca_data.T)\n",
    "    fbases = U[:, 0:k_max] / np.sqrt(L / Ne)\n",
    "    wfbases = L / Ne * fbases\n",
    "    bases = [torch.from_numpy(fbases.astype(np.float32))]\n",
    "    wbases = [torch.from_numpy(wfbases.astype(np.float32))]\n",
    "    model_type = \"GalerkinNO\"\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Bases construction error\")\n",
    "\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"model\": model_type,\n",
    "        \"dim\": dim,\n",
    "        \"modes\": modes,\n",
    "        \"fc_dim\": fc_dim,\n",
    "        \"layers\": layers,\n",
    "        \"in_dim\": in_dim,\n",
    "        \"out_dim\": out_dim,\n",
    "        \"act\": act,\n",
    "        \"pad_ratio\": pad_ratio,\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"base_lr\": base_lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"epochs\": epochs,\n",
    "        \"scheduler\": scheduler,\n",
    "        \"milestones\": milestones,\n",
    "        \"scheduler_gamma\": scheduler_gamma,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"normalization_x\": normalization_x,\n",
    "        \"normalization_y\": normalization_y,\n",
    "        \"normalization_dim\": normalization_dim,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "model = construct_model(config, bases, wbases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training  GalerkinNO\n",
      "Epoch :  0  Rel. Train L2 Loss :  0.3069101405562833  Rel. Test L2 Loss :  0.16008690232411027  Test L2 Loss :  0.07247034311876632\n",
      "Epoch :  1  Rel. Train L2 Loss :  0.11749595683068037  Rel. Test L2 Loss :  0.08777787134749815  Test L2 Loss :  0.04001648219127674\n",
      "Epoch :  2  Rel. Train L2 Loss :  0.07306342566153035  Rel. Test L2 Loss :  0.073866408347385  Test L2 Loss :  0.034203069837531075\n",
      "Epoch :  3  Rel. Train L2 Loss :  0.05907389981439337  Rel. Test L2 Loss :  0.05598695465596393  Test L2 Loss :  0.024817098659696057\n",
      "Epoch :  4  Rel. Train L2 Loss :  0.05187512669363059  Rel. Test L2 Loss :  0.058705813687993214  Test L2 Loss :  0.025549183941620868\n",
      "Epoch :  5  Rel. Train L2 Loss :  0.05691497019142844  Rel. Test L2 Loss :  0.05918113116058521  Test L2 Loss :  0.027704367646947503\n",
      "Epoch :  6  Rel. Train L2 Loss :  0.05201591248624027  Rel. Test L2 Loss :  0.047992852225434035  Test L2 Loss :  0.021752481647126842\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart training \u001b[39m\u001b[38;5;124m\"\u001b[39m, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m train_rel_l2_losses, test_rel_l2_losses, test_l2_losses, cost \u001b[38;5;241m=\u001b[39m \u001b[43mFNN_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../models/save/test.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/NeuralOperator/eq1d/../models/train.py:296\u001b[0m, in \u001b[0;36mFNN_train\u001b[0;34m(x_train, y_train, x_test, y_test, config, model, save_model_name)\u001b[0m\n\u001b[1;32m    293\u001b[0m     y \u001b[38;5;241m=\u001b[39m y_normalizer\u001b[38;5;241m.\u001b[39mdecode(y)\n\u001b[1;32m    295\u001b[0m loss \u001b[38;5;241m=\u001b[39m myloss(out\u001b[38;5;241m.\u001b[39mview(batch_size_, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), y\u001b[38;5;241m.\u001b[39mview(batch_size_, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 296\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    299\u001b[0m train_rel_l2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PyTorch/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PyTorch/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PyTorch/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start training \", config[\"model\"][\"model\"])\n",
    "train_rel_l2_losses, test_rel_l2_losses, test_l2_losses, cost = FNN_train(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    config,\n",
    "    model,\n",
    "    save_model_name=\"../models/save/test.pth\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
