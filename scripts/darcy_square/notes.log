########################################
reference PCNO r = 14  modes = 16
########################################
Casting to tensor
In PCNO_train, ndims =  2
Epoch :  0  Time:  15.391  Rel. Train L2 Loss :  0.19742434960603714  Rel. Test L2 Loss :  0.08618549019098282  Test L2 Loss :  0.0005718771007377654  inv_L_scale:  [0.966, 0.967]
Epoch :  1  Time:  14.288  Rel. Train L2 Loss :  0.06763939067721367  Rel. Test L2 Loss :  0.05575444608926773  Test L2 Loss :  0.00037001272663474084  inv_L_scale:  [0.957, 0.959]
Epoch :  2  Time:  14.086  Rel. Train L2 Loss :  0.04938131162524223  Rel. Test L2 Loss :  0.040672716945409776  Test L2 Loss :  0.00026782573550008235  inv_L_scale:  [0.952, 0.955]
Epoch :  3  Time:  14.093  Rel. Train L2 Loss :  0.03353950850665569  Rel. Test L2 Loss :  0.033356930166482925  Test L2 Loss :  0.00022027170460205524  inv_L_scale:  [0.949, 0.952]
Epoch :  4  Time:  14.222  Rel. Train L2 Loss :  0.02819534295797348  Rel. Test L2 Loss :  0.03037941165268421  Test L2 Loss :  0.00020081788068637251  inv_L_scale:  [0.948, 0.951]
Epoch :  5  Time:  14.372  Rel. Train L2 Loss :  0.023890818417072295  Rel. Test L2 Loss :  0.029518668204545975  Test L2 Loss :  0.0001945967791834846  inv_L_scale:  [0.947, 0.951]
Epoch :  6  Time:  14.498  Rel. Train L2 Loss :  0.023124959751963617  Rel. Test L2 Loss :  0.028037400245666505  Test L2 Loss :  0.00018436776183079927  inv_L_scale:  [0.946, 0.951]
Epoch :  7  Time:  14.695  Rel. Train L2 Loss :  0.022305594220757483  Rel. Test L2 Loss :  0.025350343510508536  Test L2 Loss :  0.00016731698298826813  inv_L_scale:  [0.947, 0.95]
Epoch :  8  Time:  14.821  Rel. Train L2 Loss :  0.023165085658431055  Rel. Test L2 Loss :  0.027005845457315446  Test L2 Loss :  0.00017880833940580487  inv_L_scale:  [0.947, 0.95]
Epoch :  9  Time:  14.941  Rel. Train L2 Loss :  0.020622955590486526  Rel. Test L2 Loss :  0.024773044139146806  Test L2 Loss :  0.0001630914944689721  inv_L_scale:  [0.946, 0.95]
Epoch :  10  Time:  15.162  Rel. Train L2 Loss :  0.019331963077187538  Rel. Test L2 Loss :  0.02387753553688526  Test L2 Loss :  0.00015672193374484776  inv_L_scale:  [0.946, 0.95]
Epoch :  11  Time:  15.21  Rel. Train L2 Loss :  0.01896606236696243  Rel. Test L2 Loss :  0.024704709872603415  Test L2 Loss :  0.00016377963591367007  inv_L_scale:  [0.946, 0.949]
Epoch :  12  Time:  15.376  Rel. Train L2 Loss :  0.01862709890305996  Rel. Test L2 Loss :  0.024633979871869088  Test L2 Loss :  0.0001615924236830324  inv_L_scale:  [0.946, 0.949]
Epoch :  13  Time:  15.278  Rel. Train L2 Loss :  0.019408761762082578  Rel. Test L2 Loss :  0.022634547129273416  Test L2 Loss :  0.00014869379258016123  inv_L_scale:  [0.946, 0.949]
Epoch :  14  Time:  15.426  Rel. Train L2 Loss :  0.019498799942433833  Rel. Test L2 Loss :  0.024775639995932578  Test L2 Loss :  0.0001642602268839255  inv_L_scale:  [0.946, 0.949]
Epoch :  15  Time:  15.451  Rel. Train L2 Loss :  0.01760772031545639  Rel. Test L2 Loss :  0.021485747396945955  Test L2 Loss :  0.0001420809139381163  inv_L_scale:  [0.945, 0.949]
Epoch :  16  Time:  15.754  Rel. Train L2 Loss :  0.01692119987308979  Rel. Test L2 Loss :  0.021733893677592278  Test L2 Loss :  0.00014375860686413944  inv_L_scale:  [0.945, 0.949]
Epoch :  17  Time:  15.655  Rel. Train L2 Loss :  0.017040158711373807  Rel. Test L2 Loss :  0.023856345787644385  Test L2 Loss :  0.00015873569587711246  inv_L_scale:  [0.946, 0.949]
Epoch :  18  Time:  15.808  Rel. Train L2 Loss :  0.015924894139170646  Rel. Test L2 Loss :  0.021577695682644844  Test L2 Loss :  0.0001419342277222313  inv_L_scale:  [0.945, 0.949]
Epoch :  19  Time:  15.758  Rel. Train L2 Loss :  0.016207260206341743  Rel. Test L2 Loss :  0.021788088604807854  Test L2 Loss :  0.00014329444238683208  inv_L_scale:  [0.945, 0.949]
Epoch :  20  Time:  15.772  Rel. Train L2 Loss :  0.016156765446066857  Rel. Test L2 Loss :  0.021520570367574692  Test L2 Loss :  0.00014269877661718055  inv_L_scale:  [0.945, 0.948]
Epoch :  21  Time:  15.825  Rel. Train L2 Loss :  0.016809793308377265  Rel. Test L2 Loss :  0.021164622008800506  Test L2 Loss :  0.00013972830260172486  inv_L_scale:  [0.944, 0.949]
Epoch :  22  Time:  15.804  Rel. Train L2 Loss :  0.015882722295820714  Rel. Test L2 Loss :  0.021446480602025985  Test L2 Loss :  0.00014109225885476916  inv_L_scale:  [0.944, 0.948]
Epoch :  23  Time:  15.879  Rel. Train L2 Loss :  0.0150811368227005  Rel. Test L2 Loss :  0.02386232264339924  Test L2 Loss :  0.00015837416343856604  inv_L_scale:  [0.945, 0.948]
Epoch :  24  Time:  16.123  Rel. Train L2 Loss :  0.01754373823851347  Rel. Test L2 Loss :  0.020564544349908828  Test L2 Loss :  0.00013561872154241428  inv_L_scale:  [0.945, 0.949]
Epoch :  25  Time:  16.399  Rel. Train L2 Loss :  0.016376475147902966  Rel. Test L2 Loss :  0.022760247439146043  Test L2 Loss :  0.00014930035657016562  inv_L_scale:  [0.944, 0.948]
Epoch :  26  Time:  16.198  Rel. Train L2 Loss :  0.016388055086135864  Rel. Test L2 Loss :  0.02103012055158615  Test L2 Loss :  0.00013848419854184613  inv_L_scale:  [0.944, 0.948]
Epoch :  27  Time:  16.135  Rel. Train L2 Loss :  0.01499972765147686  Rel. Test L2 Loss :  0.020508286282420157  Test L2 Loss :  0.00013601903978269548  inv_L_scale:  [0.944, 0.948]
Epoch :  28  Time:  16.138  Rel. Train L2 Loss :  0.014927353836596012  Rel. Test L2 Loss :  0.01907298546284437  Test L2 Loss :  0.00012585048127220945  inv_L_scale:  [0.944, 0.948]
Epoch :  29  Time:  16.723  Rel. Train L2 Loss :  0.016090694822371007  Rel. Test L2 Loss :  0.021663989275693893  Test L2 Loss :  0.00014390293275937438  inv_L_scale:  [0.944, 0.948]
Epoch :  30  Time:  16.809  Rel. Train L2 Loss :  0.015663603231310844  Rel. Test L2 Loss :  0.020431563705205918  Test L2 Loss :  0.00013546986680012195  inv_L_scale:  [0.945, 0.948]
Epoch :  31  Time:  16.832  Rel. Train L2 Loss :  0.015098568372428418  Rel. Test L2 Loss :  0.020476356148719788  Test L2 Loss :  0.00013451709208311513  inv_L_scale:  [0.944, 0.948]
Epoch :  32  Time:  16.545  Rel. Train L2 Loss :  0.014192446075379849  Rel. Test L2 Loss :  0.018550048843026162  Test L2 Loss :  0.00012223782250657677  inv_L_scale:  [0.944, 0.948]
Epoch :  33  Time:  16.524  Rel. Train L2 Loss :  0.014573937766253948  Rel. Test L2 Loss :  0.01963995285332203  Test L2 Loss :  0.0001290557815809734  inv_L_scale:  [0.944, 0.948]
Epoch :  34  Time:  16.524  Rel. Train L2 Loss :  0.014768744111061096  Rel. Test L2 Loss :  0.022257456332445146  Test L2 Loss :  0.00014616291620768605  inv_L_scale:  [0.944, 0.948]
Epoch :  35  Time:  17.974  Rel. Train L2 Loss :  0.016611565724015235  Rel. Test L2 Loss :  0.019717311933636666  Test L2 Loss :  0.00013032045564614236  inv_L_scale:  [0.944, 0.948]
Epoch :  36  Time:  16.924  Rel. Train L2 Loss :  0.016398671954870225  Rel. Test L2 Loss :  0.02085804156959057  Test L2 Loss :  0.00013781749468762426  inv_L_scale:  [0.944, 0.948]
Epoch :  37  Time:  17.579  Rel. Train L2 Loss :  0.01578658262640238  Rel. Test L2 Loss :  0.019867620952427386  Test L2 Loss :  0.00013158826215658336  inv_L_scale:  [0.944, 0.948]
Epoch :  38  Time:  16.888  Rel. Train L2 Loss :  0.014480394884943962  Rel. Test L2 Loss :  0.018906737118959425  Test L2 Loss :  0.00012450220208847895  inv_L_scale:  [0.944, 0.948]
Epoch :  39  Time:  16.906  Rel. Train L2 Loss :  0.013622917108237743  Rel. Test L2 Loss :  0.01910688020288944  Test L2 Loss :  0.00012570295541081578  inv_L_scale:  [0.943, 0.948]
Epoch :  40  Time:  16.922  Rel. Train L2 Loss :  0.015539844751358032  Rel. Test L2 Loss :  0.019607025906443597  Test L2 Loss :  0.00012953454919625073  inv_L_scale:  [0.944, 0.948]
Epoch :  41  Time:  16.908  Rel. Train L2 Loss :  0.015218994736671447  Rel. Test L2 Loss :  0.020020014494657516  Test L2 Loss :  0.0001309040604974143  inv_L_scale:  [0.944, 0.948]
Epoch :  42  Time:  16.923  Rel. Train L2 Loss :  0.015283916272222996  Rel. Test L2 Loss :  0.022146137729287148  Test L2 Loss :  0.00014764379913685842  inv_L_scale:  [0.944, 0.948]
Epoch :  43  Time:  16.92  Rel. Train L2 Loss :  0.01410582397133112  Rel. Test L2 Loss :  0.01977507472038269  Test L2 Loss :  0.00013080100383376701  inv_L_scale:  [0.943, 0.948]
Epoch :  44  Time:  16.922  Rel. Train L2 Loss :  0.014185957670211791  Rel. Test L2 Loss :  0.018732625432312487  Test L2 Loss :  0.00012296529195737093  inv_L_scale:  [0.944, 0.948]
Epoch :  45  Time:  17.233  Rel. Train L2 Loss :  0.014169795148074626  Rel. Test L2 Loss :  0.0182084896042943  Test L2 Loss :  0.00011977923684753478  inv_L_scale:  [0.944, 0.947]
Epoch :  46  Time:  18.724  Rel. Train L2 Loss :  0.015406886711716652  Rel. Test L2 Loss :  0.022257442846894265  Test L2 Loss :  0.00014733001531567425  inv_L_scale:  [0.944, 0.948]
Epoch :  47  Time:  18.127  Rel. Train L2 Loss :  0.014641045235097409  Rel. Test L2 Loss :  0.018187068961560728  Test L2 Loss :  0.00011988279118668288  inv_L_scale:  [0.944, 0.948]
Epoch :  48  Time:  16.734  Rel. Train L2 Loss :  0.015116675533354282  Rel. Test L2 Loss :  0.019910651743412017  Test L2 Loss :  0.0001316985354060307  inv_L_scale:  [0.944, 0.948]
Epoch :  49  Time:  18.277  Rel. Train L2 Loss :  0.013921825282275677  Rel. Test L2 Loss :  0.0184475127607584  Test L2 Loss :  0.00012144586013164371  inv_L_scale:  [0.944, 0.948]
Epoch :  50  Time:  17.861  Rel. Train L2 Loss :  0.013924786791205407  Rel. Test L2 Loss :  0.018796581588685513  Test L2 Loss :  0.0001238888074294664  inv_L_scale:  [0.944, 0.948]
Epoch :  51  Time:  18.527  Rel. Train L2 Loss :  0.013489442884922027  Rel. Test L2 Loss :  0.01921129897236824  Test L2 Loss :  0.0001275625915150158  inv_L_scale:  [0.944, 0.948]
Epoch :  52  Time:  17.933  Rel. Train L2 Loss :  0.01439571813493967  Rel. Test L2 Loss :  0.017577792704105377  Test L2 Loss :  0.00011603820836171508  inv_L_scale:  [0.943, 0.948]
Epoch :  53  Time:  17.91  Rel. Train L2 Loss :  0.0140369034409523  Rel. Test L2 Loss :  0.02223665066063404  Test L2 Loss :  0.00014841803873423488  inv_L_scale:  [0.944, 0.948]
Epoch :  54  Time:  17.907  Rel. Train L2 Loss :  0.013585857436060906  Rel. Test L2 Loss :  0.02415655881166458  Test L2 Loss :  0.00015780922025442122  inv_L_scale:  [0.943, 0.948]
Epoch :  55  Time:  17.46  Rel. Train L2 Loss :  0.015077833630144597  Rel. Test L2 Loss :  0.01894171230494976  Test L2 Loss :  0.00012465764972148464  inv_L_scale:  [0.943, 0.947]
Epoch :  56  Time:  17.499  Rel. Train L2 Loss :  0.014598208092153073  Rel. Test L2 Loss :  0.019863902255892754  Test L2 Loss :  0.0001311054587131366  inv_L_scale:  [0.944, 0.948]
Epoch :  57  Time:  17.503  Rel. Train L2 Loss :  0.013406576611101628  Rel. Test L2 Loss :  0.01882185071706772  Test L2 Loss :  0.00012356773047940805  inv_L_scale:  [0.944, 0.948]
Epoch :  58  Time:  17.501  Rel. Train L2 Loss :  0.01402868665009737  Rel. Test L2 Loss :  0.0190594794601202  Test L2 Loss :  0.00012598456029081718  inv_L_scale:  [0.944, 0.948]
Epoch :  59  Time:  17.528  Rel. Train L2 Loss :  0.015226767390966416  Rel. Test L2 Loss :  0.019784879758954047  Test L2 Loss :  0.00012958957871887832  inv_L_scale:  [0.944, 0.948]
Epoch :  60  Time:  17.537  Rel. Train L2 Loss :  0.014136479713022709  Rel. Test L2 Loss :  0.018113965541124342  Test L2 Loss :  0.00012002944480627775  inv_L_scale:  [0.944, 0.948]
Epoch :  61  Time:  17.547  Rel. Train L2 Loss :  0.014690272882580757  Rel. Test L2 Loss :  0.0199766905605793  Test L2 Loss :  0.0001305564446374774  inv_L_scale:  [0.943, 0.948]
Epoch :  62  Time:  17.545  Rel. Train L2 Loss :  0.01408976323157549  Rel. Test L2 Loss :  0.022897036373615266  Test L2 Loss :  0.0001495392870856449  inv_L_scale:  [0.943, 0.948]
Epoch :  63  Time:  18.381  Rel. Train L2 Loss :  0.013823310412466526  Rel. Test L2 Loss :  0.017509619444608687  Test L2 Loss :  0.00011516704194946215  inv_L_scale:  [0.944, 0.948]
Epoch :  64  Time:  18.722  Rel. Train L2 Loss :  0.013382610365748405  Rel. Test L2 Loss :  0.017996315322816373  Test L2 Loss :  0.00011888135340996087  inv_L_scale:  [0.943, 0.948]
Epoch :  65  Time:  18.703  Rel. Train L2 Loss :  0.015079772740602492  Rel. Test L2 Loss :  0.01998500883579254  Test L2 Loss :  0.00013062422338407486  inv_L_scale:  [0.944, 0.948]
Epoch :  66  Time:  17.725  Rel. Train L2 Loss :  0.014732843391597271  Rel. Test L2 Loss :  0.018225350193679334  Test L2 Loss :  0.00011974419961916283  inv_L_scale:  [0.943, 0.948]
Epoch :  67  Time:  17.626  Rel. Train L2 Loss :  0.015303026497364045  Rel. Test L2 Loss :  0.017482377514243128  Test L2 Loss :  0.00011520388827193528  inv_L_scale:  [0.944, 0.948]
Epoch :  68  Time:  17.666  Rel. Train L2 Loss :  0.013356297694146634  Rel. Test L2 Loss :  0.018104306012392043  Test L2 Loss :  0.00011938752431888134  inv_L_scale:  [0.944, 0.948]
Epoch :  69  Time:  18.684  Rel. Train L2 Loss :  0.013765109464526177  Rel. Test L2 Loss :  0.01892290696501732  Test L2 Loss :  0.0001255745501839556  inv_L_scale:  [0.944, 0.948]
Epoch :  70  Time:  18.7  Rel. Train L2 Loss :  0.014760862931609154  Rel. Test L2 Loss :  0.018543607778847216  Test L2 Loss :  0.00012230523658217861  inv_L_scale:  [0.943, 0.948]
Epoch :  71  Time:  18.679  Rel. Train L2 Loss :  0.0134076189994812  Rel. Test L2 Loss :  0.018419415652751923  Test L2 Loss :  0.00012170727539341897  inv_L_scale:  [0.944, 0.948]
Epoch :  72  Time:  18.705  Rel. Train L2 Loss :  0.01420697682350874  Rel. Test L2 Loss :  0.01781000167131424  Test L2 Loss :  0.0001171855631400831  inv_L_scale:  [0.944, 0.948]
Epoch :  73  Time:  18.725  Rel. Train L2 Loss :  0.015037889122962951  Rel. Test L2 Loss :  0.019099850878119468  Test L2 Loss :  0.00012622902111615985  inv_L_scale:  [0.944, 0.948]
Epoch :  74  Time:  18.693  Rel. Train L2 Loss :  0.01326553238928318  Rel. Test L2 Loss :  0.018107298612594604  Test L2 Loss :  0.00011934881476918235  inv_L_scale:  [0.943, 0.948]
Epoch :  75  Time:  18.704  Rel. Train L2 Loss :  0.014312667421996593  Rel. Test L2 Loss :  0.02063204288482666  Test L2 Loss :  0.00013682029297342523  inv_L_scale:  [0.943, 0.948]
Epoch :  76  Time:  18.698  Rel. Train L2 Loss :  0.013404862008988857  Rel. Test L2 Loss :  0.017116581127047538  Test L2 Loss :  0.00011262965679634363  inv_L_scale:  [0.943, 0.948]
Epoch :  77  Time:  18.684  Rel. Train L2 Loss :  0.012656181454658508  Rel. Test L2 Loss :  0.022752788439393042  Test L2 Loss :  0.00015207657037535682  inv_L_scale:  [0.944, 0.948]
Epoch :  78  Time:  18.688  Rel. Train L2 Loss :  0.014723504766821861  Rel. Test L2 Loss :  0.01764441281557083  Test L2 Loss :  0.00011632225796347484  inv_L_scale:  [0.944, 0.948]
Epoch :  79  Time:  18.687  Rel. Train L2 Loss :  0.014251090206205845  Rel. Test L2 Loss :  0.017627808079123498  Test L2 Loss :  0.00011628344422206283  inv_L_scale:  [0.943, 0.948]
Epoch :  80  Time:  18.675  Rel. Train L2 Loss :  0.014250918805599212  Rel. Test L2 Loss :  0.01874417930841446  Test L2 Loss :  0.0001231859327526763  inv_L_scale:  [0.944, 0.949]
Epoch :  81  Time:  18.689  Rel. Train L2 Loss :  0.014249756090343  Rel. Test L2 Loss :  0.01807551771402359  Test L2 Loss :  0.00011949315550737083  inv_L_scale:  [0.944, 0.948]
Epoch :  82  Time:  18.688  Rel. Train L2 Loss :  0.014569986812770367  Rel. Test L2 Loss :  0.018341601192951203  Test L2 Loss :  0.00012074655329342931  inv_L_scale:  [0.944, 0.948]
Epoch :  83  Time:  18.681  Rel. Train L2 Loss :  0.013866695821285248  Rel. Test L2 Loss :  0.018489062003791332  Test L2 Loss :  0.00012196957279229536  inv_L_scale:  [0.944, 0.949]
Epoch :  84  Time:  18.674  Rel. Train L2 Loss :  0.014609368622303009  Rel. Test L2 Loss :  0.01837706808000803  Test L2 Loss :  0.00012166439060820266  inv_L_scale:  [0.944, 0.949]
Epoch :  85  Time:  18.689  Rel. Train L2 Loss :  0.014404461644589901  Rel. Test L2 Loss :  0.018181199952960015  Test L2 Loss :  0.00011978838942013682  inv_L_scale:  [0.944, 0.949]
Epoch :  86  Time:  18.681  Rel. Train L2 Loss :  0.014044356033205986  Rel. Test L2 Loss :  0.01829193565994501  Test L2 Loss :  0.00012092491291696206  inv_L_scale:  [0.944, 0.949]
Epoch :  87  Time:  18.701  Rel. Train L2 Loss :  0.014002672411501408  Rel. Test L2 Loss :  0.017389959767460824  Test L2 Loss :  0.0001145174220437184  inv_L_scale:  [0.944, 0.949]
Epoch :  88  Time:  18.689  Rel. Train L2 Loss :  0.013753828451037406  Rel. Test L2 Loss :  0.017892838940024375  Test L2 Loss :  0.00011807443457655609  inv_L_scale:  [0.944, 0.949]
Epoch :  89  Time:  18.683  Rel. Train L2 Loss :  0.013333572015166283  Rel. Test L2 Loss :  0.01841915484517813  Test L2 Loss :  0.00012181396479718387  inv_L_scale:  [0.944, 0.949]
Epoch :  90  Time:  18.685  Rel. Train L2 Loss :  0.014471899896860123  Rel. Test L2 Loss :  0.01831204939633608  Test L2 Loss :  0.00012138780992245302  inv_L_scale:  [0.944, 0.948]
Epoch :  91  Time:  18.687  Rel. Train L2 Loss :  0.014089678816497326  Rel. Test L2 Loss :  0.018620516657829284  Test L2 Loss :  0.00012342910515144467  inv_L_scale:  [0.944, 0.949]
Epoch :  92  Time:  18.702  Rel. Train L2 Loss :  0.014561968311667442  Rel. Test L2 Loss :  0.018643172904849053  Test L2 Loss :  0.00012262461066711694  inv_L_scale:  [0.944, 0.949]
Epoch :  93  Time:  18.667  Rel. Train L2 Loss :  0.01364416355639696  Rel. Test L2 Loss :  0.017560742162168026  Test L2 Loss :  0.00011681369214784354  inv_L_scale:  [0.944, 0.949]
Epoch :  94  Time:  18.684  Rel. Train L2 Loss :  0.01379890003055334  Rel. Test L2 Loss :  0.022800492271780968  Test L2 Loss :  0.0001493513330933638  inv_L_scale:  [0.944, 0.949]
Epoch :  95  Time:  18.69  Rel. Train L2 Loss :  0.013528337098658085  Rel. Test L2 Loss :  0.01791776515543461  Test L2 Loss :  0.00011787302530137822  inv_L_scale:  [0.944, 0.949]
Epoch :  96  Time:  18.7  Rel. Train L2 Loss :  0.012937993928790092  Rel. Test L2 Loss :  0.017576310336589813  Test L2 Loss :  0.00011580636113649234  inv_L_scale:  [0.944, 0.949]
Epoch :  97  Time:  18.703  Rel. Train L2 Loss :  0.013632333569228649  Rel. Test L2 Loss :  0.01907841518521309  Test L2 Loss :  0.00012501172808697446  inv_L_scale:  [0.944, 0.949]
Epoch :  98  Time:  18.702  Rel. Train L2 Loss :  0.014173852801322937  Rel. Test L2 Loss :  0.018110157772898674  Test L2 Loss :  0.00011970718711381778  inv_L_scale:  [0.945, 0.949]
Epoch :  99  Time:  19.068  Rel. Train L2 Loss :  0.014949075020849705  Rel. Test L2 Loss :  0.017190131321549416  Test L2 Loss :  0.00011283433006610722  inv_L_scale:  [0.945, 0.949]
Epoch :  100  Time:  18.962  Rel. Train L2 Loss :  0.014201105579733848  Rel. Test L2 Loss :  0.02027267687022686  Test L2 Loss :  0.0001344421348767355  inv_L_scale:  [0.945, 0.95]



#######################################
reference PCNO r = 14  modes = 8
#######################################
Casting to tensor
In PCNO_train, ndims =  2
Epoch :  0  Time:  8.563  Rel. Train L2 Loss :  0.20380036002397536  Rel. Test L2 Loss :  0.08281957924365997  Test L2 Loss :  0.0005496866523753851  inv_L_scale:  [0.957, 0.96]
Epoch :  1  Time:  8.072  Rel. Train L2 Loss :  0.06407091361284256  Rel. Test L2 Loss :  0.05116905853152275  Test L2 Loss :  0.0003380489978007972  inv_L_scale:  [0.946, 0.947]
Epoch :  2  Time:  8.366  Rel. Train L2 Loss :  0.047620640724897384  Rel. Test L2 Loss :  0.04369960457086563  Test L2 Loss :  0.00028927328705322  inv_L_scale:  [0.941, 0.942]
Epoch :  3  Time:  8.123  Rel. Train L2 Loss :  0.03703241620957851  Rel. Test L2 Loss :  0.036255812272429466  Test L2 Loss :  0.0002409663109574467  inv_L_scale:  [0.938, 0.939]
Epoch :  4  Time:  8.127  Rel. Train L2 Loss :  0.030436411678791048  Rel. Test L2 Loss :  0.03100250944495201  Test L2 Loss :  0.00020391292113345117  inv_L_scale:  [0.936, 0.937]
Epoch :  5  Time:  8.158  Rel. Train L2 Loss :  0.02664171992242336  Rel. Test L2 Loss :  0.02803151324391365  Test L2 Loss :  0.00018536774034146218  inv_L_scale:  [0.935, 0.936]
Epoch :  6  Time:  8.12  Rel. Train L2 Loss :  0.02318259461224079  Rel. Test L2 Loss :  0.02779542051255703  Test L2 Loss :  0.0001839230622863397  inv_L_scale:  [0.934, 0.935]
Epoch :  7  Time:  8.304  Rel. Train L2 Loss :  0.02231997314095497  Rel. Test L2 Loss :  0.025184661000967026  Test L2 Loss :  0.000165997858857736  inv_L_scale:  [0.934, 0.935]
Epoch :  8  Time:  8.254  Rel. Train L2 Loss :  0.022155136421322823  Rel. Test L2 Loss :  0.02616755686700344  Test L2 Loss :  0.00017320958664640784  inv_L_scale:  [0.933, 0.935]
Epoch :  9  Time:  8.289  Rel. Train L2 Loss :  0.02083020131289959  Rel. Test L2 Loss :  0.024580099061131476  Test L2 Loss :  0.00016215384588576853  inv_L_scale:  [0.933, 0.935]
Epoch :  10  Time:  8.354  Rel. Train L2 Loss :  0.019865209318697452  Rel. Test L2 Loss :  0.022592894434928894  Test L2 Loss :  0.00014919435983756557  inv_L_scale:  [0.933, 0.934]
Epoch :  11  Time:  8.535  Rel. Train L2 Loss :  0.01996917262673378  Rel. Test L2 Loss :  0.027842913568019868  Test L2 Loss :  0.00018318344664294272  inv_L_scale:  [0.932, 0.934]
Epoch :  12  Time:  8.418  Rel. Train L2 Loss :  0.019986788973212242  Rel. Test L2 Loss :  0.029662437587976456  Test L2 Loss :  0.0001951625559013337  inv_L_scale:  [0.933, 0.934]
Epoch :  13  Time:  8.313  Rel. Train L2 Loss :  0.022462487041950224  Rel. Test L2 Loss :  0.026170999705791474  Test L2 Loss :  0.00017458610353060066  inv_L_scale:  [0.933, 0.934]
Epoch :  14  Time:  8.263  Rel. Train L2 Loss :  0.019932080186903477  Rel. Test L2 Loss :  0.022118201777338982  Test L2 Loss :  0.00014631107274908572  inv_L_scale:  [0.933, 0.935]
Epoch :  15  Time:  8.669  Rel. Train L2 Loss :  0.01744781508296728  Rel. Test L2 Loss :  0.024006903618574143  Test L2 Loss :  0.00015877288533374667  inv_L_scale:  [0.932, 0.934]
Epoch :  16  Time:  8.773  Rel. Train L2 Loss :  0.01872474776953459  Rel. Test L2 Loss :  0.0222770207375288  Test L2 Loss :  0.00014738576719537378  inv_L_scale:  [0.933, 0.934]
Epoch :  17  Time:  8.639  Rel. Train L2 Loss :  0.018342489883303642  Rel. Test L2 Loss :  0.021018047332763672  Test L2 Loss :  0.00013862422842066734  inv_L_scale:  [0.932, 0.934]
Epoch :  18  Time:  8.695  Rel. Train L2 Loss :  0.01549888651072979  Rel. Test L2 Loss :  0.021605668291449547  Test L2 Loss :  0.00014323883864562957  inv_L_scale:  [0.932, 0.933]
Epoch :  19  Time:  8.777  Rel. Train L2 Loss :  0.0164795039370656  Rel. Test L2 Loss :  0.022740085944533348  Test L2 Loss :  0.00014990589115768672  inv_L_scale:  [0.932, 0.933]
Epoch :  20  Time:  8.687  Rel. Train L2 Loss :  0.017029377497732638  Rel. Test L2 Loss :  0.021966705024242403  Test L2 Loss :  0.00014534612215356903  inv_L_scale:  [0.932, 0.933]



##################
gabor filter
###################
Casting to tensor
In PCNO_train, ndims =  2
Epoch :  0  Time:  9.995  Rel. Train L2 Loss :  0.1985353496670723  Rel. Test L2 Loss :  0.08041201964020729  Test L2 Loss :  0.0005420791683718562  inv_L_scale:  [0.952, 0.956]
Epoch :  1  Time:  9.086  Rel. Train L2 Loss :  0.06541079261898994  Rel. Test L2 Loss :  0.05273953929543495  Test L2 Loss :  0.0003503396245650947  inv_L_scale:  [0.944, 0.947]
Epoch :  2  Time:  9.336  Rel. Train L2 Loss :  0.046406895756721495  Rel. Test L2 Loss :  0.04151675745844841  Test L2 Loss :  0.00027606767253018916  inv_L_scale:  [0.938, 0.942]
Epoch :  3  Time:  9.755  Rel. Train L2 Loss :  0.042164236843585966  Rel. Test L2 Loss :  0.03800704598426819  Test L2 Loss :  0.0002509293134789914  inv_L_scale:  [0.936, 0.939]
Epoch :  4  Time:  9.823  Rel. Train L2 Loss :  0.030400414511561395  Rel. Test L2 Loss :  0.03209941424429417  Test L2 Loss :  0.00021033198048826308  inv_L_scale:  [0.934, 0.937]
Epoch :  5  Time:  10.242  Rel. Train L2 Loss :  0.024961414203047753  Rel. Test L2 Loss :  0.02753008410334587  Test L2 Loss :  0.00018177603604272008  inv_L_scale:  [0.933, 0.936]
Epoch :  6  Time:  9.87  Rel. Train L2 Loss :  0.02439292246103287  Rel. Test L2 Loss :  0.0337450472265482  Test L2 Loss :  0.0002250789274694398  inv_L_scale:  [0.932, 0.936]
Epoch :  7  Time:  10.11  Rel. Train L2 Loss :  0.0219511875808239  Rel. Test L2 Loss :  0.028000590652227403  Test L2 Loss :  0.00018542633682955056  inv_L_scale:  [0.932, 0.935]
Epoch :  8  Time:  10.087  Rel. Train L2 Loss :  0.020018960520625115  Rel. Test L2 Loss :  0.024978391900658606  Test L2 Loss :  0.00016392434248700737  inv_L_scale:  [0.931, 0.935]
Epoch :  9  Time:  10.114  Rel. Train L2 Loss :  0.018736989490687847  Rel. Test L2 Loss :  0.024953988641500474  Test L2 Loss :  0.00016539784497581421  inv_L_scale:  [0.931, 0.935]
Epoch :  10  Time:  10.059  Rel. Train L2 Loss :  0.021517713814973832  Rel. Test L2 Loss :  0.02739400252699852  Test L2 Loss :  0.0001800358883338049  inv_L_scale:  [0.931, 0.934]
Epoch :  11  Time:  10.238  Rel. Train L2 Loss :  0.01980226320028305  Rel. Test L2 Loss :  0.02384116396307945  Test L2 Loss :  0.00015739935683086514  inv_L_scale:  [0.931, 0.934]
Epoch :  12  Time:  10.599  Rel. Train L2 Loss :  0.019168064817786216  Rel. Test L2 Loss :  0.022170277833938597  Test L2 Loss :  0.00014546399819664657  inv_L_scale:  [0.931, 0.935]
Epoch :  13  Time:  10.631  Rel. Train L2 Loss :  0.017510635659098626  Rel. Test L2 Loss :  0.022034326791763304  Test L2 Loss :  0.00014527342864312232  inv_L_scale:  [0.93, 0.934]
Epoch :  14  Time:  10.371  Rel. Train L2 Loss :  0.017311347857117652  Rel. Test L2 Loss :  0.021440183147788047  Test L2 Loss :  0.00014123413973720744  inv_L_scale:  [0.93, 0.934]
Epoch :  15  Time:  10.445  Rel. Train L2 Loss :  0.01847520036995411  Rel. Test L2 Loss :  0.026104105412960054  Test L2 Loss :  0.00017329090856947006  inv_L_scale:  [0.93, 0.934]
Epoch :  16  Time:  10.364  Rel. Train L2 Loss :  0.018731626272201537  Rel. Test L2 Loss :  0.021098170056939126  Test L2 Loss :  0.00013896382297389208  inv_L_scale:  [0.93, 0.934]
Epoch :  17  Time:  10.254  Rel. Train L2 Loss :  0.016459386005997657  Rel. Test L2 Loss :  0.02184944711625576  Test L2 Loss :  0.00014329023862956093  inv_L_scale:  [0.93, 0.933]
Epoch :  18  Time:  10.236  Rel. Train L2 Loss :  0.017184933438897133  Rel. Test L2 Loss :  0.025910142883658407  Test L2 Loss :  0.0001714388106483966  inv_L_scale:  [0.93, 0.934]
Epoch :  19  Time:  10.212  Rel. Train L2 Loss :  0.01741927423328161  Rel. Test L2 Loss :  0.02132226191461086  Test L2 Loss :  0.00014085759234149008  inv_L_scale:  [0.93, 0.934]
Epoch :  20  Time:  10.567  Rel. Train L2 Loss :  0.016824451453983785  Rel. Test L2 Loss :  0.025171862691640855  Test L2 Loss :  0.00016509947832673788  inv_L_scale:  [0.93, 0.934]
Epoch :  21  Time:  10.748  Rel. Train L2 Loss :  0.016723620474338532  Rel. Test L2 Loss :  0.020908916369080544  Test L2 Loss :  0.00013798325409879907  inv_L_scale:  [0.93, 0.933]
Epoch :  22  Time:  10.746  Rel. Train L2 Loss :  0.016628341034054757  Rel. Test L2 Loss :  0.023544503077864647  Test L2 Loss :  0.00015467879187781365  inv_L_scale:  [0.93, 0.934]
Epoch :  23  Time:  10.684  Rel. Train L2 Loss :  0.015587367042899132  Rel. Test L2 Loss :  0.019931080378592016  Test L2 Loss :  0.00013075569295324386  inv_L_scale:  [0.93, 0.934]
Epoch :  24  Time:  10.524  Rel. Train L2 Loss :  0.01501925103366375  Rel. Test L2 Loss :  0.02256956659257412  Test L2 Loss :  0.00014799739699810743  inv_L_scale:  [0.929, 0.933]
Epoch :  25  Time:  10.198  Rel. Train L2 Loss :  0.01741459621489048  Rel. Test L2 Loss :  0.02139966353774071  Test L2 Loss :  0.00014061298163142055  inv_L_scale:  [0.93, 0.934]
Epoch :  26  Time:  10.185  Rel. Train L2 Loss :  0.014797338083386421  Rel. Test L2 Loss :  0.021417099982500076  Test L2 Loss :  0.00014133369142655284  inv_L_scale:  [0.929, 0.933]
Epoch :  27  Time:  10.429  Rel. Train L2 Loss :  0.015468715615570545  Rel. Test L2 Loss :  0.020448747426271438  Test L2 Loss :  0.000134971596125979  inv_L_scale:  [0.93, 0.934]
Epoch :  28  Time:  10.654  Rel. Train L2 Loss :  0.01379552585631609  Rel. Test L2 Loss :  0.018530919514596463  Test L2 Loss :  0.0001222312363097444  inv_L_scale:  [0.929, 0.933]
Epoch :  29  Time:  10.607  Rel. Train L2 Loss :  0.015761686235666275  Rel. Test L2 Loss :  0.020335812717676163  Test L2 Loss :  0.00013419849623460322  inv_L_scale:  [0.93, 0.934]
Epoch :  30  Time:  10.626  Rel. Train L2 Loss :  0.015012822471559047  Rel. Test L2 Loss :  0.020129997059702875  Test L2 Loss :  0.0001331054457114078  inv_L_scale:  [0.929, 0.934]
Epoch :  31  Time:  10.62  Rel. Train L2 Loss :  0.016645111940801142  Rel. Test L2 Loss :  0.027555191069841386  Test L2 Loss :  0.0001826954772695899  inv_L_scale:  [0.93, 0.934]
Epoch :  32  Time:  10.585  Rel. Train L2 Loss :  0.01629219962656498  Rel. Test L2 Loss :  0.01983333721756935  Test L2 Loss :  0.00013121180119924248  inv_L_scale:  [0.93, 0.934]
Epoch :  33  Time:  10.625  Rel. Train L2 Loss :  0.014181847013533115  Rel. Test L2 Loss :  0.019598187059164048  Test L2 Loss :  0.00012918759282911195  inv_L_scale:  [0.929, 0.934]
Epoch :  34  Time:  10.581  Rel. Train L2 Loss :  0.014310103341937065  Rel. Test L2 Loss :  0.018887876123189925  Test L2 Loss :  0.0001243368038558401  inv_L_scale:  [0.929, 0.934]
Epoch :  35  Time:  10.411  Rel. Train L2 Loss :  0.014783294409513474  Rel. Test L2 Loss :  0.02115455262362957  Test L2 Loss :  0.0001400338386883959  inv_L_scale:  [0.929, 0.934]
Epoch :  36  Time:  10.439  Rel. Train L2 Loss :  0.015338290877640248  Rel. Test L2 Loss :  0.019890002608299255  Test L2 Loss :  0.00013164495874661952  inv_L_scale:  [0.93, 0.934]
Epoch :  37  Time:  10.829  Rel. Train L2 Loss :  0.015081494390964508  Rel. Test L2 Loss :  0.01918482914566994  Test L2 Loss :  0.00012585666932864116  inv_L_scale:  [0.929, 0.934]
Epoch :  38  Time:  10.755  Rel. Train L2 Loss :  0.016198868967592716  Rel. Test L2 Loss :  0.020606025829911234  Test L2 Loss :  0.00013518723979359494  inv_L_scale:  [0.93, 0.933]
Epoch :  39  Time:  10.882  Rel. Train L2 Loss :  0.015058827951550485  Rel. Test L2 Loss :  0.021231481805443764  Test L2 Loss :  0.00013950802123872564  inv_L_scale:  [0.929, 0.933]
Epoch :  40  Time:  10.825  Rel. Train L2 Loss :  0.014868315950036048  Rel. Test L2 Loss :  0.019071585908532144  Test L2 Loss :  0.00012577428773511201  inv_L_scale:  [0.929, 0.933]
Epoch :  41  Time:  10.78  Rel. Train L2 Loss :  0.014126798279583455  Rel. Test L2 Loss :  0.019516041353344917  Test L2 Loss :  0.00012885833566542715  inv_L_scale:  [0.929, 0.933]
Epoch :  42  Time:  10.907  Rel. Train L2 Loss :  0.01445808458328247  Rel. Test L2 Loss :  0.01853548690676689  Test L2 Loss :  0.00012245526013430209  inv_L_scale:  [0.929, 0.934]
Epoch :  43  Time:  10.642  Rel. Train L2 Loss :  0.014762754082679748  Rel. Test L2 Loss :  0.019644555747509004  Test L2 Loss :  0.00012985434295842424  inv_L_scale:  [0.929, 0.934]
Epoch :  44  Time:  10.951  Rel. Train L2 Loss :  0.014423968471586704  Rel. Test L2 Loss :  0.018220977559685707  Test L2 Loss :  0.00012000004906440154  inv_L_scale:  [0.929, 0.933]
Epoch :  45  Time:  10.743  Rel. Train L2 Loss :  0.014416409842669963  Rel. Test L2 Loss :  0.019787485226988793  Test L2 Loss :  0.0001312672227504663  inv_L_scale:  [0.929, 0.933]
Epoch :  46  Time:  10.373  Rel. Train L2 Loss :  0.016241460122168063  Rel. Test L2 Loss :  0.018724976032972337  Test L2 Loss :  0.00012350723176496103  inv_L_scale:  [0.929, 0.933]
Epoch :  47  Time:  10.318  Rel. Train L2 Loss :  0.01497418738156557  Rel. Test L2 Loss :  0.019377864003181457  Test L2 Loss :  0.00012750983005389572  inv_L_scale:  [0.929, 0.933]
Epoch :  48  Time:  10.433  Rel. Train L2 Loss :  0.014429707206785679  Rel. Test L2 Loss :  0.01956067182123661  Test L2 Loss :  0.00012897895358037202  inv_L_scale:  [0.929, 0.933]
Epoch :  49  Time:  10.858  Rel. Train L2 Loss :  0.014583189472556114  Rel. Test L2 Loss :  0.023893581181764604  Test L2 Loss :  0.00015991530352039263  inv_L_scale:  [0.929, 0.934]
Epoch :  50  Time:  10.933  Rel. Train L2 Loss :  0.015727172270417214  Rel. Test L2 Loss :  0.018279552459716797  Test L2 Loss :  0.00012112184864236041  inv_L_scale:  [0.929, 0.934]
Epoch :  51  Time:  10.907  Rel. Train L2 Loss :  0.013872885338962079  Rel. Test L2 Loss :  0.019379157796502112  Test L2 Loss :  0.0001269415329443291  inv_L_scale:  [0.929, 0.933]
Epoch :  52  Time:  10.92  Rel. Train L2 Loss :  0.014364392474293709  Rel. Test L2 Loss :  0.019436220824718475  Test L2 Loss :  0.00012827788217691704  inv_L_scale:  [0.929, 0.934]
Epoch :  53  Time:  10.923  Rel. Train L2 Loss :  0.014350233122706413  Rel. Test L2 Loss :  0.020276479050517084  Test L2 Loss :  0.00013449108402710408  inv_L_scale:  [0.929, 0.934]
Epoch :  54  Time:  10.935  Rel. Train L2 Loss :  0.013491462625563144  Rel. Test L2 Loss :  0.017745316363871097  Test L2 Loss :  0.00011672524793539196  inv_L_scale:  [0.929, 0.933]
Epoch :  55  Time:  10.972  Rel. Train L2 Loss :  0.012802434913814068  Rel. Test L2 Loss :  0.01888599995523691  Test L2 Loss :  0.00012512058892752976  inv_L_scale:  [0.929, 0.934]
Epoch :  56  Time:  10.764  Rel. Train L2 Loss :  0.01418442242592573  Rel. Test L2 Loss :  0.01992015264928341  Test L2 Loss :  0.000130860481003765  inv_L_scale:  [0.929, 0.934]
Epoch :  57  Time:  10.432  Rel. Train L2 Loss :  0.01485978578031063  Rel. Test L2 Loss :  0.02000491350889206  Test L2 Loss :  0.00013165432028472424  inv_L_scale:  [0.929, 0.934]
Epoch :  58  Time:  10.523  Rel. Train L2 Loss :  0.01427713880687952  Rel. Test L2 Loss :  0.017765776328742502  Test L2 Loss :  0.00011706034390954301  inv_L_scale:  [0.929, 0.934]
Epoch :  59  Time:  10.939  Rel. Train L2 Loss :  0.013932170540094376  Rel. Test L2 Loss :  0.01947038348764181  Test L2 Loss :  0.0001284676804789342  inv_L_scale:  [0.929, 0.934]
Epoch :  60  Time:  10.825  Rel. Train L2 Loss :  0.014198936015367508  Rel. Test L2 Loss :  0.019277951940894128  Test L2 Loss :  0.00012697096506599336  inv_L_scale:  [0.929, 0.934]
Epoch :  61  Time:  10.173  Rel. Train L2 Loss :  0.014023881882429123  Rel. Test L2 Loss :  0.018620421774685383  Test L2 Loss :  0.00012303057446843012  inv_L_scale:  [0.929, 0.934]
Epoch :  62  Time:  10.292  Rel. Train L2 Loss :  0.014617546439170837  Rel. Test L2 Loss :  0.017629740983247755  Test L2 Loss :  0.00011599192424910143  inv_L_scale:  [0.929, 0.934]
Epoch :  63  Time:  10.738  Rel. Train L2 Loss :  0.013024111524224282  Rel. Test L2 Loss :  0.017289312556385994  Test L2 Loss :  0.00011365317448507994  inv_L_scale:  [0.929, 0.934]
Epoch :  64  Time:  10.502  Rel. Train L2 Loss :  0.013405195817351341  Rel. Test L2 Loss :  0.017113625518977642  Test L2 Loss :  0.00011282653751550242  inv_L_scale:  [0.929, 0.934]
Epoch :  65  Time:  10.59  Rel. Train L2 Loss :  0.013713598370552063  Rel. Test L2 Loss :  0.01794665675610304  Test L2 Loss :  0.0001186924657667987  inv_L_scale:  [0.929, 0.933]
Epoch :  66  Time:  10.467  Rel. Train L2 Loss :  0.013709649339318276  Rel. Test L2 Loss :  0.02130706302821636  Test L2 Loss :  0.0001395553830661811  inv_L_scale:  [0.929, 0.934]
Epoch :  67  Time:  10.412  Rel. Train L2 Loss :  0.014564578965306283  Rel. Test L2 Loss :  0.018505892306566237  Test L2 Loss :  0.00012254845758434386  inv_L_scale:  [0.93, 0.934]
Epoch :  68  Time:  10.463  Rel. Train L2 Loss :  0.014159777209162712  Rel. Test L2 Loss :  0.01799962133169174  Test L2 Loss :  0.0001186756876995787  inv_L_scale:  [0.93, 0.934]
Epoch :  69  Time:  10.456  Rel. Train L2 Loss :  0.015475167490541935  Rel. Test L2 Loss :  0.020419992581009863  Test L2 Loss :  0.00013533986377296968  inv_L_scale:  [0.93, 0.934]
Epoch :  70  Time:  10.814  Rel. Train L2 Loss :  0.015242663875222206  Rel. Test L2 Loss :  0.017487895600497724  Test L2 Loss :  0.00011560327344341204  inv_L_scale:  [0.929, 0.934]
Epoch :  71  Time:  10.705  Rel. Train L2 Loss :  0.014500300638377666  Rel. Test L2 Loss :  0.019416394755244256  Test L2 Loss :  0.0001269587071146816  inv_L_scale:  [0.929, 0.934]
Epoch :  72  Time:  10.653  Rel. Train L2 Loss :  0.014034836776554585  Rel. Test L2 Loss :  0.018065850995481015  Test L2 Loss :  0.00011858610523631796  inv_L_scale:  [0.929, 0.934]
Epoch :  73  Time:  10.759  Rel. Train L2 Loss :  0.014707274653017521  Rel. Test L2 Loss :  0.019778731539845468  Test L2 Loss :  0.00012961079715751112  inv_L_scale:  [0.929, 0.934]
Epoch :  74  Time:  10.745  Rel. Train L2 Loss :  0.014663000799715518  Rel. Test L2 Loss :  0.019674144089221954  Test L2 Loss :  0.00012996916339034215  inv_L_scale:  [0.93, 0.934]
Epoch :  75  Time:  10.759  Rel. Train L2 Loss :  0.015042431958019733  Rel. Test L2 Loss :  0.018711373209953308  Test L2 Loss :  0.00012295119842747226  inv_L_scale:  [0.929, 0.934]
Epoch :  76  Time:  10.79  Rel. Train L2 Loss :  0.014386292703449726  Rel. Test L2 Loss :  0.022828422263264658  Test L2 Loss :  0.00014974889811128378  inv_L_scale:  [0.929, 0.934]
Epoch :  77  Time:  10.797  Rel. Train L2 Loss :  0.01570200502872467  Rel. Test L2 Loss :  0.018768782392144204  Test L2 Loss :  0.00012390984717058017  inv_L_scale:  [0.929, 0.934]
Epoch :  78  Time:  10.827  Rel. Train L2 Loss :  0.016700278401374818  Rel. Test L2 Loss :  0.019904904067516327  Test L2 Loss :  0.00013193115446483717  inv_L_scale:  [0.929, 0.935]
Epoch :  79  Time:  10.776  Rel. Train L2 Loss :  0.013750967420637607  Rel. Test L2 Loss :  0.017439759448170663  Test L2 Loss :  0.0001144633351941593  inv_L_scale:  [0.93, 0.934]
Epoch :  80  Time:  10.557  Rel. Train L2 Loss :  0.012464789628982544  Rel. Test L2 Loss :  0.01805554211139679  Test L2 Loss :  0.00011910141736734659  inv_L_scale:  [0.929, 0.935]
Epoch :  81  Time:  10.632  Rel. Train L2 Loss :  0.013243426032364369  Rel. Test L2 Loss :  0.02053209736943245  Test L2 Loss :  0.0001369010269991122  inv_L_scale:  [0.929, 0.935]
Epoch :  82  Time:  10.492  Rel. Train L2 Loss :  0.015535094566643239  Rel. Test L2 Loss :  0.01886917419731617  Test L2 Loss :  0.00012461496720789  inv_L_scale:  [0.929, 0.935]
Epoch :  83  Time:  10.596  Rel. Train L2 Loss :  0.013450884245336055  Rel. Test L2 Loss :  0.017017015777528285  Test L2 Loss :  0.00011196771258255467  inv_L_scale:  [0.93, 0.934]
Epoch :  84  Time:  10.499  Rel. Train L2 Loss :  0.014725182093679906  Rel. Test L2 Loss :  0.017798044234514237  Test L2 Loss :  0.00011690509098116308  inv_L_scale:  [0.93, 0.935]
Epoch :  85  Time:  10.508  Rel. Train L2 Loss :  0.013779051743447781  Rel. Test L2 Loss :  0.020801520720124245  Test L2 Loss :  0.00013579758349806072  inv_L_scale:  [0.93, 0.934]
Epoch :  86  Time:  10.68  Rel. Train L2 Loss :  0.014061704449355603  Rel. Test L2 Loss :  0.017638156190514565  Test L2 Loss :  0.00011613218899583444  inv_L_scale:  [0.93, 0.935]
Epoch :  87  Time:  10.806  Rel. Train L2 Loss :  0.01681728685647249  Rel. Test L2 Loss :  0.022750643119215964  Test L2 Loss :  0.00014763504266738893  inv_L_scale:  [0.93, 0.935]
Epoch :  88  Time:  10.855  Rel. Train L2 Loss :  0.014180230028927326  Rel. Test L2 Loss :  0.01738747216761112  Test L2 Loss :  0.00011376958660548553  inv_L_scale:  [0.93, 0.935]
Epoch :  89  Time:  10.635  Rel. Train L2 Loss :  0.014142107263207436  Rel. Test L2 Loss :  0.017340031266212464  Test L2 Loss :  0.00011407793732360006  inv_L_scale:  [0.93, 0.935]
Epoch :  90  Time:  10.767  Rel. Train L2 Loss :  0.01295887502282858  Rel. Test L2 Loss :  0.017576711662113668  Test L2 Loss :  0.00011629219865426421  inv_L_scale:  [0.93, 0.935]
Epoch :  91  Time:  10.843  Rel. Train L2 Loss :  0.012987484268844128  Rel. Test L2 Loss :  0.019415850266814232  Test L2 Loss :  0.00012920004111947493  inv_L_scale:  [0.93, 0.935]
Epoch :  92  Time:  10.868  Rel. Train L2 Loss :  0.013224541313946247  Rel. Test L2 Loss :  0.019741981774568557  Test L2 Loss :  0.00013124349701683968  inv_L_scale:  [0.93, 0.935]
Epoch :  93  Time:  10.79  Rel. Train L2 Loss :  0.013766368575394154  Rel. Test L2 Loss :  0.02020235911011696  Test L2 Loss :  0.00013447965146042406  inv_L_scale:  [0.93, 0.935]
Epoch :  94  Time:  10.744  Rel. Train L2 Loss :  0.014158376976847648  Rel. Test L2 Loss :  0.018438837826251983  Test L2 Loss :  0.00012085241207387298  inv_L_scale:  [0.93, 0.935]
Epoch :  95  Time:  10.822  Rel. Train L2 Loss :  0.013051979012787342  Rel. Test L2 Loss :  0.01835319008678198  Test L2 Loss :  0.00012037727050483227  inv_L_scale:  [0.93, 0.935]
Epoch :  96  Time:  10.8  Rel. Train L2 Loss :  0.01408326905965805  Rel. Test L2 Loss :  0.01721676964312792  Test L2 Loss :  0.00011375845817383378  inv_L_scale:  [0.93, 0.935]
Epoch :  97  Time:  10.838  Rel. Train L2 Loss :  0.012957371346652508  Rel. Test L2 Loss :  0.01893294297158718  Test L2 Loss :  0.00012410502909915522  inv_L_scale:  [0.93, 0.934]
Epoch :  98  Time:  10.87  Rel. Train L2 Loss :  0.01355208533257246  Rel. Test L2 Loss :  0.016893295496702196  Test L2 Loss :  0.00011128241138067097  inv_L_scale:  [0.93, 0.935]
Epoch :  99  Time:  10.822  Rel. Train L2 Loss :  0.013854590594768524  Rel. Test L2 Loss :  0.017805987671017647  Test L2 Loss :  0.00011705569573678076  inv_L_scale:  [0.93, 0.935]
Epoch :  100  Time:  10.515  Rel. Train L2 Loss :  0.016008330196142197  Rel. Test L2 Loss :  0.019457916244864464  Test L2 Loss :  0.00012744855805067346  inv_L_scale:  [0.93, 0.935]
Epoch :  101  Time:  10.5  Rel. Train L2 Loss :  0.014560564868152142  Rel. Test L2 Loss :  0.017697083465754986  Test L2 Loss :  0.00011615154944593087  inv_L_scale:  [0.93, 0.935]
Epoch :  102  Time:  10.485  Rel. Train L2 Loss :  0.013673497505486012  Rel. Test L2 Loss :  0.018866753093898295  Test L2 Loss :  0.00012311869009863587  inv_L_scale:  [0.93, 0.935]
Epoch :  103  Time:  10.472  Rel. Train L2 Loss :  0.01417858176678419  Rel. Test L2 Loss :  0.01821884833276272  Test L2 Loss :  0.00011987706238869578  inv_L_scale:  [0.93, 0.935]
Epoch :  104  Time:  10.883  Rel. Train L2 Loss :  0.015463016182184219  Rel. Test L2 Loss :  0.019535756707191466  Test L2 Loss :  0.00012899324792670086  inv_L_scale:  [0.93, 0.935]
Epoch :  105  Time:  10.971  Rel. Train L2 Loss :  0.014775540560483932  Rel. Test L2 Loss :  0.01833765983581543  Test L2 Loss :  0.00012069161573890596  inv_L_scale:  [0.93, 0.935]
Epoch :  106  Time:  10.789  Rel. Train L2 Loss :  0.013370167560875416  Rel. Test L2 Loss :  0.018583380803465843  Test L2 Loss :  0.00012290207319892943  inv_L_scale:  [0.93, 0.935]
Epoch :  107  Time:  10.817  Rel. Train L2 Loss :  0.015046359360218049  Rel. Test L2 Loss :  0.02182626396417618  Test L2 Loss :  0.00014288158185081555  inv_L_scale:  [0.93, 0.934]
Epoch :  108  Time:  10.869  Rel. Train L2 Loss :  0.014804431162774563  Rel. Test L2 Loss :  0.017460211366415023  Test L2 Loss :  0.00011551842209883034  inv_L_scale:  [0.93, 0.935]
Epoch :  109  Time:  10.835  Rel. Train L2 Loss :  0.01246728003025055  Rel. Test L2 Loss :  0.016353696547448636  Test L2 Loss :  0.00010798942967085167  inv_L_scale:  [0.93, 0.935]
Epoch :  110  Time:  10.832  Rel. Train L2 Loss :  0.012160633176565171  Rel. Test L2 Loss :  0.016676186844706536  Test L2 Loss :  0.00011015278258128092  inv_L_scale:  [0.93, 0.934]
Epoch :  111  Time:  10.956  Rel. Train L2 Loss :  0.013411493793129921  Rel. Test L2 Loss :  0.017008015140891074  Test L2 Loss :  0.00011242863722145558  inv_L_scale:  [0.93, 0.935]
Epoch :  112  Time:  10.906  Rel. Train L2 Loss :  0.013154407039284706  Rel. Test L2 Loss :  0.02086336672306061  Test L2 Loss :  0.00013701517425943166  inv_L_scale:  [0.93, 0.934]
Epoch :  113  Time:  10.767  Rel. Train L2 Loss :  0.013684587173163891  Rel. Test L2 Loss :  0.01816275227814913  Test L2 Loss :  0.00011984478129306808  inv_L_scale:  [0.93, 0.935]
Epoch :  114  Time:  10.623  Rel. Train L2 Loss :  0.014610482111573219  Rel. Test L2 Loss :  0.021188442930579185  Test L2 Loss :  0.00013846433081198483  inv_L_scale:  [0.93, 0.935]
Epoch :  115  Time:  10.612  Rel. Train L2 Loss :  0.014052661210298538  Rel. Test L2 Loss :  0.018527652025222777  Test L2 Loss :  0.0001222703736857511  inv_L_scale:  [0.931, 0.935]
Epoch :  116  Time:  10.646  Rel. Train L2 Loss :  0.014169574737548828  Rel. Test L2 Loss :  0.017548769935965536  Test L2 Loss :  0.00011528437171364203  inv_L_scale:  [0.931, 0.935]
Epoch :  117  Time:  10.588  Rel. Train L2 Loss :  0.013939672350883484  Rel. Test L2 Loss :  0.017155942246317862  Test L2 Loss :  0.00011301210906822234  inv_L_scale:  [0.93, 0.934]
Epoch :  118  Time:  10.903  Rel. Train L2 Loss :  0.013346014901995659  Rel. Test L2 Loss :  0.01782196342945099  Test L2 Loss :  0.00011837676342111081  inv_L_scale:  [0.931, 0.935]
Epoch :  119  Time:  10.879  Rel. Train L2 Loss :  0.013134726636111736  Rel. Test L2 Loss :  0.018212846592068673  Test L2 Loss :  0.00012062939669704064  inv_L_scale:  [0.931, 0.935]
Epoch :  120  Time:  10.803  Rel. Train L2 Loss :  0.014825041636824608  Rel. Test L2 Loss :  0.019762505516409873  Test L2 Loss :  0.00012899202469270676  inv_L_scale:  [0.931, 0.935]
Epoch :  121  Time:  10.868  Rel. Train L2 Loss :  0.015127808786928654  Rel. Test L2 Loss :  0.018504471704363822  Test L2 Loss :  0.00012125663575716316  inv_L_scale:  [0.93, 0.935]
Epoch :  122  Time:  10.869  Rel. Train L2 Loss :  0.013735005162656307  Rel. Test L2 Loss :  0.017156954631209375  Test L2 Loss :  0.00011311234295135363  inv_L_scale:  [0.931, 0.935]
Epoch :  123  Time:  10.847  Rel. Train L2 Loss :  0.013654914133250713  Rel. Test L2 Loss :  0.016330105662345887  Test L2 Loss :  0.00010729447734775021  inv_L_scale:  [0.931, 0.935]
Epoch :  124  Time:  10.854  Rel. Train L2 Loss :  0.013760669700801373  Rel. Test L2 Loss :  0.016970203816890718  Test L2 Loss :  0.00011165951495058835  inv_L_scale:  [0.931, 0.935]
Epoch :  125  Time:  10.884  Rel. Train L2 Loss :  0.013116851173341274  Rel. Test L2 Loss :  0.0188496645539999  Test L2 Loss :  0.00012501463148510084  inv_L_scale:  [0.931, 0.935]
Epoch :  126  Time:  10.899  Rel. Train L2 Loss :  0.013069550514221191  Rel. Test L2 Loss :  0.01726807951927185  Test L2 Loss :  0.00011366359802195803  inv_L_scale:  [0.931, 0.935]
Epoch :  127  Time:  10.889  Rel. Train L2 Loss :  0.012940765619277953  Rel. Test L2 Loss :  0.016653274558484553  Test L2 Loss :  0.00010947408620268106  inv_L_scale:  [0.931, 0.934]
Epoch :  128  Time:  10.798  Rel. Train L2 Loss :  0.012843761272728443  Rel. Test L2 Loss :  0.01745338410139084  Test L2 Loss :  0.00011501260218210518  inv_L_scale:  [0.931, 0.935]
Epoch :  129  Time:  10.871  Rel. Train L2 Loss :  0.012830095276236533  Rel. Test L2 Loss :  0.018502728566527366  Test L2 Loss :  0.000123150312283542  inv_L_scale:  [0.931, 0.935]
Epoch :  130  Time:  10.927  Rel. Train L2 Loss :  0.01458707807958126  Rel. Test L2 Loss :  0.018580362275242804  Test L2 Loss :  0.00012320998357608914  inv_L_scale:  [0.931, 0.935]
Epoch :  131  Time:  10.929  Rel. Train L2 Loss :  0.013036484584212303  Rel. Test L2 Loss :  0.017471457831561565  Test L2 Loss :  0.00011477579508209601  inv_L_scale:  [0.931, 0.935]
Epoch :  132  Time:  10.727  Rel. Train L2 Loss :  0.012996482752263547  Rel. Test L2 Loss :  0.022836030423641206  Test L2 Loss :  0.00014963345311116427  inv_L_scale:  [0.931, 0.935]
Epoch :  133  Time:  10.725  Rel. Train L2 Loss :  0.01272889919579029  Rel. Test L2 Loss :  0.017940043285489083  Test L2 Loss :  0.00011771902354666963  inv_L_scale:  [0.931, 0.935]
Epoch :  134  Time:  10.713  Rel. Train L2 Loss :  0.013382771246135235  Rel. Test L2 Loss :  0.01764665499329567  Test L2 Loss :  0.00011626470746705309  inv_L_scale:  [0.93, 0.935]
Epoch :  135  Time:  10.771  Rel. Train L2 Loss :  0.013723089270293712  Rel. Test L2 Loss :  0.0185335286706686  Test L2 Loss :  0.00012243232456967234  inv_L_scale:  [0.931, 0.935]
Epoch :  136  Time:  10.759  Rel. Train L2 Loss :  0.012915557593107224  Rel. Test L2 Loss :  0.017032151371240617  Test L2 Loss :  0.00011236092599574477  inv_L_scale:  [0.93, 0.935]
Epoch :  137  Time:  10.721  Rel. Train L2 Loss :  0.013083924889564514  Rel. Test L2 Loss :  0.016867073364555837  Test L2 Loss :  0.00011097670445451512  inv_L_scale:  [0.931, 0.935]
Epoch :  138  Time:  10.808  Rel. Train L2 Loss :  0.013718815535306931  Rel. Test L2 Loss :  0.016780811958014966  Test L2 Loss :  0.00011036102776415646  inv_L_scale:  [0.93, 0.935]
Epoch :  139  Time:  10.812  Rel. Train L2 Loss :  0.012696244023740292  Rel. Test L2 Loss :  0.01858948826789856  Test L2 Loss :  0.00012145000568125397  inv_L_scale:  [0.93, 0.935]
Epoch :  140  Time:  10.805  Rel. Train L2 Loss :  0.014021856725215913  Rel. Test L2 Loss :  0.017293891534209252  Test L2 Loss :  0.00011435825435910374  inv_L_scale:  [0.931, 0.935]
Epoch :  141  Time:  10.874  Rel. Train L2 Loss :  0.013468803167343139  Rel. Test L2 Loss :  0.019430385157465934  Test L2 Loss :  0.0001290756170055829  inv_L_scale:  [0.931, 0.935]
Epoch :  142  Time:  10.823  Rel. Train L2 Loss :  0.013414865054190159  Rel. Test L2 Loss :  0.016523394882678986  Test L2 Loss :  0.00010863441159017384  inv_L_scale:  [0.93, 0.935]
Epoch :  143  Time:  10.959  Rel. Train L2 Loss :  0.013510208696126938  Rel. Test L2 Loss :  0.017095979005098343  Test L2 Loss :  0.00011245355097344145  inv_L_scale:  [0.931, 0.935]
Epoch :  144  Time:  10.955  Rel. Train L2 Loss :  0.013672225959599018  Rel. Test L2 Loss :  0.016969777829945087  Test L2 Loss :  0.00011139935581013561  inv_L_scale:  [0.931, 0.935]
Epoch :  145  Time:  10.94  Rel. Train L2 Loss :  0.01223772343248129  Rel. Test L2 Loss :  0.01768885154277086  Test L2 Loss :  0.00011635501810815185  inv_L_scale:  [0.931, 0.935]
Epoch :  146  Time:  10.824  Rel. Train L2 Loss :  0.012349471755325794  Rel. Test L2 Loss :  0.017685462534427644  Test L2 Loss :  0.00011617747048148886  inv_L_scale:  [0.931, 0.934]
Epoch :  147  Time:  10.859  Rel. Train L2 Loss :  0.012286896862089634  Rel. Test L2 Loss :  0.018665016107261182  Test L2 Loss :  0.00012232688430231065  inv_L_scale:  [0.931, 0.935]
Epoch :  148  Time:  10.907  Rel. Train L2 Loss :  0.012500767797231674  Rel. Test L2 Loss :  0.016590347066521644  Test L2 Loss :  0.0001093398142256774  inv_L_scale:  [0.931, 0.935]
Epoch :  149  Time:  10.895  Rel. Train L2 Loss :  0.01302785938233137  Rel. Test L2 Loss :  0.018782682307064532  Test L2 Loss :  0.00012362843874143435  inv_L_scale:  [0.931, 0.935]
Epoch :  150  Time:  10.858  Rel. Train L2 Loss :  0.013015017971396446  Rel. Test L2 Loss :  0.016671076901257038  Test L2 Loss :  0.0001101819274481386  inv_L_scale:  [0.931, 0.936]
Epoch :  151  Time:  10.875  Rel. Train L2 Loss :  0.013110500350594521  Rel. Test L2 Loss :  0.016280312687158585  Test L2 Loss :  0.00010720898455474526  inv_L_scale:  [0.931, 0.935]
Epoch :  152  Time:  10.838  Rel. Train L2 Loss :  0.013163668125867844  Rel. Test L2 Loss :  0.01844406820833683  Test L2 Loss :  0.00012138055142713711  inv_L_scale:  [0.931, 0.935]
Epoch :  153  Time:  10.826  Rel. Train L2 Loss :  0.01254709642380476  Rel. Test L2 Loss :  0.018587302267551422  Test L2 Loss :  0.00012206024315673858  inv_L_scale:  [0.931, 0.935]
Epoch :  154  Time:  10.95  Rel. Train L2 Loss :  0.013013458766043186  Rel. Test L2 Loss :  0.01680451672524214  Test L2 Loss :  0.00011151976737892255  inv_L_scale:  [0.93, 0.935]
Epoch :  155  Time:  11.015  Rel. Train L2 Loss :  0.012520882800221443  Rel. Test L2 Loss :  0.016948598846793176  Test L2 Loss :  0.00011191531550139189  inv_L_scale:  [0.931, 0.935]
Epoch :  156  Time:  10.903  Rel. Train L2 Loss :  0.012574049256742001  Rel. Test L2 Loss :  0.019014821015298368  Test L2 Loss :  0.00012514094152720644  inv_L_scale:  [0.93, 0.935]
Epoch :  157  Time:  10.899  Rel. Train L2 Loss :  0.012924339517951012  Rel. Test L2 Loss :  0.017261719033122062  Test L2 Loss :  0.00011416551162255928  inv_L_scale:  [0.931, 0.936]
Epoch :  158  Time:  10.855  Rel. Train L2 Loss :  0.012870428770780564  Rel. Test L2 Loss :  0.01791014101356268  Test L2 Loss :  0.00011803449539002031  inv_L_scale:  [0.931, 0.935]
Epoch :  159  Time:  10.916  Rel. Train L2 Loss :  0.012526079192757606  Rel. Test L2 Loss :  0.016621096432209014  Test L2 Loss :  0.00010925540322205051  inv_L_scale:  [0.931, 0.935]
Epoch :  160  Time:  10.901  Rel. Train L2 Loss :  0.013054431907832623  Rel. Test L2 Loss :  0.016155827343463897  Test L2 Loss :  0.00010621896508382634  inv_L_scale:  [0.93, 0.935]
Epoch :  161  Time:  10.965  Rel. Train L2 Loss :  0.012918361097574233  Rel. Test L2 Loss :  0.017471205219626428  Test L2 Loss :  0.00011505235161166639  inv_L_scale:  [0.931, 0.935]
Epoch :  162  Time:  10.905  Rel. Train L2 Loss :  0.012442886486649513  Rel. Test L2 Loss :  0.016634908728301524  Test L2 Loss :  0.00011010897113010287  inv_L_scale:  [0.931, 0.935]
Epoch :  163  Time:  10.861  Rel. Train L2 Loss :  0.01337074975669384  Rel. Test L2 Loss :  0.016587971597909926  Test L2 Loss :  0.00010883681359700858  inv_L_scale:  [0.931, 0.935]
Epoch :  164  Time:  11.013  Rel. Train L2 Loss :  0.012615578092634678  Rel. Test L2 Loss :  0.017114280350506307  Test L2 Loss :  0.00011299905716441572  inv_L_scale:  [0.931, 0.935]
Epoch :  165  Time:  10.871  Rel. Train L2 Loss :  0.012835505969822407  Rel. Test L2 Loss :  0.017002496756613254  Test L2 Loss :  0.00011192500067409129  inv_L_scale:  [0.931, 0.935]
Epoch :  166  Time:  10.855  Rel. Train L2 Loss :  0.012733287900686265  Rel. Test L2 Loss :  0.016659774109721182  Test L2 Loss :  0.00011009889829438179  inv_L_scale:  [0.931, 0.935]
Epoch :  167  Time:  10.923  Rel. Train L2 Loss :  0.012199515201151371  Rel. Test L2 Loss :  0.016819143928587438  Test L2 Loss :  0.00011120495531940833  inv_L_scale:  [0.931, 0.935]
Epoch :  168  Time:  10.862  Rel. Train L2 Loss :  0.012515246473252773  Rel. Test L2 Loss :  0.01874740831553936  Test L2 Loss :  0.00012483404192607849  inv_L_scale:  [0.932, 0.935]
Epoch :  169  Time:  10.849  Rel. Train L2 Loss :  0.01248378424346447  Rel. Test L2 Loss :  0.017727155797183515  Test L2 Loss :  0.00011750206263968721  inv_L_scale:  [0.932, 0.935]
Epoch :  170  Time:  10.916  Rel. Train L2 Loss :  0.013302597858011722  Rel. Test L2 Loss :  0.01707770898938179  Test L2 Loss :  0.0001126901968382299  inv_L_scale:  [0.932, 0.936]
Epoch :  171  Time:  10.922  Rel. Train L2 Loss :  0.013306223779916764  Rel. Test L2 Loss :  0.018385004028677942  Test L2 Loss :  0.00012052131729433314  inv_L_scale:  [0.931, 0.935]
Epoch :  172  Time:  10.8  Rel. Train L2 Loss :  0.012694024361670017  Rel. Test L2 Loss :  0.01596775893121958  Test L2 Loss :  0.00010515863279579207  inv_L_scale:  [0.931, 0.935]
Epoch :  173  Time:  10.822  Rel. Train L2 Loss :  0.01288681823015213  Rel. Test L2 Loss :  0.01871840253472328  Test L2 Loss :  0.00012451578833861277  inv_L_scale:  [0.932, 0.935]
Epoch :  174  Time:  11.037  Rel. Train L2 Loss :  0.01191595908999443  Rel. Test L2 Loss :  0.016909818239510058  Test L2 Loss :  0.00011186400428414345  inv_L_scale:  [0.932, 0.935]
Epoch :  175  Time:  10.898  Rel. Train L2 Loss :  0.012187617719173432  Rel. Test L2 Loss :  0.017535331696271896  Test L2 Loss :  0.00011552364827366546  inv_L_scale:  [0.931, 0.935]
Epoch :  176  Time:  10.84  Rel. Train L2 Loss :  0.012011678919196129  Rel. Test L2 Loss :  0.017205794565379618  Test L2 Loss :  0.00011332374095218256  inv_L_scale:  [0.932, 0.935]
Epoch :  177  Time:  10.912  Rel. Train L2 Loss :  0.012256089985370635  Rel. Test L2 Loss :  0.015762809962034226  Test L2 Loss :  0.00010367297218181193  inv_L_scale:  [0.932, 0.935]
Epoch :  178  Time:  10.902  Rel. Train L2 Loss :  0.012258147440850735  Rel. Test L2 Loss :  0.017475721314549445  Test L2 Loss :  0.00011568866961169988  inv_L_scale:  [0.932, 0.935]
Epoch :  179  Time:  10.945  Rel. Train L2 Loss :  0.012683264896273612  Rel. Test L2 Loss :  0.019223842173814773  Test L2 Loss :  0.00012620959198102354  inv_L_scale:  [0.932, 0.935]
Epoch :  180  Time:  10.953  Rel. Train L2 Loss :  0.012223350532352925  Rel. Test L2 Loss :  0.016693301387131213  Test L2 Loss :  0.00011031349160475656  inv_L_scale:  [0.932, 0.935]
Epoch :  181  Time:  10.835  Rel. Train L2 Loss :  0.012587478421628474  Rel. Test L2 Loss :  0.016907348483800887  Test L2 Loss :  0.00011200732697034255  inv_L_scale:  [0.932, 0.935]
Epoch :  182  Time:  10.903  Rel. Train L2 Loss :  0.012243339546024799  Rel. Test L2 Loss :  0.016163784489035608  Test L2 Loss :  0.00010654128680471332  inv_L_scale:  [0.931, 0.935]
Epoch :  183  Time:  11.03  Rel. Train L2 Loss :  0.011894343808293342  Rel. Test L2 Loss :  0.016325872913002967  Test L2 Loss :  0.00010805348050780594  inv_L_scale:  [0.932, 0.935]
Epoch :  184  Time:  10.952  Rel. Train L2 Loss :  0.011997552879154683  Rel. Test L2 Loss :  0.016171194054186343  Test L2 Loss :  0.00010647680755937472  inv_L_scale:  [0.931, 0.935]
Epoch :  185  Time:  10.778  Rel. Train L2 Loss :  0.011968272522091865  Rel. Test L2 Loss :  0.01600860957056284  Test L2 Loss :  0.0001054964927607216  inv_L_scale:  [0.932, 0.936]
Epoch :  186  Time:  10.892  Rel. Train L2 Loss :  0.011991516649723054  Rel. Test L2 Loss :  0.017239632047712804  Test L2 Loss :  0.00011466087598819286  inv_L_scale:  [0.932, 0.935]
Epoch :  187  Time:  10.916  Rel. Train L2 Loss :  0.012231628634035588  Rel. Test L2 Loss :  0.016610374376177788  Test L2 Loss :  0.00010989114263793454  inv_L_scale:  [0.932, 0.936]
Epoch :  188  Time:  10.938  Rel. Train L2 Loss :  0.012227166879922152  Rel. Test L2 Loss :  0.016753402575850486  Test L2 Loss :  0.00011068618798162788  inv_L_scale:  [0.932, 0.935]
Epoch :  189  Time:  10.978  Rel. Train L2 Loss :  0.01182211435586214  Rel. Test L2 Loss :  0.01598081611096859  Test L2 Loss :  0.00010522237222176046  inv_L_scale:  [0.931, 0.935]
Epoch :  190  Time:  10.971  Rel. Train L2 Loss :  0.012728245712816715  Rel. Test L2 Loss :  0.016502701379358768  Test L2 Loss :  0.00010904551803832874  inv_L_scale:  [0.932, 0.936]
Epoch :  191  Time:  11.012  Rel. Train L2 Loss :  0.012515236757695674  Rel. Test L2 Loss :  0.01906050905585289  Test L2 Loss :  0.00012688545335549862  inv_L_scale:  [0.932, 0.936]
Epoch :  192  Time:  10.829  Rel. Train L2 Loss :  0.01226150082051754  Rel. Test L2 Loss :  0.016905323043465613  Test L2 Loss :  0.00011088389990618453  inv_L_scale:  [0.931, 0.935]
Epoch :  193  Time:  10.907  Rel. Train L2 Loss :  0.011831669017672538  Rel. Test L2 Loss :  0.01600720688700676  Test L2 Loss :  0.00010599203233141452  inv_L_scale:  [0.932, 0.935]
Epoch :  194  Time:  10.888  Rel. Train L2 Loss :  0.011854737646877766  Rel. Test L2 Loss :  0.016212284639477728  Test L2 Loss :  0.00010663376626325771  inv_L_scale:  [0.931, 0.935]
Epoch :  195  Time:  10.969  Rel. Train L2 Loss :  0.011679752394557  Rel. Test L2 Loss :  0.01701615609228611  Test L2 Loss :  0.00011272106581600383  inv_L_scale:  [0.932, 0.935]
Epoch :  196  Time:  10.9  Rel. Train L2 Loss :  0.010829581461846828  Rel. Test L2 Loss :  0.016874187588691712  Test L2 Loss :  0.00011160761612700299  inv_L_scale:  [0.932, 0.935]
Epoch :  197  Time:  10.869  Rel. Train L2 Loss :  0.011945795878767967  Rel. Test L2 Loss :  0.01596791833639145  Test L2 Loss :  0.00010521566669922321  inv_L_scale:  [0.932, 0.935]
Epoch :  198  Time:  10.869  Rel. Train L2 Loss :  0.011443510375916958  Rel. Test L2 Loss :  0.016576730348169803  Test L2 Loss :  0.00010861828195629641  inv_L_scale:  [0.931, 0.935]
Epoch :  199  Time:  10.756  Rel. Train L2 Loss :  0.01101400700956583  Rel. Test L2 Loss :  0.016073576509952545  Test L2 Loss :  0.00010590064077405259  inv_L_scale:  [0.932, 0.935]
Epoch :  200  Time:  10.687  Rel. Train L2 Loss :  0.012408619306981564  Rel. Test L2 Loss :  0.01748576555401087  Test L2 Loss :  0.00011589058121899142  inv_L_scale:  [0.932, 0.935]
Epoch :  201  Time:  10.724  Rel. Train L2 Loss :  0.011913957431912422  Rel. Test L2 Loss :  0.018376320637762546  Test L2 Loss :  0.00012053048179950565  inv_L_scale:  [0.931, 0.935]
Epoch :  202  Time:  10.762  Rel. Train L2 Loss :  0.011779431827366352  Rel. Test L2 Loss :  0.01600623071193695  Test L2 Loss :  0.00010604498500470072  inv_L_scale:  [0.932, 0.935]
Epoch :  203  Time:  10.794  Rel. Train L2 Loss :  0.011832772120833397  Rel. Test L2 Loss :  0.017961838506162165  Test L2 Loss :  0.00011785003036493435  inv_L_scale:  [0.933, 0.935]
Epoch :  204  Time:  10.972  Rel. Train L2 Loss :  0.011815755002200603  Rel. Test L2 Loss :  0.016068637631833554  Test L2 Loss :  0.000106138558476232  inv_L_scale:  [0.932, 0.935]
Epoch :  205  Time:  11.008  Rel. Train L2 Loss :  0.011497925266623497  Rel. Test L2 Loss :  0.018707774132490158  Test L2 Loss :  0.00012417099467711522  inv_L_scale:  [0.932, 0.935]
Epoch :  206  Time:  10.931  Rel. Train L2 Loss :  0.011532756328582763  Rel. Test L2 Loss :  0.017067237980663775  Test L2 Loss :  0.00011205411778064444  inv_L_scale:  [0.932, 0.935]
Epoch :  207  Time:  10.961  Rel. Train L2 Loss :  0.0116481506600976  Rel. Test L2 Loss :  0.016092377454042434  Test L2 Loss :  0.0001062456049839966  inv_L_scale:  [0.931, 0.935]
Epoch :  208  Time:  10.96  Rel. Train L2 Loss :  0.01164280390739441  Rel. Test L2 Loss :  0.01823593184351921  Test L2 Loss :  0.00011939767573494464  inv_L_scale:  [0.931, 0.935]
Epoch :  209  Time:  11.126  Rel. Train L2 Loss :  0.011451323337852955  Rel. Test L2 Loss :  0.018255952410399914  Test L2 Loss :  0.00012130136339692399  inv_L_scale:  [0.932, 0.935]
Epoch :  210  Time:  10.846  Rel. Train L2 Loss :  0.011252501890063285  Rel. Test L2 Loss :  0.018256831392645837  Test L2 Loss :  0.0001218699911260046  inv_L_scale:  [0.932, 0.935]
Epoch :  211  Time:  10.953  Rel. Train L2 Loss :  0.011063840828835964  Rel. Test L2 Loss :  0.015977164469659327  Test L2 Loss :  0.00010519220668356866  inv_L_scale:  [0.932, 0.934]
Epoch :  212  Time:  10.973  Rel. Train L2 Loss :  0.012265478402376176  Rel. Test L2 Loss :  0.016990684866905213  Test L2 Loss :  0.00011121483694296331  inv_L_scale:  [0.932, 0.935]
Epoch :  213  Time:  10.845  Rel. Train L2 Loss :  0.012450125478208065  Rel. Test L2 Loss :  0.016103484816849232  Test L2 Loss :  0.00010568599565885961  inv_L_scale:  [0.932, 0.935]
Epoch :  214  Time:  10.883  Rel. Train L2 Loss :  0.012575713507831097  Rel. Test L2 Loss :  0.017067991606891156  Test L2 Loss :  0.0001129420948564075  inv_L_scale:  [0.932, 0.935]
Epoch :  215  Time:  10.903  Rel. Train L2 Loss :  0.012439236983656883  Rel. Test L2 Loss :  0.01581519711762667  Test L2 Loss :  0.00010429120884509757  inv_L_scale:  [0.932, 0.935]
Epoch :  216  Time:  10.815  Rel. Train L2 Loss :  0.011322773426771165  Rel. Test L2 Loss :  0.01619094293564558  Test L2 Loss :  0.00010701104038162157  inv_L_scale:  [0.932, 0.934]
Epoch :  217  Time:  10.874  Rel. Train L2 Loss :  0.011145200237631797  Rel. Test L2 Loss :  0.01705276120454073  Test L2 Loss :  0.000112219491857104  inv_L_scale:  [0.932, 0.935]
Epoch :  218  Time:  10.778  Rel. Train L2 Loss :  0.011347219057381152  Rel. Test L2 Loss :  0.015374042652547359  Test L2 Loss :  0.00010146879765670746  inv_L_scale:  [0.932, 0.935]
Epoch :  219  Time:  10.921  Rel. Train L2 Loss :  0.011204058952629567  Rel. Test L2 Loss :  0.015890872627496718  Test L2 Loss :  0.00010459416458616033  inv_L_scale:  [0.932, 0.935]
Epoch :  220  Time:  10.921  Rel. Train L2 Loss :  0.010738663695752621  Rel. Test L2 Loss :  0.01592173654586077  Test L2 Loss :  0.00010510125779546798  inv_L_scale:  [0.932, 0.935]
Epoch :  221  Time:  10.984  Rel. Train L2 Loss :  0.010995617590844631  Rel. Test L2 Loss :  0.015171403959393502  Test L2 Loss :  9.983012045267969e-05  inv_L_scale:  [0.932, 0.935]
Epoch :  222  Time:  10.871  Rel. Train L2 Loss :  0.010201277054846287  Rel. Test L2 Loss :  0.015490723550319671  Test L2 Loss :  0.00010207470157183707  inv_L_scale:  [0.932, 0.935]
Epoch :  223  Time:  10.869  Rel. Train L2 Loss :  0.010822919003665447  Rel. Test L2 Loss :  0.01637110933661461  Test L2 Loss :  0.00010816431458806619  inv_L_scale:  [0.932, 0.935]
Epoch :  224  Time:  10.914  Rel. Train L2 Loss :  0.011471411921083927  Rel. Test L2 Loss :  0.0174781196936965  Test L2 Loss :  0.00011632465466391296  inv_L_scale:  [0.933, 0.935]
Epoch :  225  Time:  10.862  Rel. Train L2 Loss :  0.010760265693068505  Rel. Test L2 Loss :  0.01731188517063856  Test L2 Loss :  0.00011347167193889618  inv_L_scale:  [0.932, 0.935]
Epoch :  226  Time:  10.884  Rel. Train L2 Loss :  0.011501993201673031  Rel. Test L2 Loss :  0.01802457429468632  Test L2 Loss :  0.00011777042993344367  inv_L_scale:  [0.932, 0.934]
Epoch :  227  Time:  10.878  Rel. Train L2 Loss :  0.011197539038956166  Rel. Test L2 Loss :  0.02007858730852604  Test L2 Loss :  0.00013433087849989533  inv_L_scale:  [0.933, 0.935]
Epoch :  228  Time:  10.953  Rel. Train L2 Loss :  0.011567494951188565  Rel. Test L2 Loss :  0.015805651172995566  Test L2 Loss :  0.00010432555311126634  inv_L_scale:  [0.932, 0.935]
Epoch :  229  Time:  10.958  Rel. Train L2 Loss :  0.011422896794974803  Rel. Test L2 Loss :  0.015885149985551836  Test L2 Loss :  0.00010442838596645743  inv_L_scale:  [0.932, 0.935]
Epoch :  230  Time:  10.823  Rel. Train L2 Loss :  0.011065484628081322  Rel. Test L2 Loss :  0.01573120079934597  Test L2 Loss :  0.00010334882215829567  inv_L_scale:  [0.933, 0.935]
Epoch :  231  Time:  10.938  Rel. Train L2 Loss :  0.01056256676837802  Rel. Test L2 Loss :  0.016929896883666515  Test L2 Loss :  0.00011146120901685208  inv_L_scale:  [0.932, 0.935]
Epoch :  232  Time:  10.822  Rel. Train L2 Loss :  0.01097809112071991  Rel. Test L2 Loss :  0.01579830065369606  Test L2 Loss :  0.00010406249144580215  inv_L_scale:  [0.932, 0.934]
Epoch :  233  Time:  10.976  Rel. Train L2 Loss :  0.010694467410445213  Rel. Test L2 Loss :  0.015252111591398715  Test L2 Loss :  0.00010054822050733491  inv_L_scale:  [0.932, 0.935]
Epoch :  234  Time:  10.871  Rel. Train L2 Loss :  0.010406665965914726  Rel. Test L2 Loss :  0.015218285024166106  Test L2 Loss :  0.00010021969879744574  inv_L_scale:  [0.932, 0.934]
Epoch :  235  Time:  10.85  Rel. Train L2 Loss :  0.011007248111069203  Rel. Test L2 Loss :  0.015341230854392051  Test L2 Loss :  0.0001008128232206218  inv_L_scale:  [0.932, 0.935]
Epoch :  236  Time:  10.969  Rel. Train L2 Loss :  0.010463857896625996  Rel. Test L2 Loss :  0.015330373197793961  Test L2 Loss :  0.00010101594147272408  inv_L_scale:  [0.932, 0.935]
Epoch :  237  Time:  10.852  Rel. Train L2 Loss :  0.00991399012133479  Rel. Test L2 Loss :  0.016811584383249284  Test L2 Loss :  0.0001110413295100443  inv_L_scale:  [0.932, 0.935]
Epoch :  238  Time:  10.933  Rel. Train L2 Loss :  0.011265104219317435  Rel. Test L2 Loss :  0.01641583889722824  Test L2 Loss :  0.00010790403670398519  inv_L_scale:  [0.932, 0.935]
Epoch :  239  Time:  10.98  Rel. Train L2 Loss :  0.010939112208783627  Rel. Test L2 Loss :  0.016289952993392944  Test L2 Loss :  0.00010708476562285795  inv_L_scale:  [0.932, 0.935]
Epoch :  240  Time:  10.897  Rel. Train L2 Loss :  0.010667020209133625  Rel. Test L2 Loss :  0.01658476147800684  Test L2 Loss :  0.00010880752612138167  inv_L_scale:  [0.933, 0.935]
Epoch :  241  Time:  10.868  Rel. Train L2 Loss :  0.010259378470480442  Rel. Test L2 Loss :  0.014919703714549542  Test L2 Loss :  9.831507602939382e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  242  Time:  10.805  Rel. Train L2 Loss :  0.010370434194803237  Rel. Test L2 Loss :  0.01559148520231247  Test L2 Loss :  0.00010245472949463875  inv_L_scale:  [0.932, 0.934]
Epoch :  243  Time:  10.984  Rel. Train L2 Loss :  0.010182613112032413  Rel. Test L2 Loss :  0.017431178092956544  Test L2 Loss :  0.0001158936056890525  inv_L_scale:  [0.933, 0.935]
Epoch :  244  Time:  10.938  Rel. Train L2 Loss :  0.01037309344857931  Rel. Test L2 Loss :  0.016767677702009678  Test L2 Loss :  0.0001112204251694493  inv_L_scale:  [0.932, 0.935]
Epoch :  245  Time:  10.869  Rel. Train L2 Loss :  0.010012163326144219  Rel. Test L2 Loss :  0.0164194817468524  Test L2 Loss :  0.00010843102616490796  inv_L_scale:  [0.932, 0.934]
Epoch :  246  Time:  10.962  Rel. Train L2 Loss :  0.010410609185695648  Rel. Test L2 Loss :  0.015809836648404598  Test L2 Loss :  0.00010355749138398096  inv_L_scale:  [0.932, 0.935]
Epoch :  247  Time:  10.941  Rel. Train L2 Loss :  0.010840653695166111  Rel. Test L2 Loss :  0.015702159553766252  Test L2 Loss :  0.00010380663472460583  inv_L_scale:  [0.932, 0.935]
Epoch :  248  Time:  10.897  Rel. Train L2 Loss :  0.010692780710756779  Rel. Test L2 Loss :  0.015304637625813484  Test L2 Loss :  0.00010098538128659129  inv_L_scale:  [0.932, 0.934]
Epoch :  249  Time:  10.843  Rel. Train L2 Loss :  0.01058042699098587  Rel. Test L2 Loss :  0.017248324006795882  Test L2 Loss :  0.00011275744647718966  inv_L_scale:  [0.932, 0.934]
Epoch :  250  Time:  10.913  Rel. Train L2 Loss :  0.010829814553260804  Rel. Test L2 Loss :  0.015721829235553743  Test L2 Loss :  0.0001038075188989751  inv_L_scale:  [0.932, 0.935]
Epoch :  251  Time:  10.824  Rel. Train L2 Loss :  0.01002331455796957  Rel. Test L2 Loss :  0.015455232597887517  Test L2 Loss :  0.0001015300618018955  inv_L_scale:  [0.932, 0.934]
Epoch :  252  Time:  10.811  Rel. Train L2 Loss :  0.010752949140965938  Rel. Test L2 Loss :  0.016764205545186997  Test L2 Loss :  0.00011122550058644265  inv_L_scale:  [0.932, 0.935]
Epoch :  253  Time:  10.842  Rel. Train L2 Loss :  0.01065855460613966  Rel. Test L2 Loss :  0.01580815650522709  Test L2 Loss :  0.0001040952917537652  inv_L_scale:  [0.932, 0.935]
Epoch :  254  Time:  10.864  Rel. Train L2 Loss :  0.01047777034342289  Rel. Test L2 Loss :  0.01733584739267826  Test L2 Loss :  0.00011357272829627618  inv_L_scale:  [0.932, 0.934]
Epoch :  255  Time:  10.934  Rel. Train L2 Loss :  0.010169826716184617  Rel. Test L2 Loss :  0.016120003014802934  Test L2 Loss :  0.00010626014351146296  inv_L_scale:  [0.932, 0.934]
Epoch :  256  Time:  10.832  Rel. Train L2 Loss :  0.010208991132676601  Rel. Test L2 Loss :  0.01564771041274071  Test L2 Loss :  0.00010314985964214429  inv_L_scale:  [0.933, 0.935]
Epoch :  257  Time:  11.092  Rel. Train L2 Loss :  0.009829235903918742  Rel. Test L2 Loss :  0.015131700821220874  Test L2 Loss :  9.98772558523342e-05  inv_L_scale:  [0.933, 0.934]
Epoch :  258  Time:  10.872  Rel. Train L2 Loss :  0.010948474816977978  Rel. Test L2 Loss :  0.016225129663944245  Test L2 Loss :  0.0001073215898941271  inv_L_scale:  [0.932, 0.935]
Epoch :  259  Time:  10.869  Rel. Train L2 Loss :  0.010378570593893528  Rel. Test L2 Loss :  0.016584916189312935  Test L2 Loss :  0.00011013026960426942  inv_L_scale:  [0.932, 0.934]
Epoch :  260  Time:  10.894  Rel. Train L2 Loss :  0.010038988038897514  Rel. Test L2 Loss :  0.015501256920397282  Test L2 Loss :  0.00010237154667265713  inv_L_scale:  [0.932, 0.934]
Epoch :  261  Time:  10.876  Rel. Train L2 Loss :  0.009962821695953607  Rel. Test L2 Loss :  0.016129783503711224  Test L2 Loss :  0.00010688890208257362  inv_L_scale:  [0.932, 0.934]
Epoch :  262  Time:  10.941  Rel. Train L2 Loss :  0.009635520361363888  Rel. Test L2 Loss :  0.014836514331400395  Test L2 Loss :  9.784824593225494e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  263  Time:  10.883  Rel. Train L2 Loss :  0.009741110220551491  Rel. Test L2 Loss :  0.015867625437676905  Test L2 Loss :  0.00010478943149792031  inv_L_scale:  [0.932, 0.934]
Epoch :  264  Time:  10.96  Rel. Train L2 Loss :  0.01011581540852785  Rel. Test L2 Loss :  0.01627331204712391  Test L2 Loss :  0.00010754971619462594  inv_L_scale:  [0.932, 0.934]
Epoch :  265  Time:  10.969  Rel. Train L2 Loss :  0.010358773469924927  Rel. Test L2 Loss :  0.015385982058942317  Test L2 Loss :  0.00010085791087476537  inv_L_scale:  [0.932, 0.934]
Epoch :  266  Time:  10.954  Rel. Train L2 Loss :  0.010488378662616014  Rel. Test L2 Loss :  0.016167825050652028  Test L2 Loss :  0.00010691563249565661  inv_L_scale:  [0.933, 0.935]
Epoch :  267  Time:  10.869  Rel. Train L2 Loss :  0.011569087371230125  Rel. Test L2 Loss :  0.017004030160605907  Test L2 Loss :  0.00011226267903111874  inv_L_scale:  [0.932, 0.935]
Epoch :  268  Time:  10.941  Rel. Train L2 Loss :  0.011374491311609745  Rel. Test L2 Loss :  0.015702281221747397  Test L2 Loss :  0.00010344046546379104  inv_L_scale:  [0.932, 0.935]
Epoch :  269  Time:  10.918  Rel. Train L2 Loss :  0.010260134998708963  Rel. Test L2 Loss :  0.01529946267604828  Test L2 Loss :  0.00010073155659483746  inv_L_scale:  [0.932, 0.935]
Epoch :  270  Time:  10.952  Rel. Train L2 Loss :  0.009872989468276501  Rel. Test L2 Loss :  0.015466004833579064  Test L2 Loss :  0.00010233996988972649  inv_L_scale:  [0.932, 0.934]
Epoch :  271  Time:  10.933  Rel. Train L2 Loss :  0.00972441128641367  Rel. Test L2 Loss :  0.015910054221749306  Test L2 Loss :  0.0001052173730568029  inv_L_scale:  [0.932, 0.934]
Epoch :  272  Time:  10.908  Rel. Train L2 Loss :  0.009593049809336663  Rel. Test L2 Loss :  0.015814453326165677  Test L2 Loss :  0.00010407784604467452  inv_L_scale:  [0.932, 0.934]
Epoch :  273  Time:  10.886  Rel. Train L2 Loss :  0.010412642050534486  Rel. Test L2 Loss :  0.015189462192356586  Test L2 Loss :  0.00010034626087872312  inv_L_scale:  [0.932, 0.935]
Epoch :  274  Time:  10.963  Rel. Train L2 Loss :  0.0094122059866786  Rel. Test L2 Loss :  0.015631164908409118  Test L2 Loss :  0.00010271245409967377  inv_L_scale:  [0.932, 0.934]
Epoch :  275  Time:  10.848  Rel. Train L2 Loss :  0.00960601431503892  Rel. Test L2 Loss :  0.01724015824496746  Test L2 Loss :  0.00011423660558648407  inv_L_scale:  [0.933, 0.934]
Epoch :  276  Time:  10.876  Rel. Train L2 Loss :  0.010176918387413025  Rel. Test L2 Loss :  0.015158101432025433  Test L2 Loss :  9.990416408982128e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  277  Time:  10.926  Rel. Train L2 Loss :  0.009748347662389278  Rel. Test L2 Loss :  0.015137547105550766  Test L2 Loss :  9.941165102645754e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  278  Time:  10.915  Rel. Train L2 Loss :  0.00947151131927967  Rel. Test L2 Loss :  0.015897858180105685  Test L2 Loss :  0.00010475677583599464  inv_L_scale:  [0.932, 0.934]
Epoch :  279  Time:  10.865  Rel. Train L2 Loss :  0.010006862416863441  Rel. Test L2 Loss :  0.017855488136410712  Test L2 Loss :  0.00011796994600445032  inv_L_scale:  [0.933, 0.935]
Epoch :  280  Time:  10.892  Rel. Train L2 Loss :  0.00987018446996808  Rel. Test L2 Loss :  0.017741156443953514  Test L2 Loss :  0.00011799386731581763  inv_L_scale:  [0.932, 0.935]
Epoch :  281  Time:  10.892  Rel. Train L2 Loss :  0.009760051272809505  Rel. Test L2 Loss :  0.015739772021770477  Test L2 Loss :  0.00010330199467716738  inv_L_scale:  [0.932, 0.934]
Epoch :  282  Time:  10.872  Rel. Train L2 Loss :  0.01072484651952982  Rel. Test L2 Loss :  0.01718179017305374  Test L2 Loss :  0.0001131434296257794  inv_L_scale:  [0.932, 0.934]
Epoch :  283  Time:  10.9  Rel. Train L2 Loss :  0.010128127049654722  Rel. Test L2 Loss :  0.015701791234314443  Test L2 Loss :  0.00010412060568341985  inv_L_scale:  [0.932, 0.934]
Epoch :  284  Time:  10.914  Rel. Train L2 Loss :  0.009238278955221176  Rel. Test L2 Loss :  0.016191842891275884  Test L2 Loss :  0.00010698678699554875  inv_L_scale:  [0.932, 0.934]
Epoch :  285  Time:  10.927  Rel. Train L2 Loss :  0.009976314540952444  Rel. Test L2 Loss :  0.015140769705176353  Test L2 Loss :  9.987882687710225e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  286  Time:  10.864  Rel. Train L2 Loss :  0.009964481256902217  Rel. Test L2 Loss :  0.01692521546036005  Test L2 Loss :  0.00011093443084973842  inv_L_scale:  [0.932, 0.934]
Epoch :  287  Time:  10.905  Rel. Train L2 Loss :  0.009629682414233684  Rel. Test L2 Loss :  0.015631900019943714  Test L2 Loss :  0.00010314659797586501  inv_L_scale:  [0.932, 0.934]
Epoch :  288  Time:  11.033  Rel. Train L2 Loss :  0.010311853051185607  Rel. Test L2 Loss :  0.015161178149282932  Test L2 Loss :  9.963639691704884e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  289  Time:  10.927  Rel. Train L2 Loss :  0.01069642585888505  Rel. Test L2 Loss :  0.01559005543589592  Test L2 Loss :  0.00010291627753758803  inv_L_scale:  [0.932, 0.934]
Epoch :  290  Time:  10.862  Rel. Train L2 Loss :  0.01019621242210269  Rel. Test L2 Loss :  0.01533036194741726  Test L2 Loss :  0.00010073731216834858  inv_L_scale:  [0.932, 0.934]
Epoch :  291  Time:  10.884  Rel. Train L2 Loss :  0.009312194302678108  Rel. Test L2 Loss :  0.0158265695348382  Test L2 Loss :  0.00010380850842921063  inv_L_scale:  [0.933, 0.934]
Epoch :  292  Time:  10.86  Rel. Train L2 Loss :  0.009400634631514549  Rel. Test L2 Loss :  0.014992323331534862  Test L2 Loss :  9.862416365649551e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  293  Time:  10.981  Rel. Train L2 Loss :  0.009254767823964357  Rel. Test L2 Loss :  0.015130671411752701  Test L2 Loss :  9.969111793907359e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  294  Time:  10.943  Rel. Train L2 Loss :  0.009074907429516315  Rel. Test L2 Loss :  0.014853517860174179  Test L2 Loss :  9.777828090591356e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  295  Time:  10.915  Rel. Train L2 Loss :  0.0092052389010787  Rel. Test L2 Loss :  0.015302511900663375  Test L2 Loss :  0.00010064848087495193  inv_L_scale:  [0.932, 0.934]
Epoch :  296  Time:  10.951  Rel. Train L2 Loss :  0.009585849225521088  Rel. Test L2 Loss :  0.015843201987445356  Test L2 Loss :  0.00010429800720885397  inv_L_scale:  [0.932, 0.934]
Epoch :  297  Time:  10.928  Rel. Train L2 Loss :  0.009391948867589235  Rel. Test L2 Loss :  0.015394822657108308  Test L2 Loss :  0.00010101550695253536  inv_L_scale:  [0.932, 0.934]
Epoch :  298  Time:  10.896  Rel. Train L2 Loss :  0.009553884495049715  Rel. Test L2 Loss :  0.016489940918982027  Test L2 Loss :  0.00010952330252621323  inv_L_scale:  [0.932, 0.934]
Epoch :  299  Time:  10.856  Rel. Train L2 Loss :  0.009680433340370656  Rel. Test L2 Loss :  0.015168764367699624  Test L2 Loss :  0.00010010148776927963  inv_L_scale:  [0.932, 0.934]
Epoch :  300  Time:  10.922  Rel. Train L2 Loss :  0.010557254206389189  Rel. Test L2 Loss :  0.015619401708245278  Test L2 Loss :  0.00010323442606022582  inv_L_scale:  [0.932, 0.934]
Epoch :  301  Time:  10.972  Rel. Train L2 Loss :  0.009392914284020662  Rel. Test L2 Loss :  0.01750856641680002  Test L2 Loss :  0.00011660459480481223  inv_L_scale:  [0.932, 0.934]
Epoch :  302  Time:  10.851  Rel. Train L2 Loss :  0.009469327572733164  Rel. Test L2 Loss :  0.015368628390133381  Test L2 Loss :  0.00010105256224051118  inv_L_scale:  [0.932, 0.934]
Epoch :  303  Time:  10.967  Rel. Train L2 Loss :  0.009240184813737869  Rel. Test L2 Loss :  0.015143644735217095  Test L2 Loss :  9.988246369175612e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  304  Time:  10.94  Rel. Train L2 Loss :  0.008822506945580243  Rel. Test L2 Loss :  0.014841300211846828  Test L2 Loss :  9.790152485948055e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  305  Time:  10.993  Rel. Train L2 Loss :  0.008946460288017988  Rel. Test L2 Loss :  0.014865793883800507  Test L2 Loss :  9.792258642846719e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  306  Time:  10.917  Rel. Train L2 Loss :  0.009936589814722539  Rel. Test L2 Loss :  0.018479479923844338  Test L2 Loss :  0.00012123827415052801  inv_L_scale:  [0.932, 0.934]
Epoch :  307  Time:  10.882  Rel. Train L2 Loss :  0.009242040742188692  Rel. Test L2 Loss :  0.014893605485558509  Test L2 Loss :  9.816817939281464e-05  inv_L_scale:  [0.933, 0.934]
Epoch :  308  Time:  10.88  Rel. Train L2 Loss :  0.01081635219231248  Rel. Test L2 Loss :  0.015040974915027618  Test L2 Loss :  9.910690016113222e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  309  Time:  10.95  Rel. Train L2 Loss :  0.009780806638300419  Rel. Test L2 Loss :  0.015657178200781344  Test L2 Loss :  0.00010265666525810957  inv_L_scale:  [0.932, 0.934]
Epoch :  310  Time:  10.948  Rel. Train L2 Loss :  0.009150384362787008  Rel. Test L2 Loss :  0.01524199303239584  Test L2 Loss :  0.00010064034227980301  inv_L_scale:  [0.932, 0.934]
Epoch :  311  Time:  10.975  Rel. Train L2 Loss :  0.00931904000043869  Rel. Test L2 Loss :  0.015556375421583652  Test L2 Loss :  0.00010244027886074037  inv_L_scale:  [0.932, 0.933]
Epoch :  312  Time:  10.942  Rel. Train L2 Loss :  0.009186217118054629  Rel. Test L2 Loss :  0.015207495726644992  Test L2 Loss :  0.00010053672041976825  inv_L_scale:  [0.932, 0.934]
Epoch :  313  Time:  10.906  Rel. Train L2 Loss :  0.00913346129283309  Rel. Test L2 Loss :  0.01500250305980444  Test L2 Loss :  9.902084973873571e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  314  Time:  10.975  Rel. Train L2 Loss :  0.00893677282705903  Rel. Test L2 Loss :  0.015196389444172382  Test L2 Loss :  0.00010018846514867619  inv_L_scale:  [0.932, 0.934]
Epoch :  315  Time:  10.895  Rel. Train L2 Loss :  0.009500834323465825  Rel. Test L2 Loss :  0.015657956041395665  Test L2 Loss :  0.0001035082250018604  inv_L_scale:  [0.932, 0.934]
Epoch :  316  Time:  10.936  Rel. Train L2 Loss :  0.008977009356021881  Rel. Test L2 Loss :  0.014870003461837769  Test L2 Loss :  9.817559941438958e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  317  Time:  10.954  Rel. Train L2 Loss :  0.009310260344296694  Rel. Test L2 Loss :  0.016199099272489546  Test L2 Loss :  0.00010663243650924415  inv_L_scale:  [0.932, 0.934]
Epoch :  318  Time:  10.88  Rel. Train L2 Loss :  0.009159391704946757  Rel. Test L2 Loss :  0.01573807530105114  Test L2 Loss :  0.00010334028862416744  inv_L_scale:  [0.932, 0.934]
Epoch :  319  Time:  10.883  Rel. Train L2 Loss :  0.009348311468958854  Rel. Test L2 Loss :  0.01611074563115835  Test L2 Loss :  0.00010674588644178584  inv_L_scale:  [0.933, 0.934]
Epoch :  320  Time:  10.877  Rel. Train L2 Loss :  0.009623445454984903  Rel. Test L2 Loss :  0.015317229554057121  Test L2 Loss :  0.00010115321463672445  inv_L_scale:  [0.933, 0.934]
Epoch :  321  Time:  10.937  Rel. Train L2 Loss :  0.00929083949699998  Rel. Test L2 Loss :  0.015695393569767475  Test L2 Loss :  0.00010342950030462816  inv_L_scale:  [0.932, 0.933]
Epoch :  322  Time:  10.888  Rel. Train L2 Loss :  0.009005281917750836  Rel. Test L2 Loss :  0.01584915030747652  Test L2 Loss :  0.00010444620449561625  inv_L_scale:  [0.932, 0.933]
Epoch :  323  Time:  10.809  Rel. Train L2 Loss :  0.008664278239011764  Rel. Test L2 Loss :  0.016706310622394086  Test L2 Loss :  0.00011065554805099965  inv_L_scale:  [0.932, 0.934]
Epoch :  324  Time:  10.923  Rel. Train L2 Loss :  0.009811421439051627  Rel. Test L2 Loss :  0.015574218146502972  Test L2 Loss :  0.00010231041815131903  inv_L_scale:  [0.932, 0.934]
Epoch :  325  Time:  10.961  Rel. Train L2 Loss :  0.009054068814963102  Rel. Test L2 Loss :  0.015185001455247402  Test L2 Loss :  0.00010039939254056662  inv_L_scale:  [0.932, 0.934]
Epoch :  326  Time:  11.015  Rel. Train L2 Loss :  0.008893696270883083  Rel. Test L2 Loss :  0.014909009709954261  Test L2 Loss :  9.852610441157594e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  327  Time:  10.861  Rel. Train L2 Loss :  0.008459620680660009  Rel. Test L2 Loss :  0.01483618538826704  Test L2 Loss :  9.80598694877699e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  328  Time:  10.844  Rel. Train L2 Loss :  0.008776197075843812  Rel. Test L2 Loss :  0.015183609835803509  Test L2 Loss :  9.971230756491422e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  329  Time:  10.887  Rel. Train L2 Loss :  0.009136408757418394  Rel. Test L2 Loss :  0.015105462111532688  Test L2 Loss :  9.930693835485726e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  330  Time:  10.881  Rel. Train L2 Loss :  0.009303307626396418  Rel. Test L2 Loss :  0.014762667268514633  Test L2 Loss :  9.727981523610651e-05  inv_L_scale:  [0.932, 0.934]
Epoch :  331  Time:  10.885  Rel. Train L2 Loss :  0.009507933057844638  Rel. Test L2 Loss :  0.015412248596549034  Test L2 Loss :  0.00010153857845580206  inv_L_scale:  [0.932, 0.934]
Epoch :  332  Time:  10.894  Rel. Train L2 Loss :  0.009232346072793007  Rel. Test L2 Loss :  0.015142738670110702  Test L2 Loss :  0.00010000397887779399  inv_L_scale:  [0.932, 0.934]
Epoch :  333  Time:  10.854  Rel. Train L2 Loss :  0.008739401936531067  Rel. Test L2 Loss :  0.016021642982959747  Test L2 Loss :  0.0001059788346174173  inv_L_scale:  [0.932, 0.933]
Epoch :  334  Time:  10.851  Rel. Train L2 Loss :  0.009246083244681359  Rel. Test L2 Loss :  0.015560004115104675  Test L2 Loss :  0.0001030706349411048  inv_L_scale:  [0.932, 0.934]
Epoch :  335  Time:  10.947  Rel. Train L2 Loss :  0.00868274373561144  Rel. Test L2 Loss :  0.015266840681433677  Test L2 Loss :  0.00010040950874099508  inv_L_scale:  [0.932, 0.933]
Epoch :  336  Time:  10.876  Rel. Train L2 Loss :  0.00858319777995348  Rel. Test L2 Loss :  0.016467530205845834  Test L2 Loss :  0.00010944721492705866  inv_L_scale:  [0.932, 0.933]
Epoch :  337  Time:  10.904  Rel. Train L2 Loss :  0.008742816988378763  Rel. Test L2 Loss :  0.01579127263277769  Test L2 Loss :  0.0001048130207345821  inv_L_scale:  [0.932, 0.933]
Epoch :  338  Time:  10.909  Rel. Train L2 Loss :  0.00902902640774846  Rel. Test L2 Loss :  0.014946540258824826  Test L2 Loss :  9.826932015130296e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  339  Time:  10.863  Rel. Train L2 Loss :  0.008323098544031382  Rel. Test L2 Loss :  0.014719176217913627  Test L2 Loss :  9.703510178951546e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  340  Time:  10.861  Rel. Train L2 Loss :  0.008311108320951461  Rel. Test L2 Loss :  0.015373819805681706  Test L2 Loss :  0.0001012458693003282  inv_L_scale:  [0.932, 0.933]
Epoch :  341  Time:  10.855  Rel. Train L2 Loss :  0.009254496425390243  Rel. Test L2 Loss :  0.016078732162714004  Test L2 Loss :  0.0001058475871104747  inv_L_scale:  [0.932, 0.933]
Epoch :  342  Time:  10.925  Rel. Train L2 Loss :  0.008896441850811242  Rel. Test L2 Loss :  0.01509474441409111  Test L2 Loss :  9.928668558131904e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  343  Time:  10.993  Rel. Train L2 Loss :  0.008894356563687325  Rel. Test L2 Loss :  0.015751753002405167  Test L2 Loss :  0.0001034966780571267  inv_L_scale:  [0.932, 0.933]
Epoch :  344  Time:  10.929  Rel. Train L2 Loss :  0.008351297605782747  Rel. Test L2 Loss :  0.015580540671944618  Test L2 Loss :  0.0001030914206057787  inv_L_scale:  [0.932, 0.934]
Epoch :  345  Time:  10.885  Rel. Train L2 Loss :  0.008482353363186122  Rel. Test L2 Loss :  0.01578282192349434  Test L2 Loss :  0.0001047643122728914  inv_L_scale:  [0.932, 0.933]
Epoch :  346  Time:  10.87  Rel. Train L2 Loss :  0.008221007239073515  Rel. Test L2 Loss :  0.015556214191019536  Test L2 Loss :  0.00010291406622854993  inv_L_scale:  [0.932, 0.933]
Epoch :  347  Time:  10.779  Rel. Train L2 Loss :  0.008436838831752539  Rel. Test L2 Loss :  0.016048988588154317  Test L2 Loss :  0.00010563702468061819  inv_L_scale:  [0.932, 0.933]
Epoch :  348  Time:  10.893  Rel. Train L2 Loss :  0.008604682367295027  Rel. Test L2 Loss :  0.014950335212051868  Test L2 Loss :  9.864304796792566e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  349  Time:  10.84  Rel. Train L2 Loss :  0.008876490373164415  Rel. Test L2 Loss :  0.016695847250521183  Test L2 Loss :  0.00010989764501573518  inv_L_scale:  [0.931, 0.933]
Epoch :  350  Time:  10.865  Rel. Train L2 Loss :  0.008856123082339764  Rel. Test L2 Loss :  0.01484901674091816  Test L2 Loss :  9.824537235544995e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  351  Time:  10.867  Rel. Train L2 Loss :  0.00847229128703475  Rel. Test L2 Loss :  0.014818260371685028  Test L2 Loss :  9.764886344783008e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  352  Time:  10.915  Rel. Train L2 Loss :  0.008219375517219305  Rel. Test L2 Loss :  0.014819788299500942  Test L2 Loss :  9.796809608815238e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  353  Time:  10.904  Rel. Train L2 Loss :  0.007984317943453789  Rel. Test L2 Loss :  0.015039725452661514  Test L2 Loss :  9.895470080664381e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  354  Time:  10.878  Rel. Train L2 Loss :  0.008814956236630677  Rel. Test L2 Loss :  0.014949693083763123  Test L2 Loss :  9.83993211411871e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  355  Time:  10.932  Rel. Train L2 Loss :  0.008374585900455714  Rel. Test L2 Loss :  0.015100009851157664  Test L2 Loss :  9.959686227375641e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  356  Time:  10.897  Rel. Train L2 Loss :  0.008701655589044094  Rel. Test L2 Loss :  0.015037790909409522  Test L2 Loss :  9.89391832263209e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  357  Time:  10.995  Rel. Train L2 Loss :  0.008674849234521389  Rel. Test L2 Loss :  0.015146817788481712  Test L2 Loss :  0.00010008027369622141  inv_L_scale:  [0.931, 0.933]
Epoch :  358  Time:  10.968  Rel. Train L2 Loss :  0.008189899824559688  Rel. Test L2 Loss :  0.01563596673309803  Test L2 Loss :  0.00010266323282849043  inv_L_scale:  [0.931, 0.933]
Epoch :  359  Time:  10.946  Rel. Train L2 Loss :  0.008384786155074835  Rel. Test L2 Loss :  0.015133634433150292  Test L2 Loss :  9.947812592145055e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  360  Time:  10.893  Rel. Train L2 Loss :  0.008441371865570545  Rel. Test L2 Loss :  0.015246981345117093  Test L2 Loss :  0.00010087861854117364  inv_L_scale:  [0.932, 0.933]
Epoch :  361  Time:  10.849  Rel. Train L2 Loss :  0.008153374787420035  Rel. Test L2 Loss :  0.014732612520456314  Test L2 Loss :  9.705948934424669e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  362  Time:  10.802  Rel. Train L2 Loss :  0.008362570624798536  Rel. Test L2 Loss :  0.01670204970985651  Test L2 Loss :  0.00011104491917649284  inv_L_scale:  [0.932, 0.933]
Epoch :  363  Time:  10.892  Rel. Train L2 Loss :  0.008134420901536942  Rel. Test L2 Loss :  0.015791800171136856  Test L2 Loss :  0.00010442977218190208  inv_L_scale:  [0.932, 0.933]
Epoch :  364  Time:  10.868  Rel. Train L2 Loss :  0.00819446662068367  Rel. Test L2 Loss :  0.014787363857030868  Test L2 Loss :  9.73501440603286e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  365  Time:  10.837  Rel. Train L2 Loss :  0.008552199825644494  Rel. Test L2 Loss :  0.014826527424156665  Test L2 Loss :  9.795027115615085e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  366  Time:  10.895  Rel. Train L2 Loss :  0.008727323163300754  Rel. Test L2 Loss :  0.015788746289908884  Test L2 Loss :  0.00010498262854525819  inv_L_scale:  [0.932, 0.933]
Epoch :  367  Time:  10.899  Rel. Train L2 Loss :  0.007921162426471711  Rel. Test L2 Loss :  0.014901482351124287  Test L2 Loss :  9.843807230936364e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  368  Time:  10.904  Rel. Train L2 Loss :  0.008371442027390003  Rel. Test L2 Loss :  0.014977995343506335  Test L2 Loss :  9.844775922829285e-05  inv_L_scale:  [0.932, 0.933]
Epoch :  369  Time:  10.621  Rel. Train L2 Loss :  0.008492330592125653  Rel. Test L2 Loss :  0.015737467221915723  Test L2 Loss :  0.00010422306368127466  inv_L_scale:  [0.931, 0.933]
Epoch :  370  Time:  17.713  Rel. Train L2 Loss :  0.007923404548317194  Rel. Test L2 Loss :  0.015298269987106324  Test L2 Loss :  0.00010071059863548725  inv_L_scale:  [0.932, 0.933]
Epoch :  371  Time:  32.083  Rel. Train L2 Loss :  0.008559143096208572  Rel. Test L2 Loss :  0.014829089120030402  Test L2 Loss :  9.762943634996191e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  372  Time:  32.925  Rel. Train L2 Loss :  0.008533557072281837  Rel. Test L2 Loss :  0.015081294253468514  Test L2 Loss :  9.938890842022375e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  373  Time:  32.964  Rel. Train L2 Loss :  0.008089464146643877  Rel. Test L2 Loss :  0.014955297932028771  Test L2 Loss :  9.85132236382924e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  374  Time:  33.02  Rel. Train L2 Loss :  0.008209248833358287  Rel. Test L2 Loss :  0.01560628529638052  Test L2 Loss :  0.00010279026464559138  inv_L_scale:  [0.931, 0.933]
Epoch :  375  Time:  33.141  Rel. Train L2 Loss :  0.008178786300122738  Rel. Test L2 Loss :  0.014932782761752606  Test L2 Loss :  9.867774439044297e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  376  Time:  33.029  Rel. Train L2 Loss :  0.008227301344275474  Rel. Test L2 Loss :  0.014864277802407741  Test L2 Loss :  9.794108191272244e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  377  Time:  33.057  Rel. Train L2 Loss :  0.008089262925088405  Rel. Test L2 Loss :  0.01533481802791357  Test L2 Loss :  0.00010134047974133864  inv_L_scale:  [0.931, 0.933]
Epoch :  378  Time:  33.071  Rel. Train L2 Loss :  0.007904991384595633  Rel. Test L2 Loss :  0.014663772024214268  Test L2 Loss :  9.669527702499181e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  379  Time:  36.289  Rel. Train L2 Loss :  0.00818243258818984  Rel. Test L2 Loss :  0.015022105388343334  Test L2 Loss :  9.932345157722011e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  380  Time:  33.073  Rel. Train L2 Loss :  0.0081103287152946  Rel. Test L2 Loss :  0.01485525980591774  Test L2 Loss :  9.779647109098733e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  381  Time:  35.136  Rel. Train L2 Loss :  0.008158022575080394  Rel. Test L2 Loss :  0.015294945687055589  Test L2 Loss :  0.00010113176715094596  inv_L_scale:  [0.932, 0.933]
Epoch :  382  Time:  33.127  Rel. Train L2 Loss :  0.007897204853594303  Rel. Test L2 Loss :  0.015278780534863472  Test L2 Loss :  0.0001003073828178458  inv_L_scale:  [0.931, 0.933]
Epoch :  383  Time:  4396.481  Rel. Train L2 Loss :  0.0074949532896280285  Rel. Test L2 Loss :  0.015388491451740266  Test L2 Loss :  0.00010180507641052827  inv_L_scale:  [0.931, 0.933]
Epoch :  384  Time:  11.012  Rel. Train L2 Loss :  0.008273741327226162  Rel. Test L2 Loss :  0.015017601288855075  Test L2 Loss :  9.884889179375023e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  385  Time:  9.319  Rel. Train L2 Loss :  0.008059379454702139  Rel. Test L2 Loss :  0.015773190930485726  Test L2 Loss :  0.0001035152084659785  inv_L_scale:  [0.931, 0.932]
Epoch :  386  Time:  9.16  Rel. Train L2 Loss :  0.00835944926738739  Rel. Test L2 Loss :  0.015093698278069495  Test L2 Loss :  9.954541106708347e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  387  Time:  8.899  Rel. Train L2 Loss :  0.008267406813800334  Rel. Test L2 Loss :  0.015807593576610088  Test L2 Loss :  0.0001043191086500883  inv_L_scale:  [0.931, 0.932]
Epoch :  388  Time:  8.648  Rel. Train L2 Loss :  0.008263122107833623  Rel. Test L2 Loss :  0.015506626553833485  Test L2 Loss :  0.0001017074813717045  inv_L_scale:  [0.931, 0.932]
Epoch :  389  Time:  8.656  Rel. Train L2 Loss :  0.007923113517463207  Rel. Test L2 Loss :  0.015036680288612842  Test L2 Loss :  9.927277773385867e-05  inv_L_scale:  [0.932, 0.932]
Epoch :  390  Time:  8.6  Rel. Train L2 Loss :  0.008620187755674124  Rel. Test L2 Loss :  0.015557856783270835  Test L2 Loss :  0.00010284057236276567  inv_L_scale:  [0.931, 0.933]
Epoch :  391  Time:  8.736  Rel. Train L2 Loss :  0.007992209155112505  Rel. Test L2 Loss :  0.015131267420947551  Test L2 Loss :  9.945798112312332e-05  inv_L_scale:  [0.931, 0.932]
Epoch :  392  Time:  8.564  Rel. Train L2 Loss :  0.00829901620745659  Rel. Test L2 Loss :  0.015188645608723163  Test L2 Loss :  0.00010002040857216343  inv_L_scale:  [0.931, 0.932]
Epoch :  393  Time:  8.511  Rel. Train L2 Loss :  0.007559917725622654  Rel. Test L2 Loss :  0.015004657022655011  Test L2 Loss :  9.865903353784234e-05  inv_L_scale:  [0.931, 0.932]
Epoch :  394  Time:  8.457  Rel. Train L2 Loss :  0.008191721059381962  Rel. Test L2 Loss :  0.014654093235731126  Test L2 Loss :  9.662925353040919e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  395  Time:  8.432  Rel. Train L2 Loss :  0.007863738916814328  Rel. Test L2 Loss :  0.014571525938808918  Test L2 Loss :  9.599748300388455e-05  inv_L_scale:  [0.931, 0.933]
Epoch :  396  Time:  8.522  Rel. Train L2 Loss :  0.008236994788050652  Rel. Test L2 Loss :  0.014803993105888367  Test L2 Loss :  9.773168276296928e-05  inv_L_scale:  [0.931, 0.932]



self.centor = nn.Parameter(100*torch.randn(in_channels, ndims, nmeasures, dtype=torch.float))
self.inv_sigma = nn.Parameter(0.1*torch.rand(in_channels, ndims, nmeasures, dtype=torch.float))

Casting to tensor
In PCNO_train, ndims =  2
Epoch :  0  Time:  14.491  Rel. Train L2 Loss :  0.30422430884838103  Rel. Test L2 Loss :  0.09776135236024856  Test L2 Loss :  0.0006472198781557381  inv_L_scale:  [0.931, 0.944]
Epoch :  1  Time:  8.451  Rel. Train L2 Loss :  0.07469244113564491  Rel. Test L2 Loss :  0.0581143556535244  Test L2 Loss :  0.0003819077054504305  inv_L_scale:  [0.923, 0.936]
Epoch :  2  Time:  8.37  Rel. Train L2 Loss :  0.05669227772951126  Rel. Test L2 Loss :  0.05287615165114403  Test L2 Loss :  0.0003526951454114169  inv_L_scale:  [0.919, 0.932]
Epoch :  3  Time:  14.374  Rel. Train L2 Loss :  0.05111903101205826  Rel. Test L2 Loss :  0.04061197221279144  Test L2 Loss :  0.00026664381148293613  inv_L_scale:  [0.917, 0.928]
Epoch :  4  Time:  8.742  Rel. Train L2 Loss :  0.03664859935641289  Rel. Test L2 Loss :  0.0350543811917305  Test L2 Loss :  0.00023046777467243375  inv_L_scale:  [0.913, 0.925]
Epoch :  5  Time:  8.517  Rel. Train L2 Loss :  0.03147286355495453  Rel. Test L2 Loss :  0.033744438663125036  Test L2 Loss :  0.00022340598807204514  inv_L_scale:  [0.911, 0.922]
Epoch :  6  Time:  14.346  Rel. Train L2 Loss :  0.0294216643422842  Rel. Test L2 Loss :  0.03622980803251266  Test L2 Loss :  0.00024222798005212097  inv_L_scale:  [0.909, 0.921]
Epoch :  7  Time:  8.74  Rel. Train L2 Loss :  0.026466102987527846  Rel. Test L2 Loss :  0.027254333198070527  Test L2 Loss :  0.0001795804442372173  inv_L_scale:  [0.908, 0.919]
Epoch :  8  Time:  8.911  Rel. Train L2 Loss :  0.023605797290802002  Rel. Test L2 Loss :  0.0295562332123518  Test L2 Loss :  0.00019567464769352228  inv_L_scale:  [0.907, 0.918]
Epoch :  9  Time:  15.17  Rel. Train L2 Loss :  0.02350470392405987  Rel. Test L2 Loss :  0.02569230765104294  Test L2 Loss :  0.00016922653769142925  inv_L_scale:  [0.906, 0.917]
Epoch :  10  Time:  8.762  Rel. Train L2 Loss :  0.022456114634871484  Rel. Test L2 Loss :  0.027671012431383132  Test L2 Loss :  0.00018126451352145522  inv_L_scale:  [0.906, 0.917]
Epoch :  11  Time:  8.678  Rel. Train L2 Loss :  0.021987401500344277  Rel. Test L2 Loss :  0.028921588510274886  Test L2 Loss :  0.00019273792160674928  inv_L_scale:  [0.906, 0.916]
Epoch :  12  Time:  14.477  Rel. Train L2 Loss :  0.02424308618158102  Rel. Test L2 Loss :  0.02376637704670429  Test L2 Loss :  0.0001575142570072785  inv_L_scale:  [0.905, 0.916]
Epoch :  13  Time:  8.753  Rel. Train L2 Loss :  0.0191875072196126  Rel. Test L2 Loss :  0.024779184833168984  Test L2 Loss :  0.00016234977112617344  inv_L_scale:  [0.905, 0.915]
Epoch :  14  Time:  8.828  Rel. Train L2 Loss :  0.01780716086179018  Rel. Test L2 Loss :  0.022186220288276673  Test L2 Loss :  0.00014666735281934962  inv_L_scale:  [0.904, 0.915]
Epoch :  15  Time:  14.624  Rel. Train L2 Loss :  0.018002614818513393  Rel. Test L2 Loss :  0.022423581406474113  Test L2 Loss :  0.00014840767893474548  inv_L_scale:  [0.904, 0.914]
Epoch :  16  Time:  8.787  Rel. Train L2 Loss :  0.017744214579463007  Rel. Test L2 Loss :  0.023604627028107644  Test L2 Loss :  0.00015494421881157906  inv_L_scale:  [0.904, 0.914]
Epoch :  17  Time:  8.681  Rel. Train L2 Loss :  0.016987592868506908  Rel. Test L2 Loss :  0.023513148128986357  Test L2 Loss :  0.00015388612169772386  inv_L_scale:  [0.904, 0.914]
Epoch :  18  Time:  14.506  Rel. Train L2 Loss :  0.018331097081303597  Rel. Test L2 Loss :  0.02110533505678177  Test L2 Loss :  0.00013927738036727532  inv_L_scale:  [0.904, 0.914]
Epoch :  19  Time:  8.715  Rel. Train L2 Loss :  0.016728049919009207  Rel. Test L2 Loss :  0.020272974744439124  Test L2 Loss :  0.00013393537898082286  inv_L_scale:  [0.903, 0.913]
Epoch :  20  Time:  8.776  Rel. Train L2 Loss :  0.015467468209564687  Rel. Test L2 Loss :  0.02021374337375164  Test L2 Loss :  0.00013299912709044292  inv_L_scale:  [0.903, 0.913]
Epoch :  21  Time:  14.718  Rel. Train L2 Loss :  0.01484532842040062  Rel. Test L2 Loss :  0.02160754881799221  Test L2 Loss :  0.00014208738342858852  inv_L_scale:  [0.902, 0.912]
Epoch :  22  Time:  8.782  Rel. Train L2 Loss :  0.015688999578356743  Rel. Test L2 Loss :  0.01916410557925701  Test L2 Loss :  0.00012605468451511116  inv_L_scale:  [0.903, 0.912]
Epoch :  23  Time:  8.717  Rel. Train L2 Loss :  0.015034282617270946  Rel. Test L2 Loss :  0.02084547571837902  Test L2 Loss :  0.00013756888889474795  inv_L_scale:  [0.902, 0.913]
Epoch :  24  Time:  14.86  Rel. Train L2 Loss :  0.014846553556621074  Rel. Test L2 Loss :  0.021590559855103494  Test L2 Loss :  0.00014097020437475293  inv_L_scale:  [0.902, 0.913]
Epoch :  25  Time:  8.799  Rel. Train L2 Loss :  0.015423598855733871  Rel. Test L2 Loss :  0.019321040362119676  Test L2 Loss :  0.00012743909348500893  inv_L_scale:  [0.902, 0.912]
Epoch :  26  Time:  8.697  Rel. Train L2 Loss :  0.01532899159938097  Rel. Test L2 Loss :  0.020898368433117865  Test L2 Loss :  0.0001378388507873751  inv_L_scale:  [0.902, 0.912]
Epoch :  27  Time:  14.695  Rel. Train L2 Loss :  0.01535415904968977  Rel. Test L2 Loss :  0.02404332920908928  Test L2 Loss :  0.00015644950908608735  inv_L_scale:  [0.902, 0.912]
Epoch :  28  Time:  8.696  Rel. Train L2 Loss :  0.016256177462637425  Rel. Test L2 Loss :  0.01988777667284012  Test L2 Loss :  0.00013103418808896094  inv_L_scale:  [0.902, 0.912]
Epoch :  29  Time:  8.808  Rel. Train L2 Loss :  0.013810447312891484  Rel. Test L2 Loss :  0.018547580540180207  Test L2 Loss :  0.0001219737235805951  inv_L_scale:  [0.902, 0.912]
Epoch :  30  Time:  14.89  Rel. Train L2 Loss :  0.013744998529553413  Rel. Test L2 Loss :  0.020383141711354257  Test L2 Loss :  0.00013541465043090284  inv_L_scale:  [0.902, 0.912]
Epoch :  31  Time:  8.889  Rel. Train L2 Loss :  0.014470852322876454  Rel. Test L2 Loss :  0.01869558736681938  Test L2 Loss :  0.00012330786645179615  inv_L_scale:  [0.901, 0.912]
Epoch :  32  Time:  8.831  Rel. Train L2 Loss :  0.013984081260859966  Rel. Test L2 Loss :  0.019504931941628455  Test L2 Loss :  0.00012846293742768466  inv_L_scale:  [0.901, 0.911]
Epoch :  33  Time:  14.868  Rel. Train L2 Loss :  0.014305674783885478  Rel. Test L2 Loss :  0.018867151364684105  Test L2 Loss :  0.0001237463019788265  inv_L_scale:  [0.901, 0.911]
Epoch :  34  Time:  8.78  Rel. Train L2 Loss :  0.012766099251806736  Rel. Test L2 Loss :  0.017700858041644098  Test L2 Loss :  0.00011635780654614791  inv_L_scale:  [0.901, 0.911]
Epoch :  35  Time:  8.699  Rel. Train L2 Loss :  0.013229612469673156  Rel. Test L2 Loss :  0.01819098886102438  Test L2 Loss :  0.00012018183420877903  inv_L_scale:  [0.901, 0.911]
Epoch :  36  Time:  14.717  Rel. Train L2 Loss :  0.013078383073210716  Rel. Test L2 Loss :  0.017849922329187393  Test L2 Loss :  0.00011743161623599007  inv_L_scale:  [0.901, 0.911]
Epoch :  37  Time:  8.785  Rel. Train L2 Loss :  0.013182933546602726  Rel. Test L2 Loss :  0.01840250298380852  Test L2 Loss :  0.00012167284410679712  inv_L_scale:  [0.901, 0.911]
Epoch :  38  Time:  8.742  Rel. Train L2 Loss :  0.01401086087524891  Rel. Test L2 Loss :  0.018389615565538406  Test L2 Loss :  0.00012123316642828285  inv_L_scale:  [0.901, 0.911]
Epoch :  39  Time:  14.677  Rel. Train L2 Loss :  0.013812664374709129  Rel. Test L2 Loss :  0.017191791348159312  Test L2 Loss :  0.00011341394943883642  inv_L_scale:  [0.901, 0.911]
Epoch :  40  Time:  9.085  Rel. Train L2 Loss :  0.014674834206700325  Rel. Test L2 Loss :  0.018347628563642502  Test L2 Loss :  0.00012156258977483957  inv_L_scale:  [0.901, 0.911]
Epoch :  41  Time:  9.181  Rel. Train L2 Loss :  0.012940148390829563  Rel. Test L2 Loss :  0.01873821474611759  Test L2 Loss :  0.00012379513238556684  inv_L_scale:  [0.901, 0.911]
Epoch :  42  Time:  15.485  Rel. Train L2 Loss :  0.015204114004969597  Rel. Test L2 Loss :  0.017962199710309504  Test L2 Loss :  0.00011856211349368096  inv_L_scale:  [0.901, 0.911]
Epoch :  43  Time:  9.109  Rel. Train L2 Loss :  0.014528533391654492  Rel. Test L2 Loss :  0.018406689763069153  Test L2 Loss :  0.00012153771182056516  inv_L_scale:  [0.901, 0.911]
Epoch :  44  Time:  8.993  Rel. Train L2 Loss :  0.013529105924069881  Rel. Test L2 Loss :  0.01769028276205063  Test L2 Loss :  0.000116453152440954  inv_L_scale:  [0.901, 0.91]
Epoch :  45  Time:  14.889  Rel. Train L2 Loss :  0.013654483653604985  Rel. Test L2 Loss :  0.02009036399424076  Test L2 Loss :  0.00013158378831576556  inv_L_scale:  [0.901, 0.91]
Epoch :  46  Time:  8.798  Rel. Train L2 Loss :  0.013748790003359318  Rel. Test L2 Loss :  0.017970472797751425  Test L2 Loss :  0.0001185244190855883  inv_L_scale:  [0.901, 0.91]
Epoch :  47  Time:  8.759  Rel. Train L2 Loss :  0.012639486201107502  Rel. Test L2 Loss :  0.017264706380665303  Test L2 Loss :  0.00011414205888286232  inv_L_scale:  [0.901, 0.91]
Epoch :  48  Time:  14.638  Rel. Train L2 Loss :  0.012270723909139632  Rel. Test L2 Loss :  0.017419909574091435  Test L2 Loss :  0.00011487747222417966  inv_L_scale:  [0.9, 0.91]
Epoch :  49  Time:  8.788  Rel. Train L2 Loss :  0.013172956593334674  Rel. Test L2 Loss :  0.017273715622723104  Test L2 Loss :  0.0001137408337672241  inv_L_scale:  [0.901, 0.91]
Epoch :  50  Time:  8.835  Rel. Train L2 Loss :  0.01284178413450718  Rel. Test L2 Loss :  0.01746576864272356  Test L2 Loss :  0.00011567315086722375  inv_L_scale:  [0.901, 0.91]
Epoch :  51  Time:  14.748  Rel. Train L2 Loss :  0.013721945509314538  Rel. Test L2 Loss :  0.017408414371311666  Test L2 Loss :  0.00011469901568489149  inv_L_scale:  [0.9, 0.91]
Epoch :  52  Time:  8.711  Rel. Train L2 Loss :  0.012808907955884933  Rel. Test L2 Loss :  0.017353952676057816  Test L2 Loss :  0.00011415423476137221  inv_L_scale:  [0.9, 0.911]
Epoch :  53  Time:  8.671  Rel. Train L2 Loss :  0.012936156414449215  Rel. Test L2 Loss :  0.01728430550545454  Test L2 Loss :  0.00011383977107470855  inv_L_scale:  [0.9, 0.91]
Epoch :  54  Time:  14.613  Rel. Train L2 Loss :  0.012905925959348679  Rel. Test L2 Loss :  0.01797002732753754  Test L2 Loss :  0.00011859554389957338  inv_L_scale:  [0.9, 0.91]
Epoch :  55  Time:  8.705  Rel. Train L2 Loss :  0.012659942507743835  Rel. Test L2 Loss :  0.01735479760915041  Test L2 Loss :  0.00011439885071013123  inv_L_scale:  [0.9, 0.91]
Epoch :  56  Time:  8.678  Rel. Train L2 Loss :  0.013296558678150177  Rel. Test L2 Loss :  0.018323668129742145  Test L2 Loss :  0.0001210347018786706  inv_L_scale:  [0.9, 0.91]
Epoch :  57  Time:  14.612  Rel. Train L2 Loss :  0.01318693807721138  Rel. Test L2 Loss :  0.017880376242101193  Test L2 Loss :  0.00011794737743912264  inv_L_scale:  [0.9, 0.91]
Epoch :  58  Time:  8.778  Rel. Train L2 Loss :  0.012163042433559895  Rel. Test L2 Loss :  0.017339294515550135  Test L2 Loss :  0.00011415797431254759  inv_L_scale:  [0.9, 0.91]
Epoch :  59  Time:  8.844  Rel. Train L2 Loss :  0.012406941428780556  Rel. Test L2 Loss :  0.017452898435294628  Test L2 Loss :  0.00011519132094690576  inv_L_scale:  [0.9, 0.91]
Epoch :  60  Time:  14.791  Rel. Train L2 Loss :  0.012615889266133308  Rel. Test L2 Loss :  0.018888052850961685  Test L2 Loss :  0.0001243758833152242  inv_L_scale:  [0.9, 0.91]
Epoch :  61  Time:  8.834  Rel. Train L2 Loss :  0.01633499526232481  Rel. Test L2 Loss :  0.020777728706598282  Test L2 Loss :  0.00013679165072971955  inv_L_scale:  [0.9, 0.91]
Epoch :  62  Time:  8.782  Rel. Train L2 Loss :  0.014212890863418579  Rel. Test L2 Loss :  0.01940205190330744  Test L2 Loss :  0.00012826019927160813  inv_L_scale:  [0.9, 0.91]
Epoch :  63  Time:  14.982  Rel. Train L2 Loss :  0.013431758925318718  Rel. Test L2 Loss :  0.01750640567392111  Test L2 Loss :  0.00011480960238259285  inv_L_scale:  [0.9, 0.91]
Epoch :  64  Time:  8.741  Rel. Train L2 Loss :  0.01325370167940855  Rel. Test L2 Loss :  0.0168705889955163  Test L2 Loss :  0.00011167027871124447  inv_L_scale:  [0.9, 0.91]
Epoch :  65  Time:  8.68  Rel. Train L2 Loss :  0.013988471940159798  Rel. Test L2 Loss :  0.019280781149864198  Test L2 Loss :  0.00012769660708727314  inv_L_scale:  [0.9, 0.91]
Epoch :  66  Time:  14.731  Rel. Train L2 Loss :  0.013854405269026756  Rel. Test L2 Loss :  0.017210559509694575  Test L2 Loss :  0.00011348472500685602  inv_L_scale:  [0.9, 0.91]
Epoch :  67  Time:  8.726  Rel. Train L2 Loss :  0.013075239099562168  Rel. Test L2 Loss :  0.01841107990592718  Test L2 Loss :  0.00012165885913418605  inv_L_scale:  [0.9, 0.91]
Epoch :  68  Time:  8.774  Rel. Train L2 Loss :  0.014987302981317042  Rel. Test L2 Loss :  0.017179098054766653  Test L2 Loss :  0.00011353067005984485  inv_L_scale:  [0.9, 0.91]
Epoch :  69  Time:  14.907  Rel. Train L2 Loss :  0.013283205665647984  Rel. Test L2 Loss :  0.017868273109197617  Test L2 Loss :  0.00011744457617169247  inv_L_scale:  [0.9, 0.91]
Epoch :  70  Time:  8.811  Rel. Train L2 Loss :  0.011908974327147007  Rel. Test L2 Loss :  0.017178669311106204  Test L2 Loss :  0.0001133817996014841  inv_L_scale:  [0.9, 0.91]
Epoch :  71  Time:  8.795  Rel. Train L2 Loss :  0.012962237894535066  Rel. Test L2 Loss :  0.018448836356401443  Test L2 Loss :  0.00012142821564339102  inv_L_scale:  [0.9, 0.91]
Epoch :  72  Time:  14.77  Rel. Train L2 Loss :  0.012454599983990192  Rel. Test L2 Loss :  0.016842669397592543  Test L2 Loss :  0.00011107431026175619  inv_L_scale:  [0.9, 0.91]
Epoch :  73  Time:  8.739  Rel. Train L2 Loss :  0.01323578304797411  Rel. Test L2 Loss :  0.016726700663566588  Test L2 Loss :  0.00011012678151018917  inv_L_scale:  [0.9, 0.91]
Epoch :  74  Time:  8.697  Rel. Train L2 Loss :  0.01238648422807455  Rel. Test L2 Loss :  0.017480700463056564  Test L2 Loss :  0.00011591955932090059  inv_L_scale:  [0.9, 0.91]
Epoch :  75  Time:  14.54  Rel. Train L2 Loss :  0.012933229751884937  Rel. Test L2 Loss :  0.01757953781634569  Test L2 Loss :  0.00011578465695492923  inv_L_scale:  [0.9, 0.91]
Epoch :  76  Time:  8.878  Rel. Train L2 Loss :  0.012501286305487157  Rel. Test L2 Loss :  0.018390340358018877  Test L2 Loss :  0.00012140937644289807  inv_L_scale:  [0.9, 0.91]
Epoch :  77  Time:  8.637  Rel. Train L2 Loss :  0.013191096857190132  Rel. Test L2 Loss :  0.016620361506938935  Test L2 Loss :  0.00010970696661388501  inv_L_scale:  [0.9, 0.91]
Epoch :  78  Time:  14.698  Rel. Train L2 Loss :  0.01174665094912052  Rel. Test L2 Loss :  0.016765879355370997  Test L2 Loss :  0.00011047803272958844  inv_L_scale:  [0.899, 0.91]
Epoch :  79  Time:  8.762  Rel. Train L2 Loss :  0.011754635117948055  Rel. Test L2 Loss :  0.01721072942018509  Test L2 Loss :  0.00011359794065356255  inv_L_scale:  [0.899, 0.91]
Epoch :  80  Time:  8.715  Rel. Train L2 Loss :  0.012849580235779286  Rel. Test L2 Loss :  0.017867971621453763  Test L2 Loss :  0.00011817459686426446  inv_L_scale:  [0.9, 0.91]
Epoch :  81  Time:  14.681  Rel. Train L2 Loss :  0.012485262006521225  Rel. Test L2 Loss :  0.01838335156440735  Test L2 Loss :  0.00012047168565914035  inv_L_scale:  [0.9, 0.91]
Epoch :  82  Time:  8.79  Rel. Train L2 Loss :  0.012986471407115459  Rel. Test L2 Loss :  0.021063795238733293  Test L2 Loss :  0.0001395395855070092  inv_L_scale:  [0.899, 0.91]
Epoch :  83  Time:  8.726  Rel. Train L2 Loss :  0.012480122312903404  Rel. Test L2 Loss :  0.02020852714776993  Test L2 Loss :  0.00013481046131346374  inv_L_scale:  [0.9, 0.91]
Epoch :  84  Time:  14.486  Rel. Train L2 Loss :  0.013113456323742867  Rel. Test L2 Loss :  0.018657479733228684  Test L2 Loss :  0.00012235577334649862  inv_L_scale:  [0.9, 0.91]
Epoch :  85  Time:  8.736  Rel. Train L2 Loss :  0.013319553159177303  Rel. Test L2 Loss :  0.01786507114768028  Test L2 Loss :  0.00011789810261689126  inv_L_scale:  [0.9, 0.91]
Epoch :  86  Time:  9.058  Rel. Train L2 Loss :  0.013172822885215282  Rel. Test L2 Loss :  0.02105031192302704  Test L2 Loss :  0.0001401102656382136  inv_L_scale:  [0.9, 0.91]
Epoch :  87  Time:  14.6  Rel. Train L2 Loss :  0.014211920335888863  Rel. Test L2 Loss :  0.01786634184420109  Test L2 Loss :  0.00011841913947137072  inv_L_scale:  [0.9, 0.91]
Epoch :  88  Time:  8.814  Rel. Train L2 Loss :  0.01227281777560711  Rel. Test L2 Loss :  0.016672904789447784  Test L2 Loss :  0.00011025432613678276  inv_L_scale:  [0.9, 0.91]
Epoch :  89  Time:  8.841  Rel. Train L2 Loss :  0.0123237699046731  Rel. Test L2 Loss :  0.016998682580888273  Test L2 Loss :  0.00011207390140043572  inv_L_scale:  [0.9, 0.91]
Epoch :  90  Time:  14.727  Rel. Train L2 Loss :  0.012272756271064281  Rel. Test L2 Loss :  0.01638156548142433  Test L2 Loss :  0.00010820713068824262  inv_L_scale:  [0.9, 0.91]
Epoch :  91  Time:  8.951  Rel. Train L2 Loss :  0.014018470227718353  Rel. Test L2 Loss :  0.018967463076114653  Test L2 Loss :  0.00012476049014367163  inv_L_scale:  [0.9, 0.91]
Epoch :  92  Time:  9.288  Rel. Train L2 Loss :  0.012692021317780017  Rel. Test L2 Loss :  0.02256674312055111  Test L2 Loss :  0.00015152127074543386  inv_L_scale:  [0.9, 0.91]
Epoch :  93  Time:  16.23  Rel. Train L2 Loss :  0.013518180474638939  Rel. Test L2 Loss :  0.019522112533450126  Test L2 Loss :  0.0001298177835997194  inv_L_scale:  [0.9, 0.91]
Epoch :  94  Time:  9.321  Rel. Train L2 Loss :  0.013416321747004986  Rel. Test L2 Loss :  0.017626184225082397  Test L2 Loss :  0.00011677067261189222  inv_L_scale:  [0.9, 0.91]
Epoch :  95  Time:  8.917  Rel. Train L2 Loss :  0.012626693025231362  Rel. Test L2 Loss :  0.01719393700361252  Test L2 Loss :  0.0001131658730446361  inv_L_scale:  [0.899, 0.91]
Epoch :  96  Time:  15.268  Rel. Train L2 Loss :  0.012248468562960625  Rel. Test L2 Loss :  0.01614684846252203  Test L2 Loss :  0.00010651969903847203  inv_L_scale:  [0.9, 0.91]
Epoch :  97  Time:  8.854  Rel. Train L2 Loss :  0.012311439856886864  Rel. Test L2 Loss :  0.017441443130373956  Test L2 Loss :  0.00011493326892377808  inv_L_scale:  [0.9, 0.91]
Epoch :  98  Time:  8.966  Rel. Train L2 Loss :  0.01317778667062521  Rel. Test L2 Loss :  0.01686459455639124  Test L2 Loss :  0.00011106707213912159  inv_L_scale:  [0.9, 0.909]
Epoch :  99  Time:  15.187  Rel. Train L2 Loss :  0.012803594768047332  Rel. Test L2 Loss :  0.016806236058473586  Test L2 Loss :  0.00011091671651229262  inv_L_scale:  [0.9, 0.91]
Epoch :  100  Time:  8.749  Rel. Train L2 Loss :  0.014862019143998623  Rel. Test L2 Loss :  0.01769502639770508  Test L2 Loss :  0.00011651058681309223  inv_L_scale:  [0.9, 0.91]
Epoch :  101  Time:  8.813  Rel. Train L2 Loss :  0.014078658878803253  Rel. Test L2 Loss :  0.016636978425085543  Test L2 Loss :  0.00010995509393978863  inv_L_scale:  [0.9, 0.91]
Epoch :  102  Time:  14.694  Rel. Train L2 Loss :  0.01360787795484066  Rel. Test L2 Loss :  0.01701223202049732  Test L2 Loss :  0.00011181497131474317  inv_L_scale:  [0.9, 0.91]
Epoch :  103  Time:  8.701  Rel. Train L2 Loss :  0.01310826138406992  Rel. Test L2 Loss :  0.019565792232751848  Test L2 Loss :  0.0001285539008677006  inv_L_scale:  [0.9, 0.91]
Epoch :  104  Time:  8.754  Rel. Train L2 Loss :  0.015425450704991817  Rel. Test L2 Loss :  0.017888766713440418  Test L2 Loss :  0.00011822650325484574  inv_L_scale:  [0.9, 0.91]
Epoch :  105  Time:  14.732  Rel. Train L2 Loss :  0.012525979295372964  Rel. Test L2 Loss :  0.017929672710597516  Test L2 Loss :  0.00011792476725531742  inv_L_scale:  [0.9, 0.91]
Epoch :  106  Time:  8.728  Rel. Train L2 Loss :  0.012329262427985668  Rel. Test L2 Loss :  0.017244303934276103  Test L2 Loss :  0.00011343979596858844  inv_L_scale:  [0.9, 0.91]
Epoch :  107  Time:  8.683  Rel. Train L2 Loss :  0.012371149629354476  Rel. Test L2 Loss :  0.01838244006037712  Test L2 Loss :  0.00012184056598925964  inv_L_scale:  [0.9, 0.91]
Epoch :  108  Time:  14.792  Rel. Train L2 Loss :  0.012793547302484512  Rel. Test L2 Loss :  0.01570039238780737  Test L2 Loss :  0.00010351407603593543  inv_L_scale:  [0.9, 0.91]
Epoch :  109  Time:  8.741  Rel. Train L2 Loss :  0.012818142674863338  Rel. Test L2 Loss :  0.01700212564319372  Test L2 Loss :  0.00011242165113799274  inv_L_scale:  [0.9, 0.91]
Epoch :  110  Time:  8.861  Rel. Train L2 Loss :  0.014362414367496967  Rel. Test L2 Loss :  0.016586400382220746  Test L2 Loss :  0.00010984524298692122  inv_L_scale:  [0.9, 0.91]
Epoch :  111  Time:  14.772  Rel. Train L2 Loss :  0.013152660496532917  Rel. Test L2 Loss :  0.016727524735033514  Test L2 Loss :  0.00011019438941730187  inv_L_scale:  [0.9, 0.91]
Epoch :  112  Time:  8.79  Rel. Train L2 Loss :  0.013167467877268791  Rel. Test L2 Loss :  0.016735091395676135  Test L2 Loss :  0.00011043481848901138  inv_L_scale:  [0.9, 0.91]
Epoch :  113  Time:  8.698  Rel. Train L2 Loss :  0.013431994587183  Rel. Test L2 Loss :  0.019824276491999627  Test L2 Loss :  0.0001306759685394354  inv_L_scale:  [0.9, 0.91]
Epoch :  114  Time:  14.906  Rel. Train L2 Loss :  0.01444361601769924  Rel. Test L2 Loss :  0.01827981896698475  Test L2 Loss :  0.00011957619775785134  inv_L_scale:  [0.9, 0.911]
Epoch :  115  Time:  8.739  Rel. Train L2 Loss :  0.013799666963517665  Rel. Test L2 Loss :  0.021012293398380278  Test L2 Loss :  0.000138157190522179  inv_L_scale:  [0.9, 0.91]
Epoch :  116  Time:  8.942  Rel. Train L2 Loss :  0.013026920154690742  Rel. Test L2 Loss :  0.016938606947660445  Test L2 Loss :  0.00011175583029398694  inv_L_scale:  [0.9, 0.91]
Epoch :  117  Time:  14.504  Rel. Train L2 Loss :  0.012344189181923867  Rel. Test L2 Loss :  0.017824050784111024  Test L2 Loss :  0.00011855808785185218  inv_L_scale:  [0.9, 0.91]
Epoch :  118  Time:  8.592  Rel. Train L2 Loss :  0.011877723395824432  Rel. Test L2 Loss :  0.019094455689191818  Test L2 Loss :  0.00012493707501562312  inv_L_scale:  [0.9, 0.91]
Epoch :  119  Time:  8.73  Rel. Train L2 Loss :  0.012420449271798134  Rel. Test L2 Loss :  0.016954100094735622  Test L2 Loss :  0.00011163575778482482  inv_L_scale:  [0.9, 0.91]
Epoch :  120  Time:  14.86  Rel. Train L2 Loss :  0.011800153151154518  Rel. Test L2 Loss :  0.015848745740950108  Test L2 Loss :  0.00010437805263791234  inv_L_scale:  [0.9, 0.91]
Epoch :  121  Time:  8.774  Rel. Train L2 Loss :  0.012925491407513619  Rel. Test L2 Loss :  0.01647027235478163  Test L2 Loss :  0.00010906224022619426  inv_L_scale:  [0.9, 0.911]
Epoch :  122  Time:  8.698  Rel. Train L2 Loss :  0.013845777675509452  Rel. Test L2 Loss :  0.016701272390782833  Test L2 Loss :  0.00011004014784703031  inv_L_scale:  [0.9, 0.911]
Epoch :  123  Time:  14.715  Rel. Train L2 Loss :  0.013330175399780273  Rel. Test L2 Loss :  0.0206846983730793  Test L2 Loss :  0.00013639547891216352  inv_L_scale:  [0.9, 0.911]
Epoch :  124  Time:  8.654  Rel. Train L2 Loss :  0.013475336968898773  Rel. Test L2 Loss :  0.01658615719527006  Test L2 Loss :  0.0001098785933572799  inv_L_scale:  [0.9, 0.911]
Epoch :  125  Time:  8.715  Rel. Train L2 Loss :  0.013527219161391258  Rel. Test L2 Loss :  0.01763658557087183  Test L2 Loss :  0.00011631078057689593  inv_L_scale:  [0.9, 0.911]
Epoch :  126  Time:  14.633  Rel. Train L2 Loss :  0.013293107897043228  Rel. Test L2 Loss :  0.01735122412443161  Test L2 Loss :  0.00011438283429015427  inv_L_scale:  [0.9, 0.911]
Epoch :  127  Time:  8.675  Rel. Train L2 Loss :  0.011826762296259404  Rel. Test L2 Loss :  0.01614410039037466  Test L2 Loss :  0.0001068053228664212  inv_L_scale:  [0.9, 0.911]
Epoch :  128  Time:  8.716  Rel. Train L2 Loss :  0.012581887558102607  Rel. Test L2 Loss :  0.018176772966980934  Test L2 Loss :  0.00011882139864610508  inv_L_scale:  [0.9, 0.911]
Epoch :  129  Time:  15.04  Rel. Train L2 Loss :  0.013260020412504674  Rel. Test L2 Loss :  0.01727263744920492  Test L2 Loss :  0.00011463419679785147  inv_L_scale:  [0.901, 0.911]
Epoch :  130  Time:  9.78  Rel. Train L2 Loss :  0.014365764930844308  Rel. Test L2 Loss :  0.019408788830041886  Test L2 Loss :  0.0001276792274438776  inv_L_scale:  [0.9, 0.911]
Epoch :  131  Time:  9.324  Rel. Train L2 Loss :  0.013309322349727154  Rel. Test L2 Loss :  0.017681749276816844  Test L2 Loss :  0.00011715322674717755  inv_L_scale:  [0.9, 0.911]
Epoch :  132  Time:  14.838  Rel. Train L2 Loss :  0.012721793249249458  Rel. Test L2 Loss :  0.016996254436671732  Test L2 Loss :  0.00011257867881795392  inv_L_scale:  [0.901, 0.911]
Epoch :  133  Time:  8.85  Rel. Train L2 Loss :  0.013191288188099861  Rel. Test L2 Loss :  0.01797744892537594  Test L2 Loss :  0.0001181055695633404  inv_L_scale:  [0.9, 0.91]
Epoch :  134  Time:  8.929  Rel. Train L2 Loss :  0.013978651456534862  Rel. Test L2 Loss :  0.01669814370572567  Test L2 Loss :  0.0001108522154390812  inv_L_scale:  [0.9, 0.911]
Epoch :  135  Time:  14.607  Rel. Train L2 Loss :  0.012271475218236447  Rel. Test L2 Loss :  0.017945091500878335  Test L2 Loss :  0.00011753761500585825  inv_L_scale:  [0.9, 0.911]
Epoch :  136  Time:  8.891  Rel. Train L2 Loss :  0.012574609972536564  Rel. Test L2 Loss :  0.018045091591775416  Test L2 Loss :  0.00012008934601908549  inv_L_scale:  [0.9, 0.911]
Epoch :  137  Time:  8.808  Rel. Train L2 Loss :  0.013070495873689652  Rel. Test L2 Loss :  0.017337142266333105  Test L2 Loss :  0.0001146012285607867  inv_L_scale:  [0.9, 0.911]
Epoch :  138  Time:  14.675  Rel. Train L2 Loss :  0.012161873050034046  Rel. Test L2 Loss :  0.016585829928517343  Test L2 Loss :  0.00010963712324155495  inv_L_scale:  [0.9, 0.911]
Epoch :  139  Time:  8.827  Rel. Train L2 Loss :  0.013645639516413212  Rel. Test L2 Loss :  0.018580866530537605  Test L2 Loss :  0.00012363116606138645  inv_L_scale:  [0.901, 0.911]
Epoch :  140  Time:  8.765  Rel. Train L2 Loss :  0.012832179591059685  Rel. Test L2 Loss :  0.01868199251592159  Test L2 Loss :  0.0001231290341820568  inv_L_scale:  [0.901, 0.911]
Epoch :  141  Time:  14.805  Rel. Train L2 Loss :  0.012150858893990518  Rel. Test L2 Loss :  0.016564343832433225  Test L2 Loss :  0.00010907538788160309  inv_L_scale:  [0.9, 0.911]
Epoch :  142  Time:  8.88  Rel. Train L2 Loss :  0.012452249467372894  Rel. Test L2 Loss :  0.017005333825945854  Test L2 Loss :  0.00011190452118171378  inv_L_scale:  [0.9, 0.911]
Epoch :  143  Time:  8.951  Rel. Train L2 Loss :  0.012301618710160256  Rel. Test L2 Loss :  0.0177350989356637  Test L2 Loss :  0.00011759873625123874  inv_L_scale:  [0.901, 0.911]
Epoch :  144  Time:  14.76  Rel. Train L2 Loss :  0.012304579846560955  Rel. Test L2 Loss :  0.019287828877568244  Test L2 Loss :  0.0001269968258566223  inv_L_scale:  [0.901, 0.911]
Epoch :  145  Time:  8.813  Rel. Train L2 Loss :  0.013669556893408298  Rel. Test L2 Loss :  0.016805930584669112  Test L2 Loss :  0.00011084034224040807  inv_L_scale:  [0.9, 0.911]
Epoch :  146  Time:  8.802  Rel. Train L2 Loss :  0.01621969237923622  Rel. Test L2 Loss :  0.0196268093585968  Test L2 Loss :  0.0001287634126492776  inv_L_scale:  [0.9, 0.911]
Epoch :  147  Time:  14.756  Rel. Train L2 Loss :  0.012899598531425  Rel. Test L2 Loss :  0.01731582120060921  Test L2 Loss :  0.00011423096380895003  inv_L_scale:  [0.9, 0.911]
Epoch :  148  Time:  8.826  Rel. Train L2 Loss :  0.01308104958385229  Rel. Test L2 Loss :  0.01732632227241993  Test L2 Loss :  0.00011447324010077864  inv_L_scale:  [0.9, 0.911]
Epoch :  149  Time:  8.737  Rel. Train L2 Loss :  0.013300115324556827  Rel. Test L2 Loss :  0.016173444911837576  Test L2 Loss :  0.00010701024788431824  inv_L_scale:  [0.9, 0.911]
Epoch :  150  Time:  14.655  Rel. Train L2 Loss :  0.012052798733115197  Rel. Test L2 Loss :  0.01618009738624096  Test L2 Loss :  0.00010715079813962803  inv_L_scale:  [0.901, 0.911]
Epoch :  151  Time:  8.819  Rel. Train L2 Loss :  0.01264934318512678  Rel. Test L2 Loss :  0.01845133610069752  Test L2 Loss :  0.00012102585635147988  inv_L_scale:  [0.901, 0.91]
Epoch :  152  Time:  8.819  Rel. Train L2 Loss :  0.012800277799367904  Rel. Test L2 Loss :  0.018500929810106755  Test L2 Loss :  0.00012305575859500095  inv_L_scale:  [0.901, 0.911]
Epoch :  153  Time:  14.736  Rel. Train L2 Loss :  0.012885343633592129  Rel. Test L2 Loss :  0.01621916301548481  Test L2 Loss :  0.00010730914771556855  inv_L_scale:  [0.9, 0.911]
Epoch :  154  Time:  8.837  Rel. Train L2 Loss :  0.012660001769661904  Rel. Test L2 Loss :  0.01728118147701025  Test L2 Loss :  0.00011459370522061363  inv_L_scale:  [0.9, 0.911]
Epoch :  155  Time:  9.004  Rel. Train L2 Loss :  0.013333752736449241  Rel. Test L2 Loss :  0.017499766536056994  Test L2 Loss :  0.00011568132089450955  inv_L_scale:  [0.9, 0.911]
Epoch :  156  Time:  14.959  Rel. Train L2 Loss :  0.013070635855197906  Rel. Test L2 Loss :  0.0170413114130497  Test L2 Loss :  0.0001123608768102713  inv_L_scale:  [0.9, 0.911]
Epoch :  157  Time:  8.943  Rel. Train L2 Loss :  0.012631797567009925  Rel. Test L2 Loss :  0.016570039205253125  Test L2 Loss :  0.0001095783049822785  inv_L_scale:  [0.901, 0.912]
Epoch :  158  Time:  8.942  Rel. Train L2 Loss :  0.012971117034554482  Rel. Test L2 Loss :  0.017326173223555088  Test L2 Loss :  0.00011454317718744278  inv_L_scale:  [0.901, 0.912]
Epoch :  159  Time:  14.691  Rel. Train L2 Loss :  0.012717655539512634  Rel. Test L2 Loss :  0.016501116044819356  Test L2 Loss :  0.0001087928403285332  inv_L_scale:  [0.901, 0.911]
Epoch :  160  Time:  8.815  Rel. Train L2 Loss :  0.013033934034407139  Rel. Test L2 Loss :  0.017297498323023318  Test L2 Loss :  0.00011388428159989417  inv_L_scale:  [0.901, 0.911]
Epoch :  161  Time:  8.852  Rel. Train L2 Loss :  0.01248319648206234  Rel. Test L2 Loss :  0.018972925171256066  Test L2 Loss :  0.00012604467425262556  inv_L_scale:  [0.901, 0.912]
Epoch :  162  Time:  14.914  Rel. Train L2 Loss :  0.012454316265881062  Rel. Test L2 Loss :  0.01785967532545328  Test L2 Loss :  0.00011875491123646498  inv_L_scale:  [0.901, 0.912]
Epoch :  163  Time:  8.766  Rel. Train L2 Loss :  0.012568848311901092  Rel. Test L2 Loss :  0.019296590238809586  Test L2 Loss :  0.00012623722141142936  inv_L_scale:  [0.901, 0.911]
Epoch :  164  Time:  8.881  Rel. Train L2 Loss :  0.012684237278997899  Rel. Test L2 Loss :  0.02010498695075512  Test L2 Loss :  0.0001311221532523632  inv_L_scale:  [0.901, 0.911]
Epoch :  165  Time:  14.705  Rel. Train L2 Loss :  0.013752704948186874  Rel. Test L2 Loss :  0.01701504088938236  Test L2 Loss :  0.00011253075499553233  inv_L_scale:  [0.901, 0.912]
Epoch :  166  Time:  8.647  Rel. Train L2 Loss :  0.012649995535612106  Rel. Test L2 Loss :  0.016584525778889657  Test L2 Loss :  0.00010924390779109671  inv_L_scale:  [0.901, 0.911]
Epoch :  167  Time:  8.829  Rel. Train L2 Loss :  0.012694252885878086  Rel. Test L2 Loss :  0.017343907058238982  Test L2 Loss :  0.0001137563301017508  inv_L_scale:  [0.901, 0.911]
Epoch :  168  Time:  14.756  Rel. Train L2 Loss :  0.012487933486700058  Rel. Test L2 Loss :  0.017431908436119557  Test L2 Loss :  0.00011473687336547301  inv_L_scale:  [0.901, 0.912]
Epoch :  169  Time:  8.849  Rel. Train L2 Loss :  0.012497759886085986  Rel. Test L2 Loss :  0.01875621244311333  Test L2 Loss :  0.00012377471721265466  inv_L_scale:  [0.901, 0.912]
Epoch :  170  Time:  8.921  Rel. Train L2 Loss :  0.01238549941033125  Rel. Test L2 Loss :  0.016156721115112304  Test L2 Loss :  0.00010673130862414836  inv_L_scale:  [0.901, 0.912]
Epoch :  171  Time:  14.82  Rel. Train L2 Loss :  0.013229363150894641  Rel. Test L2 Loss :  0.017160389460623263  Test L2 Loss :  0.00011367171420715749  inv_L_scale:  [0.901, 0.912]
Epoch :  172  Time:  8.862  Rel. Train L2 Loss :  0.013270187683403493  Rel. Test L2 Loss :  0.01663079410791397  Test L2 Loss :  0.00011041336285416036  inv_L_scale:  [0.901, 0.912]
Epoch :  173  Time:  8.775  Rel. Train L2 Loss :  0.012648917526006699  Rel. Test L2 Loss :  0.01601716361939907  Test L2 Loss :  0.00010574951418675483  inv_L_scale:  [0.901, 0.912]
Epoch :  174  Time:  14.769  Rel. Train L2 Loss :  0.012323160536587238  Rel. Test L2 Loss :  0.01667534451931715  Test L2 Loss :  0.00011043518548831344  inv_L_scale:  [0.901, 0.912]
Epoch :  175  Time:  9.024  Rel. Train L2 Loss :  0.012325535260140895  Rel. Test L2 Loss :  0.016547368504107  Test L2 Loss :  0.0001089325311477296  inv_L_scale:  [0.901, 0.911]
Epoch :  176  Time:  8.898  Rel. Train L2 Loss :  0.012727038815617561  Rel. Test L2 Loss :  0.01612833693623543  Test L2 Loss :  0.00010676061821868644  inv_L_scale:  [0.901, 0.912]
Epoch :  177  Time:  14.851  Rel. Train L2 Loss :  0.01175536808371544  Rel. Test L2 Loss :  0.017090281955897808  Test L2 Loss :  0.00011210615077288821  inv_L_scale:  [0.901, 0.912]
Epoch :  178  Time:  8.843  Rel. Train L2 Loss :  0.012192084036767482  Rel. Test L2 Loss :  0.015942650288343428  Test L2 Loss :  0.00010512578883208335  inv_L_scale:  [0.901, 0.912]
Epoch :  179  Time:  8.988  Rel. Train L2 Loss :  0.0118827218785882  Rel. Test L2 Loss :  0.019359544590115546  Test L2 Loss :  0.00012687753449426963  inv_L_scale:  [0.902, 0.912]
Epoch :  180  Time:  14.955  Rel. Train L2 Loss :  0.011807134456932544  Rel. Test L2 Loss :  0.017175836749374868  Test L2 Loss :  0.00011370867316145449  inv_L_scale:  [0.902, 0.912]
Epoch :  181  Time:  8.89  Rel. Train L2 Loss :  0.010985628575086594  Rel. Test L2 Loss :  0.01743954636156559  Test L2 Loss :  0.00011624831589870155  inv_L_scale:  [0.901, 0.912]
Epoch :  182  Time:  8.887  Rel. Train L2 Loss :  0.013003169365227222  Rel. Test L2 Loss :  0.017512114159762858  Test L2 Loss :  0.0001147126586874947  inv_L_scale:  [0.901, 0.912]
Epoch :  183  Time:  14.822  Rel. Train L2 Loss :  0.011958294495940209  Rel. Test L2 Loss :  0.016888355799019338  Test L2 Loss :  0.00011112483509350567  inv_L_scale:  [0.902, 0.912]
Epoch :  184  Time:  8.773  Rel. Train L2 Loss :  0.012429278083145619  Rel. Test L2 Loss :  0.01755963746458292  Test L2 Loss :  0.00011679681047098711  inv_L_scale:  [0.902, 0.912]
Epoch :  185  Time:  8.851  Rel. Train L2 Loss :  0.013416819155216217  Rel. Test L2 Loss :  0.015573789067566395  Test L2 Loss :  0.0001026343647390604  inv_L_scale:  [0.901, 0.912]
Epoch :  186  Time:  14.945  Rel. Train L2 Loss :  0.012876423723995686  Rel. Test L2 Loss :  0.01594585061073303  Test L2 Loss :  0.00010550479259109124  inv_L_scale:  [0.901, 0.912]
Epoch :  187  Time:  8.806  Rel. Train L2 Loss :  0.012111930675804615  Rel. Test L2 Loss :  0.01922276623547077  Test L2 Loss :  0.00012735778669593857  inv_L_scale:  [0.901, 0.912]
Epoch :  188  Time:  8.79  Rel. Train L2 Loss :  0.012667406506836414  Rel. Test L2 Loss :  0.016497853063046932  Test L2 Loss :  0.00010862354683922603  inv_L_scale:  [0.901, 0.912]
Epoch :  189  Time:  14.591  Rel. Train L2 Loss :  0.012097379535436631  Rel. Test L2 Loss :  0.025953480079770087  Test L2 Loss :  0.00016992988996207713  inv_L_scale:  [0.9, 0.912]
Epoch :  190  Time:  8.746  Rel. Train L2 Loss :  0.011818289645016194  Rel. Test L2 Loss :  0.016047262847423555  Test L2 Loss :  0.000106311556592118  inv_L_scale:  [0.901, 0.912]
Epoch :  191  Time:  8.952  Rel. Train L2 Loss :  0.011966897308826446  Rel. Test L2 Loss :  0.018896842077374458  Test L2 Loss :  0.00012430783244781195  inv_L_scale:  [0.901, 0.912]
Epoch :  192  Time:  14.697  Rel. Train L2 Loss :  0.011950438916683197  Rel. Test L2 Loss :  0.019181502759456636  Test L2 Loss :  0.00012775153096299618  inv_L_scale:  [0.901, 0.912]
Epoch :  193  Time:  8.911  Rel. Train L2 Loss :  0.012066688477993012  Rel. Test L2 Loss :  0.018072389625012876  Test L2 Loss :  0.00011978213005932048  inv_L_scale:  [0.901, 0.912]
Epoch :  194  Time:  8.832  Rel. Train L2 Loss :  0.0118762264624238  Rel. Test L2 Loss :  0.016258850172162057  Test L2 Loss :  0.00010768738284241408  inv_L_scale:  [0.901, 0.912]
Epoch :  195  Time:  14.679  Rel. Train L2 Loss :  0.012184379659593104  Rel. Test L2 Loss :  0.01623832158744335  Test L2 Loss :  0.00010747671738499776  inv_L_scale:  [0.901, 0.912]
Epoch :  196  Time:  8.805  Rel. Train L2 Loss :  0.01148459094762802  Rel. Test L2 Loss :  0.01828420639038086  Test L2 Loss :  0.00012139336147811264  inv_L_scale:  [0.901, 0.911]
Epoch :  197  Time:  8.759  Rel. Train L2 Loss :  0.012056106664240361  Rel. Test L2 Loss :  0.01611436627805233  Test L2 Loss :  0.000106498854001984  inv_L_scale:  [0.901, 0.912]
Epoch :  198  Time:  14.808  Rel. Train L2 Loss :  0.011945913590490818  Rel. Test L2 Loss :  0.016991675123572348  Test L2 Loss :  0.0001130101116723381  inv_L_scale:  [0.901, 0.912]
Epoch :  199  Time:  8.841  Rel. Train L2 Loss :  0.012536692634224892  Rel. Test L2 Loss :  0.0169546652212739  Test L2 Loss :  0.00011302133265417069  inv_L_scale:  [0.901, 0.912]
Epoch :  200  Time:  9.232  Rel. Train L2 Loss :  0.011082354180514812  Rel. Test L2 Loss :  0.01547868076711893  Test L2 Loss :  0.00010228157421806827  inv_L_scale:  [0.901, 0.912]
Epoch :  201  Time:  14.97  Rel. Train L2 Loss :  0.01258368194103241  Rel. Test L2 Loss :  0.017857306748628617  Test L2 Loss :  0.00011912713467609138  inv_L_scale:  [0.902, 0.912]
Epoch :  202  Time:  8.835  Rel. Train L2 Loss :  0.01197456456720829  Rel. Test L2 Loss :  0.015545426197350025  Test L2 Loss :  0.00010286725941114128  inv_L_scale:  [0.901, 0.911]
Epoch :  203  Time:  8.861  Rel. Train L2 Loss :  0.011941426046192647  Rel. Test L2 Loss :  0.01609195403754711  Test L2 Loss :  0.00010623607697198168  inv_L_scale:  [0.901, 0.911]
Epoch :  204  Time:  14.935  Rel. Train L2 Loss :  0.012178740240633487  Rel. Test L2 Loss :  0.015787200666964055  Test L2 Loss :  0.0001044264028314501  inv_L_scale:  [0.902, 0.912]
Epoch :  205  Time:  8.899  Rel. Train L2 Loss :  0.011369821466505527  Rel. Test L2 Loss :  0.015895076617598532  Test L2 Loss :  0.00010511708067497239  inv_L_scale:  [0.901, 0.912]
Epoch :  206  Time:  8.803  Rel. Train L2 Loss :  0.011907493859529496  Rel. Test L2 Loss :  0.01611229632049799  Test L2 Loss :  0.0001064874726580456  inv_L_scale:  [0.901, 0.912]
Epoch :  207  Time:  14.849  Rel. Train L2 Loss :  0.01135737404972315  Rel. Test L2 Loss :  0.01609908614307642  Test L2 Loss :  0.00010630885575665162  inv_L_scale:  [0.901, 0.911]
Epoch :  208  Time:  8.793  Rel. Train L2 Loss :  0.011050151117146014  Rel. Test L2 Loss :  0.015726070553064346  Test L2 Loss :  0.00010400825616670773  inv_L_scale:  [0.902, 0.912]
Epoch :  209  Time:  8.918  Rel. Train L2 Loss :  0.01171344531327486  Rel. Test L2 Loss :  0.016289233155548574  Test L2 Loss :  0.00010742898448370397  inv_L_scale:  [0.901, 0.912]
Epoch :  210  Time:  15.112  Rel. Train L2 Loss :  0.011933734878897666  Rel. Test L2 Loss :  0.01978382483124733  Test L2 Loss :  0.00013010167138418182  inv_L_scale:  [0.901, 0.911]
Epoch :  211  Time:  8.691  Rel. Train L2 Loss :  0.011582113675773144  Rel. Test L2 Loss :  0.01592614661902189  Test L2 Loss :  0.00010544988559558988  inv_L_scale:  [0.901, 0.912]
Epoch :  212  Time:  8.83  Rel. Train L2 Loss :  0.011558352947235107  Rel. Test L2 Loss :  0.017234945371747017  Test L2 Loss :  0.00011322794220177457  inv_L_scale:  [0.9, 0.912]
Epoch :  213  Time:  14.846  Rel. Train L2 Loss :  0.011815876990556718  Rel. Test L2 Loss :  0.01775178633630276  Test L2 Loss :  0.00011788115109084173  inv_L_scale:  [0.901, 0.912]
Epoch :  214  Time:  9.014  Rel. Train L2 Loss :  0.011761924110352992  Rel. Test L2 Loss :  0.016240720823407172  Test L2 Loss :  0.00010672265198081732  inv_L_scale:  [0.901, 0.912]
Epoch :  215  Time:  8.757  Rel. Train L2 Loss :  0.012249301001429558  Rel. Test L2 Loss :  0.01664902064949274  Test L2 Loss :  0.00010962536995066329  inv_L_scale:  [0.901, 0.912]
Epoch :  216  Time:  14.801  Rel. Train L2 Loss :  0.01141840323060751  Rel. Test L2 Loss :  0.016480752825737  Test L2 Loss :  0.00010927830910077318  inv_L_scale:  [0.901, 0.912]
Epoch :  217  Time:  8.815  Rel. Train L2 Loss :  0.011141859151422977  Rel. Test L2 Loss :  0.017671849019825457  Test L2 Loss :  0.00011634105001576245  inv_L_scale:  [0.902, 0.912]
Epoch :  218  Time:  8.746  Rel. Train L2 Loss :  0.0116155287027359  Rel. Test L2 Loss :  0.016014465875923634  Test L2 Loss :  0.00010599891567835584  inv_L_scale:  [0.901, 0.911]
Epoch :  219  Time:  14.829  Rel. Train L2 Loss :  0.011866121798753739  Rel. Test L2 Loss :  0.01622320245951414  Test L2 Loss :  0.00010743193444795907  inv_L_scale:  [0.901, 0.912]
Epoch :  220  Time:  8.805  Rel. Train L2 Loss :  0.011424346499145031  Rel. Test L2 Loss :  0.016271449737250806  Test L2 Loss :  0.00010767944593681022  inv_L_scale:  [0.901, 0.912]
Epoch :  221  Time:  8.919  Rel. Train L2 Loss :  0.011128680936992168  Rel. Test L2 Loss :  0.01558890577405691  Test L2 Loss :  0.00010279519949108363  inv_L_scale:  [0.901, 0.912]
Epoch :  222  Time:  14.866  Rel. Train L2 Loss :  0.010935183122754096  Rel. Test L2 Loss :  0.01798178117722273  Test L2 Loss :  0.00012018558307318017  inv_L_scale:  [0.902, 0.912]
Epoch :  223  Time:  8.91  Rel. Train L2 Loss :  0.011951760604977608  Rel. Test L2 Loss :  0.016286791674792765  Test L2 Loss :  0.0001077596892719157  inv_L_scale:  [0.901, 0.912]
Epoch :  224  Time:  9.093  Rel. Train L2 Loss :  0.011023650713264941  Rel. Test L2 Loss :  0.016744565218687057  Test L2 Loss :  0.00010987750603817403  inv_L_scale:  [0.901, 0.912]
Epoch :  225  Time:  14.974  Rel. Train L2 Loss :  0.010821422711014748  Rel. Test L2 Loss :  0.01541457887738943  Test L2 Loss :  0.0001016000032541342  inv_L_scale:  [0.901, 0.912]
Epoch :  226  Time:  8.765  Rel. Train L2 Loss :  0.010863605469465255  Rel. Test L2 Loss :  0.015642848797142506  Test L2 Loss :  0.00010350278636906296  inv_L_scale:  [0.901, 0.912]
Epoch :  227  Time:  8.976  Rel. Train L2 Loss :  0.011410599939525128  Rel. Test L2 Loss :  0.016204698458313942  Test L2 Loss :  0.00010774075286462903  inv_L_scale:  [0.901, 0.912]
Epoch :  228  Time:  14.878  Rel. Train L2 Loss :  0.01136807855218649  Rel. Test L2 Loss :  0.015615870803594589  Test L2 Loss :  0.00010343084170017392  inv_L_scale:  [0.901, 0.912]
Epoch :  229  Time:  8.867  Rel. Train L2 Loss :  0.012099253229796887  Rel. Test L2 Loss :  0.01563841976225376  Test L2 Loss :  0.000103419148363173  inv_L_scale:  [0.901, 0.911]
Epoch :  230  Time:  8.813  Rel. Train L2 Loss :  0.011416357852518558  Rel. Test L2 Loss :  0.015590845420956612  Test L2 Loss :  0.00010300490248482674  inv_L_scale:  [0.901, 0.912]
Epoch :  231  Time:  15.088  Rel. Train L2 Loss :  0.010887083247303963  Rel. Test L2 Loss :  0.016378654837608336  Test L2 Loss :  0.00010791896085720509  inv_L_scale:  [0.901, 0.912]
Epoch :  232  Time:  9.078  Rel. Train L2 Loss :  0.010990757629275321  Rel. Test L2 Loss :  0.016321180909872054  Test L2 Loss :  0.0001075962680624798  inv_L_scale:  [0.901, 0.912]